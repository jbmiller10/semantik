#!/usr/bin/env python3

"""
Comprehensive unit tests for all chunking strategies using the unified implementation.

This module tests all implemented chunking strategies with various
edge cases, configurations, and document types.
"""

import os
from typing import Any

import pytest
from llama_index.core.embeddings import MockEmbedding

from packages.shared.text_processing.base_chunker import ChunkResult
from packages.shared.text_processing.chunking_factory import ChunkingFactory
from packages.shared.text_processing.file_type_detector import FileTypeDetector

# Set testing environment
os.environ["TESTING"] = "true"


class TestUnifiedChunkingStrategies:
    """Comprehensive tests for all strategies using unified implementation."""

    @pytest.fixture()
    def mock_embed_model(self) -> MockEmbedding:
        """Mock embedding model for semantic chunking tests."""
        return MockEmbedding(embed_dim=384)

    # Test data fixtures
    EDGE_CASES = {
        "empty": "",
        "single_char": "A",
        "unicode": "Hello ‰∏ñÁïå! üåç ‚Üí ‚Ç¨¬£¬•",
        "very_long_line": "a" * 50000,
        "null_bytes": "Hello\x00World",
        "mixed_encoding": "Hello World with special chars: √© √† √±",
        "only_whitespace": "   \n\n\t\t  ",
        "html_injection": "<script>alert('xss')</script>",
        "sql_injection": "'; DROP TABLE chunks; --",
        "repeated_newlines": "Line1\n\n\n\n\nLine2",
        "mixed_line_endings": "Line1\r\nLine2\nLine3\r",
        "emoji_heavy": "üëç Test üòä with üöÄ many üåü emojis üéâ",
        "special_markdown": "# Header\n\n```python\ncode\n```\n\n> Quote",
    }

    CODE_SAMPLES = {
        "python": '''def fibonacci(n):
    """Calculate fibonacci number."""
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)

class Calculator:
    """Simple calculator class."""

    def add(self, a, b) -> None:
        return a + b

    def multiply(self, a, b) -> None:
        return a * b
''',
        "javascript": """function greet(name) {
    console.log(`Hello, ${name}!`);
}

const calculator = {
    add: (a, b) => a + b,
    multiply: (a, b) => a * b
};

export default calculator;
""",
    }

    MARKDOWN_SAMPLES = {
        "simple": """# Main Header

This is a paragraph with some text.

## Subheader

More content here.
""",
        "complex": """# Project Documentation

## Overview

This project implements a chunking system.

### Features

- Feature 1
- Feature 2
- Feature 3

## Installation

```bash
pip install chunking
```

### Requirements

1. Python 3.11+
2. LlamaIndex
3. Redis

## Usage

Here's how to use it:

```python
chunker = ChunkingFactory.create_chunker(config)
chunks = chunker.chunk_text(text)
```

### Advanced Usage

For more complex scenarios...
""",
    }

    ALL_STRATEGIES = ["character", "recursive", "markdown", "semantic", "hierarchical", "hybrid"]

    def get_default_config(self, strategy: str) -> dict[str, Any]:
        """Get default configuration for a strategy with token-based sizing."""
        configs = {
            "character": {
                "strategy": "character",
                "params": {
                    "max_tokens": 25,  # ~100 chars
                    "min_tokens": 10,  # ~40 chars
                    "overlap_tokens": 5,  # ~20 chars
                },
            },
            "recursive": {
                "strategy": "recursive",
                "params": {
                    "max_tokens": 25,  # ~100 chars
                    "min_tokens": 10,  # ~40 chars
                    "overlap_tokens": 5,  # ~20 chars
                },
            },
            "markdown": {
                "strategy": "markdown",
                "params": {
                    "max_tokens": 50,  # Reasonable default for markdown
                    "min_tokens": 10,
                    "overlap_tokens": 5,
                },
            },
            "semantic": {
                "strategy": "semantic",
                "params": {
                    "max_tokens": 25,  # ~100 chars
                    "min_tokens": 10,
                    "overlap_tokens": 5,
                    "breakpoint_percentile_threshold": 95,
                    "buffer_size": 1,
                },
            },
            "hierarchical": {
                "strategy": "hierarchical",
                "params": {
                    "max_tokens": 50,  # ~200 chars for parent
                    "min_tokens": 10,
                    "overlap_tokens": 2,  # ~10 chars
                    "chunk_sizes": [50, 25, 12],  # Token-based sizes
                },
            },
            "hybrid": {
                "strategy": "hybrid",
                "params": {
                    "max_tokens": 25,
                    "min_tokens": 10,
                    "overlap_tokens": 5,
                    "markdown_threshold": 0.15,
                    "semantic_coherence_threshold": 0.7,
                    "large_doc_threshold": 50000,
                    "fallback_strategy": "recursive",
                },
            },
        }
        return configs.get(strategy, configs["recursive"])

    @pytest.mark.parametrize("strategy", ALL_STRATEGIES)
    @pytest.mark.parametrize(("edge_case_name", "text"), EDGE_CASES.items())
    async def test_edge_cases(
        self,
        strategy: str,
        edge_case_name: str,
        text: str,
    ) -> None:
        """Test all strategies handle edge cases gracefully."""
        config = self.get_default_config(strategy)
        chunker = ChunkingFactory.create_chunker(config)

        # Should not raise exception
        try:
            chunks = await chunker.chunk_text_async(text, "test")

            # Validate chunks
            if text.strip():  # Non-empty input
                assert isinstance(chunks, list)
                for chunk in chunks:
                    assert isinstance(chunk, ChunkResult)
                    # Allow empty chunks for edge cases
            else:  # Empty input
                assert chunks == []

        except Exception as e:
            pytest.fail(f"{strategy} failed on {edge_case_name}: {e}")

    async def test_character_chunker_basic(self) -> None:
        """Test basic character chunker functionality."""
        from packages.shared.text_processing.strategies.character_chunker import CharacterChunker

        # Use token-based sizes
        chunker = CharacterChunker(chunk_size=50, chunk_overlap=10)

        text = "This is a test document. " * 10  # ~250 characters
        chunks = await chunker.chunk_text_async(text, "test_doc")

        # Verify chunks
        assert len(chunks) > 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

        # Verify overlap exists (but don't check exact offsets)
        assert len(chunks) >= 2

    async def test_recursive_chunker_basic(self) -> None:
        """Test basic recursive chunker functionality."""
        from packages.shared.text_processing.strategies.recursive_chunker import RecursiveChunker

        chunker = RecursiveChunker(chunk_size=50, chunk_overlap=10)

        text = "This is sentence one. This is sentence two. This is sentence three. " * 10
        chunks = await chunker.chunk_text_async(text, "test_doc")

        # Verify chunks
        assert len(chunks) > 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

    async def test_recursive_chunker_code_optimization(self) -> None:
        """Test recursive chunker with code file optimization."""
        from packages.shared.text_processing.strategies.recursive_chunker import RecursiveChunker

        chunker = RecursiveChunker()

        # Test with Python code
        metadata = {"file_type": ".py", "file_name": "test.py"}
        chunks = await chunker.chunk_text_async(
            self.CODE_SAMPLES["python"],
            "test_code",
            metadata,
        )

        # Verify code optimization was applied
        assert len(chunks) >= 1
        # Note: is_code_file may not be in metadata with unified implementation
        # Just verify chunking worked
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

    async def test_markdown_chunker_basic(self) -> None:
        """Test basic markdown chunker functionality."""
        from packages.shared.text_processing.strategies.markdown_chunker import MarkdownChunker

        chunker = MarkdownChunker()

        chunks = await chunker.chunk_text_async(
            self.MARKDOWN_SAMPLES["simple"],
            "test_doc",
        )

        # Verify chunks
        assert len(chunks) >= 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

        # Verify markdown structure is preserved
        chunk_texts = [chunk.text for chunk in chunks]
        all_text = " ".join(chunk_texts)
        assert "Main Header" in all_text
        assert "Subheader" in all_text

    async def test_markdown_chunker_complex(self) -> None:
        """Test markdown chunker with complex document."""
        from packages.shared.text_processing.strategies.markdown_chunker import MarkdownChunker

        chunker = MarkdownChunker()

        chunks = await chunker.chunk_text_async(
            self.MARKDOWN_SAMPLES["complex"],
            "test_doc",
            {"file_type": ".md"},
        )

        # Verify chunks
        assert len(chunks) >= 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

        # Code blocks should be preserved within chunks
        all_text = " ".join(c.text for c in chunks)
        assert "pip install" in all_text or "```" in all_text

    async def test_factory_creation(self) -> None:
        """Test factory creates correct chunker instances."""
        # Character chunker
        config = {"strategy": "character", "params": {"max_tokens": 25}}
        chunker = ChunkingFactory.create_chunker(config)
        # Just verify creation works
        assert chunker is not None

        # Recursive chunker
        config = {"strategy": "recursive", "params": {"max_tokens": 50}}
        chunker = ChunkingFactory.create_chunker(config)
        assert chunker is not None

        # Markdown chunker
        config = {"strategy": "markdown", "params": {}}
        chunker = ChunkingFactory.create_chunker(config)
        assert chunker is not None

    async def test_factory_invalid_strategy(self) -> None:
        """Test factory handles invalid strategy."""
        config = {"strategy": "invalid_strategy", "params": {}}

        with pytest.raises(ValueError, match="Unknown chunking strategy"):
            ChunkingFactory.create_chunker(config)

    def test_factory_available_strategies(self) -> None:
        """Test factory returns available strategies."""
        strategies = ChunkingFactory.get_available_strategies()

        assert "character" in strategies
        assert "recursive" in strategies
        assert "markdown" in strategies
        assert "semantic" in strategies
        assert "hierarchical" in strategies
        assert "hybrid" in strategies
        assert len(strategies) >= 6

    @pytest.mark.parametrize("strategy", ["character", "recursive"])
    def test_validate_config(self, strategy: str) -> None:
        """Test configuration validation."""
        config = self.get_default_config(strategy)
        chunker = ChunkingFactory.create_chunker(config)

        # Valid config
        assert chunker.validate_config(config["params"]) is True

        # Invalid overlap (greater than max_tokens)
        invalid_config = {
            "max_tokens": 100,
            "min_tokens": 50,
            "overlap_tokens": 200,  # Greater than max_tokens
        }
        assert chunker.validate_config(invalid_config) is False

    @pytest.mark.parametrize("strategy", ["character", "recursive"])
    def test_estimate_chunks(self, strategy: str) -> None:
        """Test chunk estimation."""
        config = self.get_default_config(strategy)
        chunker = ChunkingFactory.create_chunker(config)

        # Small text (~100 chars = ~25 tokens)
        estimate = chunker.estimate_chunks(100, config["params"])
        assert estimate >= 1
        assert estimate <= 5

        # Large text (~10000 chars = ~2500 tokens)
        estimate = chunker.estimate_chunks(10000, config["params"])
        assert estimate > 10
        assert estimate < 1000

    async def test_metadata_preservation(self) -> None:
        """Test metadata is preserved in chunks."""
        from packages.shared.text_processing.strategies.recursive_chunker import RecursiveChunker

        chunker = RecursiveChunker(chunk_size=50)

        metadata = {
            "source": "test_file.txt",
            "author": "Test Author",
            "custom_field": 123,
        }

        text = "This is a test. " * 10
        chunks = await chunker.chunk_text_async(text, "test_doc", metadata)

        # All chunks should have metadata
        assert all(chunk.metadata for chunk in chunks)

        # Strategy should be added to metadata
        assert all(chunk.metadata["strategy"] == "recursive" for chunk in chunks)

        # Original metadata should be preserved
        for chunk in chunks:
            assert chunk.metadata.get("source") == "test_file.txt"
            assert chunk.metadata.get("author") == "Test Author"
            assert chunk.metadata.get("custom_field") == 123

    async def test_chunk_ids_unique(self) -> None:
        """Test chunk IDs are unique."""
        from packages.shared.text_processing.strategies.character_chunker import CharacterChunker

        chunker = CharacterChunker(chunk_size=50)

        text = "Test text. " * 20
        chunks = await chunker.chunk_text_async(text, "doc123")

        # All chunk IDs should be unique
        chunk_ids = [chunk.chunk_id for chunk in chunks]
        assert len(chunk_ids) == len(set(chunk_ids))

        # Chunk IDs should follow pattern
        for _i, chunk in enumerate(chunks):
            assert chunk.chunk_id.startswith("doc123")

    async def test_unicode_handling(self) -> None:
        """Test proper Unicode handling."""
        from packages.shared.text_processing.strategies.recursive_chunker import RecursiveChunker

        chunker = RecursiveChunker(chunk_size=50)

        # Various Unicode texts
        texts = [
            "Hello ‰∏ñÁïå! Testing ‰∏≠Êñá characters.",
            "√âmojis: üöÄ üåü üéâ and symbols: ‚Üí ‚Ç¨ ¬£ ¬•",
            "Mixed: Caf√©, na√Øve, r√©sum√©, Z√ºrich",
            "RTL: ÿßŸÑÿπÿ±ÿ®Ÿäÿ© and ◊¢◊ë◊®◊ô◊™ text",
        ]

        for text in texts:
            chunks = await chunker.chunk_text_async(text, "unicode_test")

            # Should handle without errors
            assert len(chunks) >= 1

    async def test_large_document_handling(self) -> None:
        """Test handling of large documents."""
        from packages.shared.text_processing.strategies.character_chunker import CharacterChunker

        # Use larger chunk_size to avoid ChunkSizeViolationError when LlamaIndex creates slightly larger chunks
        # 1300/5 = 260 tokens which gives headroom for 252 token chunks
        chunker = CharacterChunker(chunk_size=1300, chunk_overlap=200)

        # Generate large document (1MB)
        large_text = "This is a test sentence. " * 50000

        chunks = await chunker.chunk_text_async(large_text, "large_doc")

        # Should create many chunks
        assert len(chunks) > 100

        # All chunks should be valid
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

    async def test_hierarchical_chunker_basic(self) -> None:
        """Test hierarchical chunker creates chunks."""
        from packages.shared.text_processing.strategies.hierarchical_chunker import HierarchicalChunker

        chunker = HierarchicalChunker(chunk_sizes=[200, 100, 50], chunk_overlap=10)

        # Create text that will require multiple hierarchy levels
        text = " ".join([f"Sentence {i}." for i in range(100)])  # ~700 chars
        chunks = await chunker.chunk_text_async(text, "hier_test")

        # Should create chunks
        assert len(chunks) > 0
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

    async def test_semantic_chunker_basic(self, mock_embed_model) -> None:
        """Test basic semantic chunker functionality."""
        from packages.shared.text_processing.strategies.semantic_chunker import SemanticChunker

        chunker = SemanticChunker(
            breakpoint_percentile_threshold=95, buffer_size=1, max_chunk_size=100, embed_model=mock_embed_model
        )

        text = "This is a test document. It has multiple sentences. Each sentence is different. The semantic chunker should find natural boundaries."
        chunks = await chunker.chunk_text_async(text, "semantic_test")

        # Verify chunks
        assert len(chunks) >= 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

    async def test_hybrid_chunker_basic(self) -> None:
        """Test hybrid chunker basic functionality."""
        from packages.shared.text_processing.strategies.hybrid_chunker import HybridChunker

        chunker = HybridChunker()

        # Simple text
        text = "This is a simple test document with some content."
        chunks = await chunker.chunk_text_async(text, "test_doc")

        # Should produce chunks
        assert len(chunks) >= 1
        assert all(isinstance(chunk, ChunkResult) for chunk in chunks)

        # Should have hybrid metadata
        for chunk in chunks:
            assert chunk.metadata.get("hybrid_chunker") is True
            assert "selected_strategy" in chunk.metadata

    async def test_hybrid_chunker_markdown_detection(self) -> None:
        """Test hybrid chunker selects markdown strategy for markdown content."""
        from packages.shared.text_processing.strategies.hybrid_chunker import HybridChunker

        chunker = HybridChunker(semantic_coherence_threshold=0.9)

        # Test with markdown file extension
        chunks = await chunker.chunk_text_async(
            self.MARKDOWN_SAMPLES["simple"], "test_doc", {"file_path": "/path/to/test.md"}
        )

        # Verify markdown strategy was selected
        assert len(chunks) >= 1
        for chunk in chunks:
            assert chunk.metadata["hybrid_chunker"] is True
            # Should select markdown for .md extension
            if "file extension" in chunk.metadata.get("hybrid_strategy_reasoning", ""):
                assert chunk.metadata["selected_strategy"] == "markdown"

    async def test_hybrid_chunker_edge_cases(self) -> None:
        """Test hybrid chunker with various edge cases."""
        from packages.shared.text_processing.strategies.hybrid_chunker import HybridChunker

        chunker = HybridChunker()

        # Empty text
        chunks = await chunker.chunk_text_async("", "empty_doc")
        assert chunks == []

        # Very short text
        chunks = await chunker.chunk_text_async("Short.", "short_doc")
        assert len(chunks) == 1
        assert chunks[0].metadata["selected_strategy"] in ["recursive", "character"]

    def test_hybrid_chunker_config_validation(self) -> None:
        """Test hybrid chunker configuration validation."""
        from packages.shared.text_processing.strategies.hybrid_chunker import HybridChunker

        chunker = HybridChunker()

        # Valid config
        valid_config = {
            "markdown_threshold": 0.2,
            "semantic_coherence_threshold": 0.8,
            "large_doc_threshold": 10000,
            "fallback_strategy": "character",
        }
        assert chunker.validate_config(valid_config) is True

        # Invalid threshold values
        invalid_configs: list[dict[str, Any]] = [
            {"markdown_threshold": -0.1},
            {"markdown_threshold": 1.5},
            {"semantic_coherence_threshold": "high"},
            {"large_doc_threshold": -1000},
        ]

        for config in invalid_configs:
            assert chunker.validate_config(config) is False

    @pytest.mark.integration()
    async def test_code_file_optimization(self) -> None:
        """Test code files get optimized parameters."""
        code_file = """
def fibonacci(n) -> None:
    \"\"\"Calculate fibonacci number recursively.

    Args:
        n: The position in fibonacci sequence

    Returns:
        The fibonacci number at position n
    \"\"\"
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)
"""

        # Test with .py file type
        config = FileTypeDetector.get_optimal_config("test.py")
        assert config["strategy"] == "recursive"

        # Just verify chunking works
        chunker = ChunkingFactory.create_chunker(config)
        chunks = await chunker.chunk_text_async(code_file, "test.py")

        # Verify chunks
        assert len(chunks) >= 1
        assert any("fibonacci" in chunk.text for chunk in chunks)
