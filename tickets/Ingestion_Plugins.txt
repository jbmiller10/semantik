  Phase 1: Database Schema & Core Data Structures

  Goal: Prepare persistent storage and data contracts for non‑file‑based sources without breaking existing behavior.

  ———

  Ticket 1: Database Migration for Flexible Sources

  Context:
  The current schema assumes all sources are local directories and all documents are local files. We need to support arbitrary source types (e.g. web, Slack) while remaining backward
  compatible.

  Files to analyze:

  - packages/shared/database/models.py
  - alembic/ (directory structure)
  - packages/shared/database/repositories/document_repository.py
  - Any existing Alembic migration helpers (if present)

  Requirements:

  - Create a new Alembic revision.
  - collection_sources table:
      - Add source_config column (JSON/JSONB, nullable).
      - Remove the hardcoded default "directory" from source_type (keep it nullable=False, but no default at the DB level).
      - Data migration:
          - For existing rows, backfill source_config as {"path": source_path} to make legacy directory sources usable by connectors without extra logic.
  - documents table:
      - Add uri column (String, nullable, indexed). This is the logical identifier for the document (URL, Slack message ID, file path, etc.).
      - Add source_metadata column (JSON/JSONB, nullable) to store connector‑specific metadata (raw response, headers, Slack thread info, etc.).
      - Data migration: for existing rows:
          - uri = file_path so all existing documents remain addressable via uri.
          - Leave source_metadata as NULL (preserve meta semantics as they are today).
      - Indexes/constraints:
          - Add a non‑unique index on uri or on (collection_id, uri) for efficient lookups.
          - Consider (and document the decision) whether (collection_id, uri) should be unique; if yes, enforce as a unique index constrained to non‑NULL uri.
  - ORM & repositories:
      - Update Document and CollectionSource models in packages/shared/database/models.py to include:
          - CollectionSource.source_config: JSON | None
          - Document.uri: String | None
          - Document.source_metadata: JSON | None
      - Ensure any Enum/type‑level constraints on source_type (Pydantic, OpenAPI, etc.) are updated to allow generic strings (not only "directory"), or moved to validation rules instead of
        DB defaults.
      - Update DocumentRepository to:
          - Continue to dedupe on collection_id + content_hash as today.
          - Accept and persist optional uri and source_metadata when creating documents (e.g. via a new helper or extended create signature).
  - Data compatibility:
      - Ensure existing services that rely on source_type and file_path still behave identically after migration (local directory ingestion must remain unchanged).

  Acceptance Criteria:

  - alembic upgrade head and alembic downgrade -1 work on a clean DB and on a DB with existing data.
  - Existing documents have uri populated exactly matching file_path.
  - Existing collection_sources rows have source_config = {"path": source_path}.
  - CollectionSource and Document ORM models compile and match the new columns.
  - DocumentRepository.create(...) continues to work for existing callers and can optionally handle uri and source_metadata for new code paths.

  ———

  Ticket 2: Implement IngestedDocument DTO and Hashing Helper

  Context:
  We need a single, strict data contract for ingested documents across connectors (local, web, Slack, etc.) and a consistent way to compute content hashes.

  Files to analyze:

  - packages/shared/ (check for existing DTO patterns, e.g. packages/shared/dtos/)
  - packages/shared/database/repositories/document_repository.py (to align dedupe semantics)
  - packages/shared/text_processing/ (to see how content flows today)

  Requirements:

  - Create a new module packages/shared/dtos/ingestion.py.
  - Define a dataclass (or Pydantic model if preferred for validation) IngestedDocument with:
      - content: str — fully parsed text content returned by extraction/parsing.
      - unique_id: str — logical identifier (URI, path, message ID, etc.).
      - source_type: str — e.g. "directory", "web", "slack".
      - metadata: dict[str, object] — raw/original metadata from the connector (headers, timestamps, etc.).
      - content_hash: str — SHA‑256 hash of content (hex string; 64 chars).
      - file_path: str | None = None — optional local file path for backward compatibility with file‑based sources.
  - Implement a shared helper function in the same module or a nearby utility (e.g. packages/shared/utils/hashing.py):
      - def compute_content_hash(content: str | bytes) -> str:
          - Normalize encoding (utf-8) and newline behavior as needed.
          - Return a lowercase SHA‑256 hex digest (64 chars).
      - All connectors and DocumentRegistryService must use this helper for content hashing (do not reimplement hashing logic in multiple places).
  - Typing:
      - Use precise type hints (dict[str, Any] or a dedicated TypedDict/Pydantic metadata model rather than bare dict).
      - Ensure import path works: from shared.dtos.ingestion import IngestedDocument.

  Acceptance Criteria:

  - IngestedDocument is importable via from shared.dtos.ingestion import IngestedDocument.
  - compute_content_hash produces a deterministic, 64‑character hex string for sample inputs, with tests covering simple and multi‑byte cases.
  - Type hints are strict and mypy passes for the new module.
  - No existing code paths are broken; new DTO is additive until consumed in later tickets.

  ———

  Phase 2: Refactoring Core Services

  Goal: Decouple file IO from parsing and centralize document registration/deduplication logic.

  ———

  Ticket 3: Refactor Extraction Logic (Separate Loader vs Parser)

  Context:
  packages/shared/text_processing/extraction.py currently both opens files from disk and parses them. We want to parse from in‑memory content (e.g. HTTP responses) while preserving existing
  behavior for file‑based callers.

  Files to analyze:

  - packages/shared/text_processing/extraction.py
  - Any call sites in packages/webui/tasks/ or services that use extract_and_serialize / extract_text.

  Requirements:

  - Introduce a new function in extraction.py:
      - def parse_document_content(content: bytes | str, file_extension: str, metadata: dict[str, Any] | None = None) -> list[tuple[str, dict[str, Any]]]:
          - Encapsulate the current unstructured.partition.auto.partition logic.
          - Use a file‑like object or the text/file parameters from unstructured to support in‑memory content.
          - Return the same shape as extract_and_serialize: list of (text, metadata) tuples.
          - Merge any provided metadata (e.g. connector‑level info) into element metadata where appropriate.
  - Refactor extract_and_serialize(filepath: str) to:
      - Read the file from disk (Path(filepath).suffix to determine file_extension).
      - Pass the raw bytes/string and extension into parse_document_content.
      - Preserve logging and error semantics (same exceptions and error messages where possible).
  - Keep extract_text behavior intact:
      - Continue to build on top of extract_and_serialize and return joined text.
  - Ensure no external callers see any behavioral changes:
      - Signature and return type of extract_and_serialize remain unchanged.
      - Timeouts, logging, and exception behavior remain compatible.

  Acceptance Criteria:

  - Existing unit tests for extract_and_serialize and extract_text still pass.
  - New tests cover parse_document_content being called directly with raw str and bytes content and simple extensions (e.g. ".txt").
  - Callers that pass file paths see identical output (modulo harmless metadata ordering) and error behavior.

  ———

  Ticket 4: Create DocumentRegistryService

  Context:
  DocumentScanningService mixes directory traversal (os.walk) with database registration and deduplication logic. We want a dedicated registry that other connectors can reuse for registration
  and dedupe, driven by the IngestedDocument DTO.

  Files to analyze:

  - packages/webui/services/document_scanning_service.py
  - packages/shared/database/repositories/document_repository.py
  - packages/shared/dtos/ingestion.py (Ticket 2)

  Requirements:

  - Create packages/webui/services/document_registry_service.py with a DocumentRegistryService class that:
      - Is initialized with an AsyncSession and DocumentRepository.
      - Exposes a method like async def register(self, collection_id: str, ingested: IngestedDocument, source_id: int | None = None) -> dict[str, Any]:
          - Use ingested.content_hash (validated via the hashing helper) to check for duplicates via DocumentRepository.get_by_content_hash.
          - If a document with the same content_hash already exists in the collection:
              - Return information indicating is_new=False, and the existing document ID.
          - Otherwise:
              - Create a new Document using DocumentRepository:
                  - collection_id – from argument.
                  - file_path – use ingested.file_path if present, otherwise fall back to ingested.unique_id for now.
                  - file_name – derive from ingested.file_path or unique_id as appropriate.
                  - file_size – if known from metadata; otherwise 0 or a reasonable default (document this choice).
                  - content_hash – ingested.content_hash.
                  - mime_type – optionally inferred or omitted initially.
                  - source_id – passed through.
                  - meta – set to ingested.metadata for now.
                  - uri – ingested.unique_id.
                  - source_metadata – ingested.metadata or a subset, consistent with your earlier decision in Ticket 1.
              - Handle race conditions by respecting DocumentRepository.create’s existing dedupe logic and catching IntegrityError as it already does.
          - Return a result dict consistent with the current _register_file output:
              - is_new: bool
              - document_id: str
              - file_size: int
  - Refactor DocumentScanningService:
      - Move _calculate_file_hash logic into the shared hashing helper (Ticket 2), or ensure both use the same helper.
      - Move the core registration code from _register_file into DocumentRegistryService.register where feasible.
      - Keep DocumentScanningService as a backward‑compatible wrapper that:
          - Walks the filesystem.
          - Builds IngestedDocument DTOs from local files.
          - Calls DocumentRegistryService.register for each file.
          - Preserves the shape of its public return values and error behavior.
  - Decide and document where content_hash is mandatory:
      - IngestedDocument should always contain a content_hash.
      - DocumentRegistryService validates and rejects DTOs with invalid hashes.

  Acceptance Criteria:

  - DocumentRegistryService can successfully insert new documents and skip duplicates based on content_hash.
  - DocumentScanningService.scan_directory_and_register_documents behavior (stats, logging, dedupe) remains unchanged except for using the shared registry under the hood.
  - Tests cover:
      - Registering a new document via DTO.
      - Registering a duplicate (same content_hash) and verifying dedupe.
      - Race conditions handled via DocumentRepository.create as before.

  ———

  Phase 3: Connector Architecture

  Goal: Introduce a reusable connector interface and adapter layer for various source types.

  ———

  Ticket 5: Define BaseConnector and ConnectorFactory

  Context:
  We need a standard interface for all connectors (directory, web, Slack, etc.) and a factory to resolve connectors by source_type.

  Files to analyze:

  - packages/shared/ (for base abstractions)
  - packages/webui/services/ (to see where connectors will be consumed)

  Requirements:

  - Create packages/shared/connectors/base.py with:
      - An abstract base class BaseConnector:
          - def __init__(self, config: dict[str, object]): — store the validated connector config.
          - async def authenticate(self) -> bool: — abstract; perform any auth/handshake, return True on success.
          - def validate_config(self) -> None: — optional hook to validate config (can be concrete and reused).
          - def __aiter__(self) -> AsyncIterator[IngestedDocument]: or
            async def load_documents(self) -> AsyncIterator[IngestedDocument]: — decides the canonical interface.
              - For clarity, stick to async def load_documents(...) -> AsyncIterator[IngestedDocument] and use async for in callers.
  - Create packages/webui/services/connector_factory.py with:
      - A ConnectorFactory class or module‑level function:
          - def get_connector(source_type: str, config: dict[str, object]) -> BaseConnector:
              - Normalize source_type (e.g. lowercasing).
              - Look up a mapping { 'directory': LocalFileConnector, ... }.
              - Instantiate the connector with config.
              - Raise ValueError for unknown source_type.
      - Keep registration simple for now (static mapping), but structure it so additional connectors can be added easily.
  - Typing:
      - Ensure factory and connectors depend on IngestedDocument from shared.dtos.ingestion.

  Acceptance Criteria:

  - BaseConnector enforces authenticate and load_documents implementation in subclasses.
  - ConnectorFactory.get_connector('directory', {...}) returns a BaseConnector subclass instance (once LocalFileConnector is implemented).
  - get_connector raises ValueError for unknown source_type in tests.

  ———

  Ticket 6: Implement LocalFileConnector

  Context:
  We want to port the existing directory scanning logic into a connector that yields IngestedDocument instances.

  Files to analyze:

  - packages/webui/services/document_scanning_service.py (for traversal and filtering logic)
  - packages/shared/connectors/local.py (new file)
  - packages/shared/text_processing/extraction.py
  - packages/shared/dtos/ingestion.py
  - packages/webui/services/document_registry_service.py

  Requirements:

  - Create packages/shared/connectors/local.py with:
      - class LocalFileConnector(BaseConnector) that:
          - Expects a config containing at least {"path": "<directory>"}, plus optional flags (recursive, include/exclude patterns, etc.).
          - Implements authenticate as a simple existence check for the directory path (returns True or raises on failure).
          - Implements load_documents by:
              - Reusing os.walk logic from DocumentScanningService (supported extensions, size limits, etc.).
              - For each supported file:
                  - Reads bytes (streaming if needed).
                  - Calls parse_document_content/extract_and_serialize to get text + metadata.
                  - Produces an IngestedDocument with:
                      - content = combined text for the document (consistent with how chunking expects content).
                      - unique_id = file path string.
                      - source_type = "directory".
                      - metadata = metadata merged from extraction + file system info.
                      - content_hash = compute_content_hash(content).
                      - file_path = absolute or canonical file path.
              - Yields one IngestedDocument per file.
  - Register LocalFileConnector in ConnectorFactory:
      - ConnectorFactory.get_connector('directory', config) returns an instance of LocalFileConnector.
  - Backward compatibility:
      - Keep DocumentScanningService in place for now; a later ticket (or part of Ticket 8) will shift the ingestion pipeline to use connectors + registry instead of directly scanning.

  Acceptance Criteria:

  - In a test harness, LocalFileConnector can iterate a test directory and yield IngestedDocument instances with expected content, metadata, and hashes.
  - ConnectorFactory.get_connector('directory', {'path': '/tmp'}) returns a LocalFileConnector.
  - Unsupported files and error conditions are handled consistently with the existing DocumentScanningService behavior (e.g. skipped or logged).

  ———

  Phase 4: API & Task Integration

  Goal: Expose the new configuration schema via API and route ingestion through the connector + registry pipeline.

  ———

  Ticket 7: Update AddSource API Schema & Service Contract

  Context:
  The API currently accepts a string source_path and optional config. We need to move toward source_type + source_config while keeping the old shape working.

  Files to analyze:

  - packages/webui/api/schemas.py
  - packages/webui/api/v2/collections.py
  - packages/webui/services/collection_service.py
  - packages/shared/database/models.py (CollectionSource)
  - Any OpenAPI or client generation expectations

  Requirements:

  - Modify AddSourceRequest in packages/webui/api/schemas.py:
      - Add fields:
          - source_type: str = Field(default="directory", description="Type of source (e.g. directory, web, slack)")
          - source_config: dict[str, Any] | None = Field(default=None, description="Connector-specific configuration")
      - Keep source_path: str | None but mark it as deprecated in docstring/schema description.
      - Add a validator (or model_validator) that:
          - If source_path is provided and source_type/source_config are not:
              - Set source_type = "directory".
              - Set source_config = {"path": source_path}.
          - If both source_path and source_config are present, prefer source_config but allow it for backward compatibility (document behavior).
  - Update the v2 add_source endpoint in packages/webui/api/v2/collections.py:
      - Call service.add_source with:
          - source_type=add_source_request.source_type
          - source_config=add_source_request.source_config or {}
          - Optionally still pass source_path if the service currently depends on it, but treat it as legacy.
  - Update CollectionService.add_source:
      - Signature:

        async def add_source(
            self,
            collection_id: str,
            user_id: int,
            source_type: str,
            source_config: dict[str, Any] | None = None,
            legacy_source_path: str | None = None,
        ) -> dict[str, Any]:

        or similar, with clear distinction between the new and legacy fields.
      - Inside:
          - Normalize:
              - If legacy_source_path is provided and source_config is empty, derive source_config = {"path": legacy_source_path} and source_type = "directory".
          - Persist a CollectionSource row with:
              - source_type from request.
              - source_path maintained for le


  Ticket 8: Update Ingestion Task Pipeline to Use Connectors & Registry

  Context:
  The Celery task that processes APPEND operations currently calls
  DocumentScanningService directly and assumes directory‑based sources. We want it to
  use ConnectorFactory + DocumentRegistryService so other connectors can be wired in.

  Files to analyze:

  - packages/webui/tasks/ingestion.py (_process_append_operation_impl)
  - packages/webui/services/collection_service.py (for operation config shape)
  - packages/webui/services/document_scanning_service.py
  - packages/webui/services/document_registry_service.py
  - packages/shared/connectors/base.py, packages/shared/connectors/local.py
  - packages/shared/dtos/ingestion.py

  Requirements:

  - In _process_append_operation_impl:
      - Extract from operation["config"]:
          - source_type (default "directory" if missing).
          - source_config (dict, may be { "path": source_path }).
          - source_path (legacy).
      - Fallback logic:
          - If source_type/source_config are missing but source_path is present:
              - Treat as source_type="directory", source_config={"path": source_path}.
          - This ensures operations created before Ticket 7 continue to work.
      - Instantiate DocumentRegistryService with the existing document_repo.session.
      - Use ConnectorFactory.get_connector(source_type, source_config) to get a
  connector.
      - await connector.authenticate(), and bail out with a clear error if auth fails.
      - Iterate documents:

        async for ingested_doc in connector.load_documents():
            result = await registry.register(collection["id"], ingested_doc,
  source_id=<matching CollectionSource.id>)
            # If is_new, run chunking + embedding, otherwise skip.
          - Replace the current
  DocumentScanningService.scan_directory_and_register_documents + manual filtering by
  file_path.startswith(source_path):
              - Register and track new documents via DocumentRegistryService.
              - Then, for new documents (or unprocessed ones), run the existing chunking
  + embedding block.
          - Ensure file_path usage in downstream logging and status updates gracefully
  falls back to ingested_doc.unique_id when no real file path exists.
  - Compatibility mode:
      - Optionally, for a transitional step:
          - Reuse DocumentScanningService internally for "directory" until
  LocalFileConnector is fully battle‑tested, but structure the code to make switching to
  LocalFileConnector trivial.
  - Metrics and logging:
      - Preserve existing metrics (documents_added, duplicates_skipped,
  failed_documents) by reconstructing these counters from registry results.
      - Continue sending WebSocket updates via updater.send_update(...) with source
  information derived from source_config (e.g. source_path or uri).

  Acceptance Criteria:

  - Adding a local directory source via the API still results in documents being indexed
  in Qdrant with no noticeable regression in performance.
  - _process_append_operation_impl no longer hard‑codes os.walk and
  DocumentScanningService assumptions; instead it uses connectors + registry (or is
  clearly structured to switch to them).
  - The system is now technically capable of handling a new source_type (e.g. "web") by
  plugging in a new connector without further changes to the ingestion task code.
  - All existing integration/e2e tests for APPEND operations continue to pass.




  Phase 5: Frontend Alignment

  Goal: Align frontend types and UX with the new source configuration model while
  preserving compatibility for directory sources.

  ———

  Ticket 9: Frontend AddSource Schema & UX Updates

  Context:
  The frontend currently sends { source_path, config } and assumes directory sources. We
  need to support source_type + source_config while keeping the existing flows working.

  Files to analyze:

  - apps/webui-react/src/types/collection.ts
  - apps/webui-react/src/hooks/useCollectionOperations.ts
  - apps/webui-react/src/components/CreateCollectionModal.tsx
  - apps/webui-react/src/components/AddDataToCollectionModal.tsx
  - Components that display source_path from operation config or documents:
      - apps/webui-react/src/components/ActiveOperationsTab.tsx
      - apps/webui-react/src/components/OperationProgress.tsx
      - apps/webui-react/src/components/CollectionDetailsModal.tsx
      - apps/webui-react/src/hooks/useCollectionDocuments.ts

  Requirements:

  - Update frontend AddSourceRequest type (apps/webui-react/src/types/collection.ts):
      - Extend to include:
          - source_type?: string;  // default "directory" on backend
          - source_config?: Record<string, unknown>;
      - Keep source_path: string for now, but annotate/comment it as deprecated in favor
  of source_type + source_config.
  - Update useAddSource (useCollectionOperations.ts):
      - By default, when the user provides a directory path:
          - Call the API with both:
              - source_type: 'directory'
              - source_config: { path: sourcePath, ...existingConfig }
              - And still include source_path for backward compatibility during rollout.
  - Modals:
      - CreateCollectionModal and AddDataToCollectionModal:
          - Continue to ask for a directory path for now.
          - Build the new AddSourceRequest shape as above when calling useAddSource.
          - Optionally, surface source_type in the UI (e.g. label “Directory source”).
  - Operation & document displays:
      - ActiveOperationsTab / OperationProgress:
          - Keep reading config.source_path for now.
          - Optionally, enhance helper functions to:
              - Prefer config.source_config.path when present, falling back to
  config.source_path.
      - CollectionDetailsModal and useCollectionDocuments:
          - For now, continue aggregating on doc.source_path if exposed by the backend.
          - When the backend begins returning additional fields (e.g. uri or
  source_type), plan a follow‑up fix to show a more general “Source” column.
  - Tests:
      - Update Jest/Vitest tests that construct AddSourceRequest or assert on the
  request payload to include source_type/source_config where appropriate.
      - Keep tests validating that legacy shapes (only source_path) remain accepted.

  Acceptance Criteria:

  - Existing flows for adding a directory source (in both modals) still work without
  changes to user behavior.
  - The API payloads now include source_type='directory' and source_config={ path:
  '<dir>' } in addition to source_path, enabling the backend’s new model.
  - UI components that display source information continue to render correctly for
  directory sources.
  - Tests for useAddSource, CreateCollectionModal, and AddDataToCollectionModal pass
  with the updated request shape.
 Phase 6: Source Registry & Pipeline Hardening

  Goal: Fully wire flexible sources through the database, ensure source‑aware removal, and tighten hashing behavior.

  ———

  Ticket 10: Persist CollectionSource and Link Documents via source_id

  Context:
  The flexible source schema is in place (collection_sources.source_config, documents.uri/source_metadata), and APPEND
  operations now use connectors + DocumentRegistryService. However, no CollectionSource rows are created/updated when
  sources are added, and documents are registered with source_id=None. This prevents source‑level analytics, removal, and
  future multi‑connector scenarios.

  Files to analyze:

  - packages/shared/database/models.py (CollectionSource, Document)
  - packages/shared/database/repositories/collection_repository.py
  - packages/webui/services/collection_service.py
  - packages/webui/tasks/ingestion.py (_process_append_operation_impl)
  - packages/webui/services/document_registry_service.py
  - Tests:
      - tests/webui/services/test_collection_service.py::TestAddSource
      - tests/integration/services/test_collection_service.py::TestCollectionServiceIntegration
      - Any tests that assert on collection_sources or Document.source_id (if present)

  Requirements:

  - Repository layer:
      - Create a dedicated repository for CollectionSource, e.g. packages/shared/database/repositories/
        collection_source_repository.py:
          - async def get_or_create(...) to find or create a CollectionSource for (collection_id, source_type,
            source_path) (and optionally source_config).
          - Helper methods for future use: get_by_id, list_by_collection, etc., but keep the initial surface minimal.
  - CollectionService:
      - Inject CollectionSourceRepository into CollectionService via the service factory (packages/webui/services/
        factory.py).
      - In CollectionService.add_source(...):
          - After validating collection state, resolve or create a CollectionSource row:
              - collection_id from argument.
              - source_type from normalized input (default "directory").
              - source_path:
                  - For directory sources, derive from source_config["path"] or legacy_source_path.
                  - For non‑directory sources, choose an appropriate display identifier (e.g. source_config["url"]).
              - source_config persisted as JSON (full connector config).
          - Store the CollectionSource.id in the operation config:
              - Extend config to include source_id.
              - Keep source_path and source_config in config for backward compatibility and display.
  - Ingestion task:
      - In _process_append_operation_impl:
          - Read source_id from operation["config"] when present.
          - For legacy operations lacking source_id:
              - Option 1 (preferred): Resolve CollectionSource by (collection_id, source_type, source_path) using
                CollectionSourceRepository.
              - Option 2: Allow source_id to remain None but document that legacy operations are not source‑linked.
          - Pass source_id into DocumentRegistryService.register(...) so all new documents are associated with their
            source.
  - DocumentRegistryService:
      - Update docstring and internal comments to clarify that source_id is now the canonical link between documents
        and CollectionSource.
      - No behavioral change required beyond taking source_id from the new call sites (already supported).

  Acceptance Criteria:

  - CollectionService.add_source(...):
      - Creates (or reuses) a CollectionSource with:
          - collection_id matching the target collection.
          - source_type and source_path consistent with the request.
          - source_config persisted as the connector config.
      - Stores source_id in the operation’s config.
  - _process_append_operation_impl:
      - Retrieves source_id from operation config when available and passes it to DocumentRegistryService.register(...).
      - For legacy operations without source_id, behavior remains backward‑compatible (documents can still be ingested).
  - Newly ingested documents for APPEND operations have Document.source_id set to the correct CollectionSource.id.
  - Tests:
      - Update tests/webui/services/test_collection_service.py::TestAddSource to assert that:
          - CollectionSource is created/updated with the expected fields.
          - Operation config includes source_id.
      - Update tests/integration/services/
        test_collection_service.py::test_add_source_creates_append_operation_and_sets_processing_status to use the new
        add_source signature and to validate CollectionSource persistence and source_id in operation config.
      - Add/extend tests to confirm that documents created via _process_append_operation_impl have source_id populated.

  ———

  Ticket 11: Restore and Modernize REMOVE_SOURCE Behavior Using Source Registry

  Context:
  _process_remove_source_operation currently calls document_repo.list_by_collection_and_source(collection["id"],
  source_path), but DocumentRepository no longer exposes this method. At the same time, APPEND now routes through
  DocumentRegistryService with the intent to attach documents to CollectionSource via source_id. REMOVE_SOURCE should
  rely on the source registry instead of raw path filtering, while preserving backward compatibility for legacy
  operations that only know source_path.

  Files to analyze:

  - packages/webui/tasks/ingestion.py (_process_remove_source_operation)
  - packages/shared/database/repositories/document_repository.py
  - packages/shared/database/repositories/collection_source_repository.py (from Ticket 10)
  - packages/shared/database/models.py (Document.source_id, CollectionSource)
  - Tests:
      - tests/integration/services/test_collection_service.py::test_remove_source_creates_remove_operation
      - Any existing REMOVE_SOURCE–related tests in tests/webui/services/test_collection_service.py
      - E2E tests that exercise REMOVE_SOURCE (if present)

  Requirements:

  - Repository/API for locating documents by source:
      - Add a method to DocumentRepository, e.g.:
          - async def list_by_collection_and_source_id(collection_id: str, source_id: int) -> list[dict[str, Any]] or a
            typed variant returning Document instances.
      - Optionally, provide a compatibility helper to list documents for a collection by legacy source_path, using
        file_path prefix and/or uri.
  - Remove source task:
      - Update _process_remove_source_operation to:
          - Read source_id from operation["config"] when present.
          - If source_id is present:
              - Use DocumentRepository to list documents by (collection_id, source_id) instead of source_path.
          - If source_id is not present (legacy operation):
              - Fallback to a path‑based filter:
                  - Either implement a dedicated repository method using file_path.startswith(source_path) in SQL,
                  - Or, if that’s too invasive, document/re‑introduce a minimal helper to approximate the previous
                    behavior.
      - Ensure all downstream logging and auditing continues to use source_path (from config) for user‑visible messages,
        but you may enrich logs with source_id for debugging.
  - Qdrant deletion logic:
      - No change in Qdrant interaction structure; the only change is how documents are fetched.
      - The list of doc_ids must be derived from the new document selection (by source_id or path).
  - Operation config and metrics:
      - Preserve existing operation config fields (source_path) so frontend and audit logs remain valid.
      - If source_id is present in config, ensure it is not removed or overwritten.

  Acceptance Criteria:

  - _process_remove_source_operation no longer references document_repo.list_by_collection_and_source (which doesn’t
    exist).
  - For operations created after Ticket 10:
      - Documents to be removed are selected by collection_id + source_id (via the new repository method).
      - Only documents belonging to the specified CollectionSource are deleted from Qdrant and the DB.
  - For legacy REMOVE_SOURCE operations without source_id:
      - The task still works using a path‑based filter that is behaviorally consistent with the pre‑refactor
        implementation.
  - Tests:
      - Add unit tests for the new repository method(s) (or adapt existing contract tests) to ensure selection by
        source_id behaves as expected.
      - Extend/integrate tests to cover:
          - REMOVE_SOURCE with a source_id‑backed operation (new path).
          - REMOVE_SOURCE with legacy source_path‑only config (compatibility path).
      - Existing integration tests for REMOVE_SOURCE continue to pass (or are updated to assert the new source‑aware
        behavior).

  ———

  Ticket 12: Unify Hashing Logic in DocumentScanningService with compute_content_hash

  Context:
  Connectors and the new ingestion path use shared.utils.hashing.compute_content_hash for SHA‑256 content hashing, and
  IngestedDocument/DocumentRepository.create enforce hash validity. However, DocumentScanningService._calculate_file_hash
  still implements hashing directly with hashlib.sha256(). While the behavior matches, this duplicates logic and
  increases the chance of future divergence.

  Files to analyze:

  - packages/webui/services/document_scanning_service.py
  - packages/shared/utils/hashing.py
  - Tests:
      - tests/unit/test_hashing.py
      - tests/integration/services/test_document_scanning_service.py

  Requirements:

  - Refactor DocumentScanningService._calculate_file_hash to delegate to compute_content_hash:
      - Keep streaming file reads (to avoid loading large files entirely into memory).
      - Instead of manually assembling a hex digest with hashlib, feed each chunk into a SHA‑256 object and then:
          - Either refactor compute_content_hash to accept an existing hashlib object (if you want to reuse exactly one
            implementation), or
          - Wrap the streaming logic with a tiny helper that uses compute_content_hash on aggregated bytes when file
            sizes are below a safe threshold, and falls back to a manual SHA‑256 only for very large files.
      - Document the chosen approach in a short docstring comment so future refactors know why streaming is preserved.
  - Ensure any behavior that depends on hash stability (dedupe, tests) is unaffected:
      - Hash values for a given file content MUST remain identical to the previous implementation.

  Acceptance Criteria:

  - DocumentScanningService no longer hard‑codes its own SHA‑256 implementation; it uses compute_content_hash directly or
    via a clearly documented wrapper.
  - All existing tests related to hashing and scanning:
      - tests/unit/test_hashing.py
      - tests/integration/services/test_document_scanning_service.py
        continue to pass without modification (hash outputs remain unchanged).
  - New or updated tests confirm that:
      - Hashes produced by DocumentScanningService match compute_content_hash for sample files (including multi‑byte and
        larger content).
      - No noticeable performance regression occurs for large files (streaming behavior retained where necessary).
