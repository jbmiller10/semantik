     1	"""WebSocket manager with Redis Streams for distributed state synchronization."""
     2	
     3	import asyncio
     4	import contextlib
     5	import json
     6	import logging
     7	import uuid
     8	from datetime import UTC, datetime
     9	from typing import Any
    10	
    11	import redis.asyncio as aioredis
    12	
    13	# Make redis available at module level for backward compatibility with tests
    14	import redis.asyncio as redis
    15	from fastapi import WebSocket
    16	
    17	from packages.webui.services.progress_manager import (
    18	    ProgressPayload,
    19	    ProgressSendResult,
    20	    ProgressUpdateManager,
    21	)
    22	
    23	logger = logging.getLogger(__name__)
    24	
    25	
    26	class RedisStreamWebSocketManager:
    27	    """WebSocket manager that uses Redis Streams for distributed state synchronization."""
    28	
    29	    def __init__(self, progress_manager: ProgressUpdateManager | None = None) -> None:
    30	        """Initialize the WebSocket manager."""
    31	        self.redis: aioredis.Redis | None = None
    32	        self.connections: dict[str, set[WebSocket]] = {}
    33	        self.consumer_tasks: dict[str, asyncio.Task] = {}
    34	        self.consumer_group = f"webui-{uuid.uuid4().hex[:8]}"
    35	        self.max_connections_per_user = 10  # Prevent DOS attacks per user
    36	        self.max_total_connections = 1000  # Global connection limit
    37	        self._startup_lock = asyncio.Lock()
    38	        self._startup_attempted = False
    39	        self._get_operation_func = None  # Function to get operation by ID
    40	        self._chunking_progress_throttle: dict[str, datetime] = {}  # Track last progress update time per operation
    41	        self._chunking_progress_threshold = 0.5  # Minimum seconds between progress updates
    42	        self._progress_manager = progress_manager
    43	
    44	        # Add locks for thread-safe access to shared state
    45	        self._connections_lock = asyncio.Lock()
    46	        self._consumer_tasks_lock = asyncio.Lock()
    47	        self._throttle_lock = asyncio.Lock()
    48	
    49	    async def startup(self) -> None:
    50	        """Initialize Redis connection on application startup with retry logic.
    51	
    52	        Supports both the service factory path and direct redis.from_url for
    53	        backward compatibility with tests that patch either mechanism.
    54	        """
    55	        async with self._startup_lock:
    56	            # Skip if already attempted or connected
    57	            if self._startup_attempted and self.redis is not None:
    58	                return
    59	
    60	            self._startup_attempted = True
    61	            logger.info("WebSocket manager startup initiated")
    62	            max_retries = 3
    63	            retry_delay = 1.0  # Initial delay in seconds
    64	
    65	            for attempt in range(max_retries):
    66	                try:
    67	                    logger.info(f"Attempting to connect to Redis (attempt {attempt + 1}/{max_retries})")
    68	
    69	                    # Always use from_url with retries; tests patch this call
    70	                    from packages.shared.config import settings as _settings
    71	
    72	                    redis_url = getattr(_settings, "REDIS_URL", "redis://localhost:6379/0")
    73	                    redis_client = await redis.from_url(redis_url, decode_responses=True)
    74	
    75	                    # Validate connection
    76	                    await redis_client.ping()
    77	                    self.redis = redis_client
    78	
    79	                    if self._progress_manager is None:
    80	                        self._progress_manager = ProgressUpdateManager(
    81	                            async_redis=redis_client,
    82	                            default_stream_template="operation-progress:{operation_id}",
    83	                            default_ttl=86400,
    84	                            default_maxlen=1000,
    85	                            async_throttle_interval=self._chunking_progress_threshold,
    86	                            logger_=logger.getChild("progress"),
    87	                        )
    88	                    else:
    89	                        self._progress_manager.set_async_client(redis_client)
    90	
    91	                    logger.info("WebSocket manager connected to Redis")
    92	                    return
    93	                except Exception as e:
    94	                    if attempt < max_retries - 1:
    95	                        wait_time = retry_delay * (2**attempt)  # Exponential backoff
    96	                        logger.warning(
    97	                            f"Failed to connect to Redis (attempt {attempt + 1}/{max_retries}): {e}. "
    98	                            f"Retrying in {wait_time:.1f} seconds..."
    99	                        )
   100	                        await asyncio.sleep(wait_time)
   101	                    else:
   102	                        logger.error(f"Failed to connect to Redis after {max_retries} attempts: {e}")
   103	                        # Don't raise - allow graceful degradation
   104	                        self.redis = None
   105	
   106	    async def shutdown(self) -> None:
   107	        """Clean up resources on application shutdown."""
   108	        # Cancel all consumer tasks
   109	        async with self._consumer_tasks_lock:
   110	            tasks_to_cancel = list(self.consumer_tasks.items())
   111	
   112	        for operation_id, task in tasks_to_cancel:
   113	            logger.info(f"Cancelling consumer task for operation {operation_id}")
   114	            task.cancel()
   115	            with contextlib.suppress(asyncio.CancelledError):
   116	                await task
   117	
   118	        # Close all WebSocket connections
   119	        async with self._connections_lock:
   120	            connections_to_close = list(self.connections.items())
   121	
   122	        for _, websockets in connections_to_close:
   123	            for websocket in list(websockets):
   124	                with contextlib.suppress(Exception):
   125	                    await websocket.close()
   126	
   127	        # Clear the dictionaries after cleanup
   128	        async with self._connections_lock:
   129	            self.connections.clear()
   130	        async with self._consumer_tasks_lock:
   131	            self.consumer_tasks.clear()
   132	
   133	        # Close Redis connection
   134	        if self.redis:
   135	            await self.redis.close()
   136	            logger.info("WebSocket manager Redis connection closed")
   137	
   138	    async def _record_throttle_timestamp(self, operation_id: str, timestamp: datetime) -> None:
   139	        """Update the throttle bookkeeping for chunking progress events."""
   140	
   141	        async with self._throttle_lock:
   142	            self._chunking_progress_throttle[operation_id] = timestamp
   143	
   144	    @staticmethod
   145	    def _resolve_stream_ttl(update_type: str, data: dict[str, Any]) -> int:
   146	        """Determine TTL to apply to a Redis stream entry for *update_type*."""
   147	
   148	        ttl = 86400
   149	        if update_type == "status_update":
   150	            status = data.get("status", "")
   151	            if status in {"completed", "cancelled"}:
   152	                ttl = 300
   153	            elif status == "failed":
   154	                ttl = 60
   155	        return ttl
   156	
   157	    def set_operation_getter(self, get_operation_func: Any) -> None:
   158	        """Set the function to get operation by ID.
   159	
   160	        This allows dependency injection for testing.
   161	        The function should be an async function that takes an operation_id
   162	        and returns an operation object or None.
   163	        """
   164	        self._get_operation_func = get_operation_func
   165	
   166	    async def connect(self, websocket: WebSocket, operation_id: str, user_id: str) -> None:
   167	        """Handle new WebSocket connection for operation updates with connection limit enforcement."""
   168	        # Try to reconnect to Redis if not connected
   169	        if self.redis is None:
   170	            logger.info("Redis not connected, attempting to reconnect...")
   171	            await self.startup()
   172	
   173	        # Variables to track if we should accept the connection
   174	        should_accept = False
   175	        user_connections = 0
   176	        reject_reason = ""
   177	
   178	        # Check limits and add connection atomically
   179	        async with self._connections_lock:
   180	            total_connections = sum(len(sockets) for sockets in self.connections.values())
   181	            if total_connections >= self.max_total_connections:
   182	                logger.error(f"Global connection limit reached ({self.max_total_connections})")
   183	                should_accept = False
   184	                reject_reason = "Server connection limit exceeded"
   185	            else:
   186	                # Check connection limit for this user
   187	                user_connections = sum(
   188	                    len(sockets) for key, sockets in self.connections.items() if key.startswith(f"{user_id}:")
   189	                )
   190	
   191	                if user_connections >= self.max_connections_per_user:
   192	                    logger.warning(f"User {user_id} exceeded connection limit ({self.max_connections_per_user})")
   193	                    should_accept = False
   194	                    reject_reason = "User connection limit exceeded"
   195	                else:
   196	                    should_accept = True
   197	                    # Accept and store connection while still holding the lock
   198	                    await websocket.accept()
   199	
   200	                    key = f"{user_id}:operation:{operation_id}"
   201	                    if key not in self.connections:
   202	                        self.connections[key] = set()
   203	                    self.connections[key].add(websocket)
   204	
   205	        # If we didn't accept, close the connection outside the lock
   206	        if not should_accept:
   207	            await websocket.close(code=1008, reason=reject_reason)
   208	            raise ConnectionRefusedError(reject_reason)
   209	
   210	        logger.info(
   211	            f"Operation WebSocket connected: user={user_id}, operation={operation_id} "
   212	            f"(total user connections: {user_connections + 1})"
   213	        )
   214	
   215	        # Get current operation state from database and send it
   216	        try:
   217	            operation = None
   218	
   219	            if self._get_operation_func:
   220	                # Use injected function (for testing)
   221	                operation = await self._get_operation_func(operation_id)
   222	            else:
   223	                # Use default implementation
   224	                from packages.shared.database.database import AsyncSessionLocal
   225	                from packages.shared.database.repositories.operation_repository import OperationRepository
   226	
   227	                async with AsyncSessionLocal() as session:  # type: ignore[misc]
   228	                    operation_repo = OperationRepository(session)
   229	                    operation = await operation_repo.get_by_uuid(operation_id)
   230	
   231	            if operation:
   232	                # Send current state
   233	                state_message = {
   234	                    "timestamp": datetime.now(UTC).isoformat(),
   235	                    "type": "current_state",
   236	                    "data": {
   237	                        "status": operation.status.value,
   238	                        "operation_type": operation.type.value,
   239	                        "created_at": operation.created_at.isoformat(),
   240	                        "started_at": operation.started_at.isoformat() if operation.started_at else None,
   241	                        "completed_at": operation.completed_at.isoformat() if operation.completed_at else None,
   242	                        "error_message": operation.error_message,
   243	                    },
   244	                }
   245	                await websocket.send_json(state_message)
   246	                logger.info(f"Sent current state to client for operation {operation_id}")
   247	        except Exception as e:
   248	            logger.error(f"Failed to send current state for operation {operation_id}: {e}")
   249	
   250	        # Only start Redis consumer if Redis is available
   251	        if self.redis is not None:
   252	            # Start consumer task if not exists
   253	            async with self._consumer_tasks_lock:
   254	                if operation_id not in self.consumer_tasks:
   255	                    task = asyncio.create_task(self._consume_updates(operation_id))
   256	                    self.consumer_tasks[operation_id] = task
   257	                    logger.info(f"Started consumer task for operation {operation_id}")
   258	
   259	            # Send message history
   260	            await self._send_history(websocket, operation_id)
   261	        else:
   262	            logger.warning(
   263	                f"Redis not available for operation {operation_id}. WebSocket will work in degraded mode "
   264	                "(initial state only, no real-time updates)"
   265	            )
   266	
   267	    async def disconnect(self, websocket: WebSocket, operation_id: str, user_id: str) -> None:
   268	        """Handle WebSocket disconnection for operation updates."""
   269	        key = f"{user_id}:operation:{operation_id}"
   270	        async with self._connections_lock:
   271	            if key in self.connections:
   272	                self.connections[key].discard(websocket)
   273	                if not self.connections[key]:
   274	                    del self.connections[key]
   275	
   276	            # Check if there are any more connections for this operation
   277	            has_connections = any(operation_id in k for k in self.connections)
   278	
   279	        logger.info(f"Operation WebSocket disconnected: user={user_id}, operation={operation_id}")
   280	
   281	        # Stop consumer if no more connections for this operation
   282	        if not has_connections:
   283	            async with self._consumer_tasks_lock:
   284	                if operation_id in self.consumer_tasks:
   285	                    logger.info(f"Stopping consumer task for operation {operation_id} (no more connections)")
   286	                    task = self.consumer_tasks[operation_id]
   287	                    task.cancel()
   288	                    with contextlib.suppress(asyncio.CancelledError):
   289	                        await task
   290	                    if operation_id in self.consumer_tasks:
   291	                        del self.consumer_tasks[operation_id]
   292	
   293	    async def send_update(
   294	        self,
   295	        operation_id: str,
   296	        update_type: str,
   297	        data: dict,
   298	        *,
   299	        throttle: bool = False,
   300	        throttle_key: str | None = None,
   301	    ) -> None:
   302	        """Send an update to Redis Stream for a specific operation.
   303	
   304	        If Redis is unavailable or publishing fails, updates are sent directly to
   305	        connected WebSocket clients as a fallback.
   306	        """
   307	
   308	        payload_timestamp = datetime.now(UTC)
   309	        message = {"timestamp": payload_timestamp.isoformat(), "type": update_type, "data": data}
   310	        manager = self._progress_manager
   311	        publish_result: ProgressSendResult | None = None
   312	
   313	        if manager is not None and self.redis is not None:
   314	            payload = ProgressPayload(
   315	                operation_id=operation_id,
   316	                correlation_id=data.get("correlation_id"),
   317	                status=data.get("status"),
   318	                message=data.get("message"),
   319	                progress=data.get("progress_percentage") or data.get("progress"),
   320	                extra={"type": update_type},
   321	                timestamp=payload_timestamp,
   322	            )
   323	
   324	            ttl = self._resolve_stream_ttl(update_type, data)
   325	            publish_result = await manager.send_async_update(
   326	                payload,
   327	                stream_fields={"message": json.dumps(message)},
   328	                stream_template="operation-progress:{operation_id}",
   329	                ttl=ttl,
   330	                maxlen=1000,
   331	                use_throttle=throttle,
   332	                throttle_key=throttle_key or operation_id,
   333	                redis_client=self.redis,
   334	            )
   335	
   336	            if publish_result is ProgressSendResult.SENT:
   337	                if throttle:
   338	                    await self._record_throttle_timestamp(operation_id, payload.timestamp)
   339	                logger.debug(
   340	                    "Sent update to stream operation-progress:%s: type=%s, TTL=%ss",
   341	                    operation_id,
   342	                    update_type,
   343	                    ttl,
   344	                )
   345	                return
   346	
   347	            if publish_result is ProgressSendResult.SKIPPED:
   348	                return
   349	
   350	        if throttle:
   351	            await self._record_throttle_timestamp(operation_id, payload_timestamp)
   352	
   353	        if publish_result is ProgressSendResult.FAILED:
   354	            logger.error("Failed to send update to Redis stream for %s", operation_id)
   355	        elif manager is None or self.redis is None:
   356	            logger.debug("Redis not available, broadcasting directly for operation %s", operation_id)
   357	
   358	        await self._broadcast(operation_id, message)
   359	
   360	    async def _consume_updates(self, operation_id: str) -> None:
   361	        """Consume updates from Redis Stream for a specific operation."""
   362	        stream_key = f"operation-progress:{operation_id}"
   363	        consumer_name = f"consumer-{operation_id}"
   364	        consumer_group_created = False
   365	
   366	        try:
   367	            while True:
   368	                try:
   369	                    if self.redis is None:
   370	                        raise RuntimeError("Redis connection not established")
   371	
   372	                    # Try to create consumer group if not already created
   373	                    if not consumer_group_created:
   374	                        try:
   375	                            # First check if the stream exists
   376	                            try:
   377	                                stream_info = await self.redis.xinfo_stream(stream_key)
   378	                                logger.debug(f"Stream {stream_key} exists with {stream_info.get('length', 0)} messages")
   379	                            except Exception:
   380	                                # Stream doesn't exist - this is normal for operations that haven't started yet
   381	                                logger.debug(
   382	                                    f"Stream {stream_key} does not exist yet - waiting for worker to create it"
   383	                                )
   384	                                # Wait a bit and continue the loop
   385	                                await asyncio.sleep(2)
   386	                                continue
   387	
   388	                            # Stream exists, try to create consumer group
   389	                            await self.redis.xgroup_create(stream_key, self.consumer_group, id="0")
   390	                            logger.info(f"Created consumer group {self.consumer_group} for stream {stream_key}")
   391	                            consumer_group_created = True
   392	                        except Exception as e:
   393	                            # Group might already exist
   394	                            if "BUSYGROUP" in str(e):
   395	                                logger.debug(f"Consumer group already exists for {stream_key}")
   396	                                consumer_group_created = True
   397	                            else:
   398	                                logger.debug(f"Could not create consumer group: {e}")
   399	                                await asyncio.sleep(2)
   400	                                continue
   401	
   402	                    # Read from stream with blocking
   403	                    last_id = ">"  # Start reading new messages
   404	                    messages = await self.redis.xreadgroup(
   405	                        self.consumer_group,
   406	                        consumer_name,
   407	                        {stream_key: last_id},
   408	                        count=10,
   409	                        block=1000,  # 1 second timeout
   410	                    )
   411	
   412	                    if messages:
   413	                        for _, stream_messages in messages:
   414	                            for msg_id, data in stream_messages:
   415	                                try:
   416	                                    # Parse message
   417	                                    message = json.loads(data["message"])
   418	
   419	                                    # Send to all connected clients for this operation
   420	                                    await self._broadcast(operation_id, message)
   421	
   422	                                    # Check if operation is complete and close connections
   423	                                    if message.get("type") == "status_update" and message.get("data", {}).get(
   424	                                        "status"
   425	                                    ) in ["completed", "failed", "cancelled"]:
   426	                                        logger.info(
   427	                                            f"Operation {operation_id} completed, closing WebSocket connections"
   428	                                        )
   429	                                        await self._close_connections(operation_id)
   430	
   431	                                    # Acknowledge message
   432	                                    await self.redis.xack(stream_key, self.consumer_group, msg_id)
   433	
   434	                                    logger.debug(f"Processed message {msg_id} for operation {operation_id}")
   435	                                except Exception as e:
   436	                                    logger.error(f"Error processing message {msg_id}: {e}")
   437	
   438	                    await asyncio.sleep(0.1)  # Small delay between reads
   439	
   440	                except asyncio.CancelledError:
   441	                    # Clean up consumer
   442	                    try:
   443	                        if self.redis is not None:
   444	                            await self.redis.xgroup_delconsumer(stream_key, self.consumer_group, consumer_name)
   445	                            logger.info(f"Cleaned up consumer {consumer_name}")
   446	                    except Exception:
   447	                        pass
   448	                    raise
   449	                except Exception as e:
   450	                    error_str = str(e)
   451	                    if "NOGROUP" in error_str:
   452	                        # Consumer group doesn't exist anymore, reset flag
   453	                        consumer_group_created = False
   454	                        logger.debug(f"Consumer group lost for {stream_key}, will recreate...")
   455	                        await asyncio.sleep(2)
   456	                    elif "Stream" in error_str and "does not exist" in error_str:
   457	                        # Stream doesn't exist yet
   458	                        logger.debug(f"Stream {stream_key} not ready yet, waiting...")
   459	                        await asyncio.sleep(2)
   460	                    elif "no running event loop" in error_str.lower():
   461	                        # Event loop has been closed, exit gracefully
   462	                        logger.debug(f"Event loop closed for operation {operation_id}, exiting consumer")
   463	                        return
   464	                    else:
   465	                        logger.error(f"Error in consumer loop for operation {operation_id}: {e}")
   466	                        await asyncio.sleep(5)  # Wait before retry
   467	
   468	        except asyncio.CancelledError:
   469	            logger.info(f"Consumer task cancelled for operation {operation_id}")
   470	            raise
   471	        except RuntimeError as e:
   472	            if "no running event loop" in str(e).lower():
   473	                logger.debug(f"Event loop closed for operation {operation_id}, exiting consumer")
   474	            else:
   475	                logger.error(f"Fatal runtime error in consumer for operation {operation_id}: {e}")
   476	        except Exception as e:
   477	            logger.error(f"Fatal error in consumer for operation {operation_id}: {e}")
   478	
   479	    async def _send_history(self, websocket: WebSocket, operation_id: str) -> None:
   480	        """Send historical messages to newly connected client for operation."""
   481	        if not self.redis:
   482	            logger.debug("Redis not available, skipping message history")
   483	            return
   484	
   485	        stream_key = f"operation-progress:{operation_id}"
   486	
   487	        try:
   488	            # Read last 100 messages
   489	            messages = await self.redis.xrange(stream_key, min="-", max="+", count=100)
   490	
   491	            for _, data in messages:
   492	                try:
   493	                    message = json.loads(data["message"])
   494	                    await websocket.send_json(message)
   495	                except Exception as e:
   496	                    logger.warning(f"Failed to send historical message: {e}")
   497	
   498	            if messages:
   499	                logger.info(f"Sent {len(messages)} historical messages to client for operation {operation_id}")
   500	
   501	        except Exception as e:
   502	            logger.warning(f"Failed to send history for operation {operation_id}: {e}")
   503	
   504	    async def _broadcast(self, operation_id: str, message: dict) -> None:
   505	        """Broadcast message to all connections for an operation."""
   506	        disconnected = []
   507	
   508	        # Get a snapshot of connections to send to
   509	        async with self._connections_lock:
   510	            connections_snapshot = [
   511	                (key, list(websockets))
   512	                for key, websockets in self.connections.items()
   513	                if f"operation:{operation_id}" in key
   514	            ]
   515	
   516	        # Send messages outside the lock to avoid blocking
   517	        for key, websockets in connections_snapshot:
   518	            for websocket in websockets:
   519	                try:
   520	                    await websocket.send_json(message)
   521	                except Exception as e:
   522	                    logger.warning(f"Failed to send message to websocket: {e}")
   523	                    disconnected.append((key, websocket))
   524	
   525	        # Clean up disconnected clients
   526	        if disconnected:
   527	            async with self._connections_lock:
   528	                for key, websocket in disconnected:
   529	                    if key in self.connections:
   530	                        self.connections[key].discard(websocket)
   531	                        if not self.connections[key]:
   532	                            del self.connections[key]
   533	
   534	    async def _close_connections(self, operation_id: str) -> None:
   535	        """Close all WebSocket connections for a completed operation."""
   536	        connections_to_close: list[WebSocket] = []
   537	
   538	        # Get connections and remove them atomically
   539	        async with self._connections_lock:
   540	            keys_to_remove = []
   541	            for key, websockets in self.connections.items():
   542	                if f"operation:{operation_id}" in key:
   543	                    connections_to_close.extend(websockets)
   544	                    keys_to_remove.append(key)
   545	
   546	            # Remove the connections while still holding the lock
   547	            for key in keys_to_remove:
   548	                del self.connections[key]
   549	
   550	        # Close connections outside the lock to avoid blocking
   551	        for websocket in connections_to_close:
   552	            try:
   553	                await websocket.close(code=1000, reason="Operation completed")
   554	                logger.debug(f"Closed WebSocket connection for completed operation {operation_id}")
   555	            except Exception as e:
   556	                logger.warning(f"Failed to close WebSocket connection: {e}")
   557	
   558	    async def cleanup_stream(self, operation_id: str) -> None:
   559	        """Clean up Redis stream for a completed operation.
   560	
   561	        This should be called when an operation is completed or deleted to free up Redis memory.
   562	        """
   563	        if not self.redis:
   564	            logger.debug("Redis not available, skipping stream cleanup")
   565	            return
   566	
   567	        stream_key = f"operation-progress:{operation_id}"
   568	
   569	        try:
   570	            # Delete the stream
   571	            deleted = await self.redis.delete(stream_key)
   572	            if deleted:
   573	                logger.info(f"Cleaned up Redis stream for operation {operation_id}")
   574	
   575	            # Also try to delete consumer groups associated with this stream
   576	            try:
   577	                # Get all consumer groups for this stream
   578	                groups = await self.redis.xinfo_groups(stream_key)
   579	                for group in groups:
   580	                    group_name = group.get("name", "")
   581	                    if group_name:
   582	                        await self.redis.xgroup_destroy(stream_key, group_name)
   583	                        logger.debug(f"Deleted consumer group {group_name} for operation {operation_id}")
   584	            except Exception:
   585	                # Stream might already be deleted or have no groups
   586	                pass
   587	
   588	        except Exception as e:
   589	            logger.warning(f"Failed to clean up stream for operation {operation_id}: {e}")
   590	
   591	    async def cleanup_operation_channel(self, operation_id: str) -> None:
   592	        """Clean up all resources for a completed operation.
   593	
   594	        Args:
   595	            operation_id: The operation ID to clean up
   596	        """
   597	        # Cancel consumer task if exists
   598	        async with self._consumer_tasks_lock:
   599	            if operation_id in self.consumer_tasks:
   600	                task = self.consumer_tasks[operation_id]
   601	                task.cancel()
   602	                with contextlib.suppress(asyncio.CancelledError):
   603	                    await task
   604	                del self.consumer_tasks[operation_id]
   605	
   606	        # Clean up Redis stream
   607	        await self.cleanup_stream(operation_id)
   608	
   609	        # Close all connections for this operation
   610	        await self._close_connections(operation_id)
   611	
   612	        logger.info(f"Cleaned up all resources for operation {operation_id}")
   613	
   614	    async def broadcast_to_operation(
   615	        self,
   616	        operation_id: str,
   617	        message: dict,
   618	    ) -> None:
   619	        """Broadcast a message to all connections for an operation.
   620	
   621	        Args:
   622	            operation_id: The operation ID
   623	            message: The message to broadcast
   624	        """
   625	        await self._broadcast(operation_id, message)
   626	
   627	    async def _should_send_progress_update(
   628	        self,
   629	        operation_id: str,
   630	        message: dict,
   631	    ) -> bool:
   632	        """Check if a progress update should be sent based on throttling.
   633	
   634	        Args:
   635	            operation_id: The operation ID
   636	            message: The message to check
   637	
   638	        Returns:
   639	            True if the message should be sent, False if throttled
   640	        """
   641	        # Check if this is a progress update
   642	        if message.get("type") != "chunking_progress":
   643	            return True  # Non-progress messages always go through
   644	
   645	        # Check throttling with lock for thread safety
   646	        async with self._throttle_lock:
   647	            now = datetime.now(UTC)
   648	            if operation_id in self._chunking_progress_throttle:
   649	                time_since_last = (now - self._chunking_progress_throttle[operation_id]).total_seconds()
   650	                if time_since_last < self._chunking_progress_threshold:
   651	                    # Too soon after last update, throttle it
   652	                    return False
   653	
   654	            # Update timestamp for next check
   655	            self._chunking_progress_throttle[operation_id] = now
   656	            return True
   657	
   658	    async def cleanup_stale_connections(self) -> None:
   659	        """Clean up stale WebSocket connections and their associated data.
   660	
   661	        This is called periodically to remove dead connections and free memory.
   662	        """
   663	        cleaned_count = 0
   664	
   665	        # Get a snapshot of connections to test
   666	        async with self._connections_lock:
   667	            connections_snapshot = list(self.connections.items())
   668	
   669	        dead_connections = []  # Track (key, websocket) pairs to remove
   670	
   671	        for key, websockets in connections_snapshot:
   672	            if not websockets:
   673	                continue
   674	
   675	            for websocket in list(websockets):
   676	                try:
   677	                    # For testing with mocks, check if this is a mock object
   678	                    # and call ping() if available (mock specific)
   679	                    if hasattr(websocket, "ping"):
   680	                        # This is likely a mock in tests
   681	                        await websocket.ping()
   682	                    else:
   683	                        # Try to send a ping frame to check if connection is alive
   684	                        # FastAPI WebSocket doesn't have a ping() method, so we use send_json
   685	                        # with a ping message and handle any exceptions
   686	                        await asyncio.wait_for(websocket.send_json({"type": "ping"}), timeout=1.0)
   687	                except Exception:
   688	                    # Connection is dead or timed out
   689	                    dead_connections.append((key, websocket))
   690	                    cleaned_count += 1
   691	
   692	        # Remove dead sockets from the connections dictionary
   693	        if dead_connections:
   694	            async with self._connections_lock:
   695	                for key, websocket in dead_connections:
   696	                    if key in self.connections:
   697	                        self.connections[key].discard(websocket)
   698	                        if not self.connections[key]:
   699	                            # Remove empty connection sets
   700	                            del self.connections[key]
   701	
   702	        if cleaned_count > 0:
   703	            logger.info(f"Cleaned up {cleaned_count} stale WebSocket connections")
   704	
   705	        # Clean up old progress throttle entries
   706	        async with self._throttle_lock:
   707	            now = datetime.now(UTC)
   708	            old_entries = [
   709	                op_id
   710	                for op_id, last_time in self._chunking_progress_throttle.items()
   711	                if (now - last_time).total_seconds() > 300  # Remove entries older than 5 minutes
   712	            ]
   713	
   714	            for op_id in old_entries:
   715	                del self._chunking_progress_throttle[op_id]
   716	
   717	        if old_entries:
   718	            logger.debug(f"Cleaned up {len(old_entries)} old progress throttle entries")
   719	
   720	    async def send_chunking_progress(
   721	        self,
   722	        operation_id: str,
   723	        progress_percentage: float,
   724	        documents_processed: int,
   725	        total_documents: int,
   726	        chunks_created: int,
   727	        current_document: str | None = None,
   728	        throttle: bool = True,
   729	    ) -> None:
   730	        """Send chunking progress update with optional throttling.
   731	
   732	        Args:
   733	            operation_id: The chunking operation ID
   734	            progress_percentage: Completion percentage (0-100)
   735	            documents_processed: Number of documents processed
   736	            total_documents: Total documents to process
   737	            chunks_created: Number of chunks created so far
   738	            current_document: Currently processing document name
   739	            throttle: Whether to throttle progress updates
   740	        """
   741	        await self.send_update(
   742	            operation_id,
   743	            "chunking_progress",
   744	            {
   745	                "progress_percentage": progress_percentage,
   746	                "documents_processed": documents_processed,
   747	                "total_documents": total_documents,
   748	                "chunks_created": chunks_created,
   749	                "current_document": current_document,
   750	            },
   751	            throttle=throttle,
   752	            throttle_key=operation_id,
   753	        )
   754	
   755	    async def send_chunking_event(
   756	        self,
   757	        operation_id: str,
   758	        event_type: str,
   759	        data: dict,
   760	    ) -> None:
   761	        """Send a chunking-specific event.
   762	
   763	        Supported event types:
   764	        - chunking_started: Chunking operation has started
   765	        - chunking_document_start: Started processing a document
   766	        - chunking_document_complete: Completed processing a document
   767	        - chunking_completed: Entire chunking operation completed
   768	        - chunking_failed: Chunking operation failed
   769	        - chunking_cancelled: Chunking operation was cancelled
   770	        - chunking_strategy_changed: Chunking strategy was changed
   771	
   772	        Args:
   773	            operation_id: The chunking operation ID
   774	            event_type: Type of chunking event
   775	            data: Event-specific data
   776	        """
   777	        await self.send_update(operation_id, event_type, data)
   778	
   779	    async def send_message(self, channel: str, message: dict) -> None:
   780	        """Send a message to a specific channel.
   781	
   782	        This is used for custom WebSocket channels like chunking operations.
   783	
   784	        Args:
   785	            channel: The channel identifier (e.g., "chunking:collection_id:operation_id")
   786	            message: The message to send
   787	        """
   788	        if self.redis:
   789	            stream_key = f"stream:{channel}"
   790	
   791	            try:
   792	                # Add to stream with automatic ID and proper maxlen
   793	                await self.redis.xadd(
   794	                    stream_key,
   795	                    {"message": json.dumps(message)},
   796	                    maxlen=1000,  # Increased to 1000 for consistency
   797	                )
   798	
   799	                # Set TTL for WebSocket channel streams (15 minutes)
   800	                await self.redis.expire(stream_key, 900)
   801	
   802	                logger.debug(f"Sent message to channel {channel}, TTL=900s")
   803	            except Exception as e:
   804	                logger.error(f"Failed to send message to channel: {e}")
   805	                # Fall back to direct broadcast if we have connections
   806	                await self._broadcast_to_channel(channel, message)
   807	        else:
   808	            # Redis not available - send directly to connected clients
   809	            await self._broadcast_to_channel(channel, message)
   810	
   811	    async def _broadcast_to_channel(self, channel: str, message: dict) -> None:
   812	        """Broadcast a message to all connections on a channel.
   813	
   814	        Args:
   815	            channel: The channel identifier
   816	            message: The message to broadcast
   817	        """
   818	        disconnected = []
   819	
   820	        # Get a snapshot of connections to send to
   821	        async with self._connections_lock:
   822	            connections_snapshot = [
   823	                (key, list(websockets)) for key, websockets in self.connections.items() if channel in key
   824	            ]
   825	
   826	        # Send messages outside the lock to avoid blocking
   827	        for key, websockets in connections_snapshot:
   828	            for websocket in websockets:
   829	                try:
   830	                    await websocket.send_json(message)
   831	                    logger.debug(f"Sent channel message to websocket: {channel}")
   832	                except Exception as e:
   833	                    logger.warning(f"Failed to send channel message to websocket: {e}")
   834	                    disconnected.append((key, websocket))
   835	
   836	        # Clean up disconnected clients
   837	        if disconnected:
   838	            async with self._connections_lock:
   839	                for key, websocket in disconnected:
   840	                    if key in self.connections:
   841	                        self.connections[key].discard(websocket)
   842	                        if not self.connections[key]:
   843	                            del self.connections[key]
   844	
   845	    async def connect_to_channel(self, websocket: WebSocket, channel: str, user_id: str) -> None:
   846	        """Connect a WebSocket to a custom channel.
   847	
   848	        Args:
   849	            websocket: The WebSocket connection
   850	            channel: The channel to connect to
   851	            user_id: The user ID making the connection
   852	        """
   853	        # Check global connection limit first
   854	        async with self._connections_lock:
   855	            total_connections = sum(len(sockets) for sockets in self.connections.values())
   856	            if total_connections >= self.max_total_connections:
   857	                logger.error(f"Global connection limit reached ({self.max_total_connections})")
   858	                await websocket.close(code=1008, reason="Server connection limit exceeded")
   859	                raise ConnectionRefusedError("Server connection limit exceeded")
   860	
   861	            # Check connection limit for this user
   862	            user_connections = sum(
   863	                len(sockets) for key, sockets in self.connections.items() if key.startswith(f"{user_id}:")
   864	            )
   865	
   866	            if user_connections >= self.max_connections_per_user:
   867	                logger.warning(f"User {user_id} exceeded connection limit ({self.max_connections_per_user})")
   868	                await websocket.close(code=1008, reason="User connection limit exceeded")
   869	                raise ConnectionRefusedError("User connection limit exceeded")
   870	
   871	        await websocket.accept()
   872	
   873	        # Store connection
   874	        async with self._connections_lock:
   875	            key = f"{user_id}:channel:{channel}"
   876	            if key not in self.connections:
   877	                self.connections[key] = set()
   878	            self.connections[key].add(websocket)
   879	
   880	        logger.info(
   881	            f"Channel WebSocket connected: user={user_id}, channel={channel} "
   882	            f"(total user connections: {user_connections + 1})"
   883	        )
   884	
   885	    async def disconnect_from_channel(self, websocket: WebSocket, channel: str, user_id: str) -> None:
   886	        """Disconnect a WebSocket from a custom channel.
   887	
   888	        Args:
   889	            websocket: The WebSocket connection
   890	            channel: The channel to disconnect from
   891	            user_id: The user ID
   892	        """
   893	        key = f"{user_id}:channel:{channel}"
   894	        async with self._connections_lock:
   895	            if key in self.connections:
   896	                self.connections[key].discard(websocket)
   897	                if not self.connections[key]:
   898	                    del self.connections[key]
   899	
   900	        logger.info(f"Channel WebSocket disconnected: user={user_id}, channel={channel}")
   901	
   902	
   903	# Global instance
   904	ws_manager = RedisStreamWebSocketManager()
