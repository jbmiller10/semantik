#!/usr/bin/env python3
"""
Utilities for working with partitioned tables in PostgreSQL.

This module provides helper classes and functions for efficient operations
on partitioned tables, particularly the chunks table which is partitioned
by collection_id.
"""

import hashlib
import logging
import re
from collections.abc import Callable, Iterable, Sequence
from typing import Any, TypeVar

from sqlalchemy import Select, and_, select, text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession

from shared.database.models import Chunk

logger = logging.getLogger(__name__)

T = TypeVar("T")


class PartitionValidation:
    """Validation utilities for partition operations."""

    # UUID v4 pattern
    UUID_PATTERN = re.compile(r"^[a-f0-9]{8}-[a-f0-9]{4}-4[a-f0-9]{3}-[89ab][a-f0-9]{3}-[a-f0-9]{12}$", re.IGNORECASE)

    # Limits for validation
    MAX_BATCH_SIZE = 10000  # Maximum items in a single batch operation
    MAX_STRING_LENGTH = 1000000  # 1MB max for content fields

    @classmethod
    def validate_uuid(cls, value: Any, field_name: str = "id") -> str:
        """Validate UUID format.

        Args:
            value: Value to validate
            field_name: Name of the field for error messages

        Returns:
            Validated UUID string

        Raises:
            ValueError: If value is not a valid UUID
            TypeError: If value is not a string
        """
        if not isinstance(value, str):
            raise TypeError(f"{field_name} must be a string, got {type(value).__name__}")

        if not value:
            raise ValueError(f"{field_name} cannot be empty")

        if not cls.UUID_PATTERN.match(value):
            raise ValueError(f"{field_name} must be a valid UUID v4, got: {value}")

        return value.lower()  # Normalize to lowercase

    @classmethod
    def validate_partition_key(cls, value: Any, field_name: str = "collection_id") -> str:
        """Validate partition key value.

        Args:
            value: Partition key value
            field_name: Name of the field

        Returns:
            Validated partition key

        Raises:
            ValueError: If invalid
            TypeError: If wrong type
        """
        # For now, partition keys are UUIDs
        return cls.validate_uuid(value, field_name)

    @classmethod
    def validate_chunk_data(cls, chunk_data: dict[str, Any]) -> dict[str, Any]:
        """Validate chunk data before database operations.

        Args:
            chunk_data: Chunk data dictionary

        Returns:
            Validated chunk data

        Raises:
            ValueError: If data is invalid
            TypeError: If types are incorrect
        """
        if not isinstance(chunk_data, dict):
            raise TypeError(f"chunk_data must be a dictionary, got {type(chunk_data).__name__}")

        # Required fields
        if "collection_id" not in chunk_data:
            raise ValueError("collection_id is required for chunks (partition key)")

        # Validate collection_id
        chunk_data["collection_id"] = cls.validate_partition_key(chunk_data["collection_id"], "collection_id")

        # Validate partition_key if present
        if "partition_key" in chunk_data and chunk_data["partition_key"] is not None:
            partition_key = chunk_data["partition_key"]
            if not isinstance(partition_key, int):
                raise TypeError("partition_key must be an integer")
            if partition_key < 0 or partition_key > 99:
                raise ValueError("partition_key must be between 0 and 99")

        # Validate document_id if present
        if "document_id" in chunk_data and chunk_data["document_id"] is not None:
            chunk_data["document_id"] = cls.validate_uuid(chunk_data["document_id"], "document_id")

        # Validate id if present (chunk IDs are integers, auto-generated by database sequence)
        if "id" in chunk_data and chunk_data["id"] is not None:
            if not isinstance(chunk_data["id"], int):
                raise TypeError("chunk id must be an integer")
            if chunk_data["id"] <= 0:
                raise ValueError("chunk id must be positive")

        # Validate chunk_index
        if "chunk_index" in chunk_data:
            if not isinstance(chunk_data["chunk_index"], int):
                raise TypeError("chunk_index must be an integer")
            if chunk_data["chunk_index"] < 0:
                raise ValueError("chunk_index must be non-negative")

        # Validate content length
        if "content" in chunk_data:
            if not isinstance(chunk_data["content"], str):
                raise TypeError("content must be a string")
            if len(chunk_data["content"]) > cls.MAX_STRING_LENGTH:
                raise ValueError(f"content exceeds maximum length of {cls.MAX_STRING_LENGTH} characters")

        # Validate metadata if present
        if (
            "metadata" in chunk_data
            and chunk_data["metadata"] is not None
            and not isinstance(chunk_data["metadata"], dict)
        ):
            raise TypeError("metadata must be a dictionary")

        return chunk_data

    @classmethod
    def compute_partition_key_from_hash(cls, collection_id: str) -> int:
        """Compute a deterministic partition key in pure Python.

        Falls back to a stable hashing approach when database helpers are
        unavailable (e.g., in tests or limited environments).
        """

        validated = cls.validate_partition_key(collection_id, "collection_id")
        digest = hashlib.sha256(validated.encode("utf-8")).digest()
        # Use first 8 bytes for good distribution, then map into [0, 99]
        return int.from_bytes(digest[:8], byteorder="big", signed=False) % 100

    @classmethod
    def validate_batch_size(cls, items: Sequence[Any], operation: str = "bulk operation") -> None:
        """Validate batch size for bulk operations.

        Args:
            items: Items to validate
            operation: Operation name for error message

        Raises:
            ValueError: If batch size exceeds limit
        """
        if len(items) > cls.MAX_BATCH_SIZE:
            raise ValueError(
                f"{operation} batch size ({len(items)}) exceeds maximum allowed "
                f"({cls.MAX_BATCH_SIZE}). Please split into smaller batches."
            )

    @classmethod
    def sanitize_string(cls, value: str, max_length: int = 255) -> str:
        """Sanitize string input for database operations.

        Args:
            value: String to sanitize
            max_length: Maximum allowed length

        Returns:
            Sanitized string
        """
        if not isinstance(value, str):
            return str(value)

        # Truncate if too long
        if len(value) > max_length:
            value = value[:max_length]

        # Remove null bytes which PostgreSQL doesn't like
        return value.replace("\x00", "")


def compute_partition_key_from_hash(collection_id: str) -> int:
    """Compute deterministic partition key purely in Python.

    Mirrors the database helper that hashes the collection_id and maps the
    result into the [0, 99] partition range. This allows codepaths (and tests)
    to avoid relying on triggers when computing partition keys.
    """

    validated = PartitionValidation.validate_partition_key(collection_id, "collection_id")
    digest = hashlib.sha256(validated.encode("utf-8")).digest()
    return int.from_bytes(digest[:8], byteorder="big", signed=False) % 100


class PartitionAwareMixin:
    """Mixin for repositories that work with partitioned tables.

    This mixin provides common patterns and utilities for efficient
    operations on partitioned tables, ensuring partition pruning is
    enabled and bulk operations are optimized.
    """

    @staticmethod
    def ensure_partition_key_in_filter(
        query: Select[tuple[Any, ...]], partition_key_column: Any, partition_key_value: str | None
    ) -> Select[tuple[Any, ...]]:
        """Ensure partition key is included in query filter for pruning.

        Args:
            query: SQLAlchemy select query
            partition_key_column: The partition key column (e.g., Chunk.collection_id)
            partition_key_value: The partition key value to filter by

        Returns:
            Modified query with partition key filter

        Example:
            query = select(Chunk).where(Chunk.document_id == doc_id)
            query = self.ensure_partition_key_in_filter(
                query, Chunk.collection_id, collection_id
            )
        """
        if partition_key_value is not None:
            # Validate partition key format
            validated_key = PartitionValidation.validate_partition_key(partition_key_value)
            return query.where(partition_key_column == validated_key)

        logger.warning("Query on partitioned table without partition key - this will scan all partitions")
        return query

    @staticmethod
    def group_by_partition_key(items: Iterable[T], key_getter: Callable[[T], str]) -> dict[str, list[T]]:
        """Group items by partition key for efficient bulk operations.

        Args:
            items: Items to group
            key_getter: Function to extract partition key from item

        Returns:
            Dictionary mapping partition keys to lists of items

        Example:
            chunks_by_collection = self.group_by_partition_key(
                chunks, lambda c: c.collection_id
            )
        """
        grouped: dict[str, list[T]] = {}
        for item in items:
            key = key_getter(item)
            grouped.setdefault(key, []).append(item)
        return grouped

    async def bulk_insert_partitioned(
        self,
        session: AsyncSession,
        model_class: type[T],
        items: Sequence[dict[str, Any]],
        partition_key_field: str = "collection_id",
    ) -> None:
        """Efficiently bulk insert items into a partitioned table.

        Groups items by partition key before insertion to minimize
        partition switching overhead.

        Args:
            session: Database session
            model_class: SQLAlchemy model class
            items: List of item dictionaries to insert
            partition_key_field: Name of the partition key field
        """
        if not items:
            return

        # Validate batch size
        PartitionValidation.validate_batch_size(items, "bulk insert")

        # Validate and group by partition key
        items_by_partition: dict[str, list[dict[str, Any]]] = {}
        partition_key_cache: dict[str, int] = {}
        for item in items:
            key = item.get(partition_key_field)
            if key is None:
                raise ValueError(f"Partition key '{partition_key_field}' is required for all items")

            # Validate partition key format
            validated_key = PartitionValidation.validate_partition_key(key, partition_key_field)
            item[partition_key_field] = validated_key  # Update with validated key

            # Additional validation for chunk data
            if model_class.__name__ == "Chunk":
                if "partition_key" not in item or item["partition_key"] is None:
                    if validated_key not in partition_key_cache:
                        partition_key_cache[validated_key] = await self.compute_partition_key(session, validated_key)
                    item["partition_key"] = partition_key_cache[validated_key]
                item = PartitionValidation.validate_chunk_data(item)

            items_by_partition.setdefault(validated_key, []).append(item)

        # Insert in batches by partition
        for partition_key, partition_items in items_by_partition.items():
            logger.debug(
                f"Inserting {len(partition_items)} items into partition for {partition_key_field}={partition_key}"
            )

            # Using bulk_insert_mappings for efficiency
            # Create a closure to capture partition_items properly
            def make_bulk_insert(items: list[dict[str, Any]]) -> Callable[[Any], None]:
                def bulk_insert(sync_session: Any) -> None:
                    sync_session.bulk_insert_mappings(model_class, items)

                return bulk_insert

            await session.run_sync(make_bulk_insert(partition_items))

    async def delete_by_partition_filter(
        self,
        session: AsyncSession,
        model_class: type[T],
        partition_key_column: Any,
        partition_key_value: str,
        additional_filters: list[Any] | None = None,
    ) -> int:
        """Delete records from a partitioned table with partition pruning.

        Args:
            session: Database session
            model_class: SQLAlchemy model class
            partition_key_column: The partition key column
            partition_key_value: The partition key value
            additional_filters: Additional WHERE conditions

        Returns:
            Number of deleted records
        """
        # Validate partition key
        validated_key = PartitionValidation.validate_partition_key(partition_key_value)

        filters = [partition_key_column == validated_key]
        if additional_filters:
            filters.extend(additional_filters)

        # Execute delete with proper filters
        result = await session.execute(select(model_class).where(and_(*filters)))
        records = result.scalars().all()

        for record in records:
            await session.delete(record)

        return len(records)

    @staticmethod
    async def compute_partition_key(session: AsyncSession, collection_id: str) -> int:
        """Resolve the partition key for a collection id.

        Prefers the database helper (get_partition_key) when available so that
        computed keys always match production behaviour. Falls back to a
        deterministic hash if the helper is missing (e.g., in tests with a
        simplified schema).
        """

        validated_id = PartitionValidation.validate_partition_key(collection_id, "collection_id")

        try:
            result = await session.execute(
                text("SELECT get_partition_key(:collection_id)"),
                {"collection_id": validated_id},
            )
            db_value = result.scalar_one_or_none()
            if db_value is not None:
                return int(db_value)
        except SQLAlchemyError:
            logger.debug("Database helper get_partition_key unavailable, falling back to Python hash", exc_info=True)

        return PartitionValidation.compute_partition_key_from_hash(validated_id)


class ChunkPartitionHelper:
    """Helper class specifically for chunk table partition operations."""

    @staticmethod
    def create_chunk_query_with_partition(
        collection_id: str, additional_filters: list[Any] | None = None
    ) -> Select[tuple[Chunk]]:
        """Create a chunk query with partition key for optimal performance.

        Args:
            collection_id: Collection ID (used for partition pruning)
            additional_filters: Additional WHERE conditions

        Returns:
            SQLAlchemy select query

        Note: The partition_key is computed by the database trigger,
              so we filter by collection_id which allows PostgreSQL
              to prune partitions efficiently.

        Example:
            query = ChunkPartitionHelper.create_chunk_query_with_partition(
                collection_id,
                [Chunk.document_id == doc_id]
            )
        """
        # Validate collection_id
        validated_id = PartitionValidation.validate_partition_key(collection_id, "collection_id")

        # Filter by collection_id allows PostgreSQL to prune partitions
        query = select(Chunk).where(Chunk.collection_id == validated_id)

        if additional_filters:
            query = query.where(and_(*additional_filters))

        return query

    @staticmethod
    def validate_chunk_partition_key(chunk_data: dict[str, Any]) -> None:
        """Validate that chunk data includes required partition key.

        Args:
            chunk_data: Chunk data dictionary

        Raises:
            ValueError: If collection_id is missing or invalid
            TypeError: If chunk_data is not a dictionary
        """
        # Delegate to comprehensive validation
        PartitionValidation.validate_chunk_data(chunk_data)

    @staticmethod
    async def get_partition_statistics(session: AsyncSession, collection_id: str) -> dict[str, Any]:
        """Get statistics for a specific partition.

        Args:
            session: Database session
            collection_id: Collection ID to get stats for

        Returns:
            Dictionary with partition statistics
        """
        from sqlalchemy import func

        # Validate collection_id
        validated_id = PartitionValidation.validate_partition_key(collection_id, "collection_id")

        try:
            # Count chunks in this partition
            chunk_count = await session.scalar(
                select(func.count()).select_from(Chunk).where(Chunk.collection_id == validated_id)
            )

            # Get size statistics
            stats_query = select(
                func.count().label("count"),
                func.avg(func.length(Chunk.content)).label("avg_content_length"),
                func.sum(func.length(Chunk.content)).label("total_content_length"),
                func.min(Chunk.created_at).label("oldest_chunk"),
                func.max(Chunk.created_at).label("newest_chunk"),
            ).where(Chunk.collection_id == validated_id)

            result = await session.execute(stats_query)
            stats = result.one()

            # Handle null/None values safely
            avg_length = 0.0
            if stats.avg_content_length is not None:
                try:
                    avg_length = float(stats.avg_content_length)
                except (ValueError, TypeError):
                    logger.warning(f"Invalid avg_content_length value: {stats.avg_content_length}")
                    avg_length = 0.0

            return {
                "collection_id": validated_id,
                "chunk_count": int(chunk_count or 0),
                "avg_content_length": avg_length,
                "total_content_length": int(stats.total_content_length or 0),
                "oldest_chunk": stats.oldest_chunk,
                "newest_chunk": stats.newest_chunk,
            }
        except Exception as e:
            logger.error(f"Error getting partition statistics for collection {validated_id}: {e}")
            # Return safe defaults on error
            return {
                "collection_id": validated_id,
                "chunk_count": 0,
                "avg_content_length": 0.0,
                "total_content_length": 0,
                "oldest_chunk": None,
                "newest_chunk": None,
            }


# Usage example for bulk operations
async def example_bulk_chunk_insert(session: AsyncSession, chunks_data: list[dict[str, Any]]) -> None:
    """Example of efficient bulk insert for chunks.

    This demonstrates the recommended pattern for inserting many chunks
    efficiently into the partitioned table.
    """
    helper = PartitionAwareMixin()

    # Validation is now handled internally by bulk_insert_partitioned
    # which includes batch size validation, partition key validation,
    # and comprehensive chunk data validation

    # Perform partitioned bulk insert
    await helper.bulk_insert_partitioned(session, Chunk, chunks_data, partition_key_field="collection_id")


class PartitionImplementationDetector:
    """Detect and report on partition key implementation method.

    This class helps identify whether the database is using triggers or
    GENERATED columns for partition key computation, and provides
    recommendations based on the PostgreSQL version.
    """

    @staticmethod
    async def get_postgres_version(session: AsyncSession) -> tuple[int, int]:
        """Get PostgreSQL major and minor version.

        Args:
            session: Database session

        Returns:
            Tuple of (major, minor) version numbers
        """
        try:
            result = await session.execute(text("SELECT version()"))
            version_string = result.scalar()

            if not version_string:
                logger.warning("Could not get PostgreSQL version string")
                return 11, 0

            # Parse version from string like "PostgreSQL 16.1 on x86_64-pc-linux-gnu..."
            import re

            match = re.search(r"PostgreSQL (\d+)\.(\d+)", version_string)
            if match:
                major = int(match.group(1))
                minor = int(match.group(2))
                return major, minor

            # Try alternative format for major version only (PostgreSQL 12+)
            match = re.search(r"PostgreSQL (\d+)", version_string)
            if match:
                major = int(match.group(1))
                return major, 0

        except Exception as e:
            logger.warning(f"Could not determine PostgreSQL version: {e}")

        # Default to assuming old version if detection fails
        return 11, 0

    @staticmethod
    async def detect_implementation(session: AsyncSession) -> dict[str, Any]:
        """Detect current partition key implementation method.

        Args:
            session: Database session

        Returns:
            Dictionary with implementation details and recommendations
        """
        result = {
            "method": "unknown",
            "has_trigger": False,
            "has_generated_column": False,
            "postgres_version": None,
            "supports_generated": False,
            "is_optimal": False,
            "recommendation": None,
            "performance_impact": None,
        }

        try:
            # Get PostgreSQL version
            major_version, minor_version = await PartitionImplementationDetector.get_postgres_version(session)
            result["postgres_version"] = f"{major_version}.{minor_version}" if minor_version > 0 else str(major_version)
            result["supports_generated"] = major_version >= 12

            # Check for trigger
            trigger_check = await session.execute(
                text(
                    """
                    SELECT tgname
                    FROM pg_trigger
                    WHERE tgrelid = 'chunks'::regclass
                    AND tgname = 'set_partition_key'
                """
                )
            )
            trigger = trigger_check.fetchone()
            if trigger:
                result["has_trigger"] = True
                result["method"] = "trigger"
                logger.info(f"Detected trigger-based partition key implementation: {trigger[0]}")

            # Check if partition_key is a generated column
            col_check = await session.execute(
                text(
                    """
                    SELECT attgenerated
                    FROM pg_attribute
                    WHERE attrelid = 'chunks'::regclass
                    AND attname = 'partition_key'
                """
                )
            )
            col_info = col_check.fetchone()
            if col_info and col_info[0] == "s":  # 's' means STORED generated column
                result["has_generated_column"] = True
                result["method"] = "generated"
                logger.info("Detected GENERATED column for partition_key")

            # Determine if current implementation is optimal
            if major_version >= 12:
                result["is_optimal"] = result["has_generated_column"]
                if not result["is_optimal"]:
                    result["recommendation"] = (
                        f"PostgreSQL {result['postgres_version']} supports GENERATED columns. "
                        "Consider running migration 'db003_replace_trigger' to improve performance "
                        "by replacing the trigger with a GENERATED column. This can reduce "
                        "INSERT overhead by 2-3ms per row."
                    )
                    result["performance_impact"] = "2-3ms overhead per INSERT operation"
            else:
                result["is_optimal"] = result["has_trigger"]
                if not result["is_optimal"]:
                    result["recommendation"] = (
                        f"PostgreSQL {result['postgres_version']} does not support GENERATED columns. "
                        "Trigger-based implementation is required. Consider upgrading to "
                        "PostgreSQL 12+ for better performance."
                    )
                result["performance_impact"] = "Trigger overhead is unavoidable in PostgreSQL < 12"

            # Additional check for missing implementation
            if not result["has_trigger"] and not result["has_generated_column"]:
                result["method"] = "missing"
                result["is_optimal"] = False
                result["recommendation"] = (
                    "WARNING: No partition key implementation detected! "
                    "The partition_key column may not be properly computed. "
                    "Run the appropriate migration immediately."
                )
                result["performance_impact"] = "Partitioning may not work correctly"

        except SQLAlchemyError as e:
            logger.error(f"Error detecting partition implementation: {e}")
            result["recommendation"] = f"Could not detect implementation: {e}"

        return result

    @staticmethod
    async def verify_partition_keys(session: AsyncSession, sample_size: int = 100) -> dict[str, Any]:
        """Verify that partition keys are correctly computed.

        Args:
            session: Database session
            sample_size: Number of records to check

        Returns:
            Dictionary with verification results
        """
        verification: dict[str, Any] = {
            "checked": 0,
            "correct": 0,
            "incorrect": 0,
            "errors": [],
            "is_valid": False,
        }

        try:
            # Get a sample of records to verify
            result = await session.execute(
                text(
                    f"""
                    SELECT
                        collection_id,
                        partition_key,
                        abs(hashtext(collection_id::text)) % 100 as computed_key
                    FROM chunks
                    LIMIT {sample_size}
                """
                )
            )

            for row in result:
                verification["checked"] += 1
                if row.partition_key == row.computed_key:
                    verification["correct"] += 1
                else:
                    verification["incorrect"] += 1
                    verification["errors"].append(
                        {
                            "collection_id": row.collection_id,
                            "stored": row.partition_key,
                            "expected": row.computed_key,
                        }
                    )

            verification["is_valid"] = verification["incorrect"] == 0

            if not verification["is_valid"]:
                logger.error(
                    f"Partition key verification failed: "
                    f"{verification['incorrect']} incorrect out of {verification['checked']}"
                )
            else:
                logger.info(f"Partition key verification passed: all {verification['checked']} keys are correct")

        except SQLAlchemyError as e:
            logger.error(f"Error verifying partition keys: {e}")
            verification["errors"].append({"error": str(e)})

        return verification

    @staticmethod
    async def get_performance_metrics(session: AsyncSession) -> dict[str, Any]:
        """Get performance metrics for partition operations.

        Args:
            session: Database session

        Returns:
            Dictionary with performance metrics
        """
        metrics: dict[str, Any] = {
            "partition_count": 0,
            "total_chunks": 0,
            "avg_chunks_per_partition": 0,
            "max_chunks_in_partition": 0,
            "min_chunks_in_partition": 0,
            "empty_partitions": 0,
            "hot_partitions": [],  # Partitions with significantly more data
        }

        try:
            # Get partition distribution
            result = await session.execute(
                text(
                    """
                    SELECT
                        partition_key,
                        COUNT(*) as chunk_count
                    FROM chunks
                    GROUP BY partition_key
                    ORDER BY chunk_count DESC
                """
                )
            )

            distribution = list(result)

            if distribution:
                counts = [row.chunk_count for row in distribution]
                metrics["partition_count"] = len(distribution)
                metrics["total_chunks"] = sum(counts)
                metrics["avg_chunks_per_partition"] = sum(counts) / len(counts)
                metrics["max_chunks_in_partition"] = max(counts)
                metrics["min_chunks_in_partition"] = min(counts)

                # Identify hot partitions (> 2x average)
                avg = metrics["avg_chunks_per_partition"]
                for row in distribution:
                    if row.chunk_count > avg * 2:
                        metrics["hot_partitions"].append(
                            {
                                "partition_key": row.partition_key,
                                "chunk_count": row.chunk_count,
                                "ratio_to_avg": row.chunk_count / avg,
                            }
                        )

                # Count empty partitions (0 to 99 that have no data)
                used_partitions = {row.partition_key for row in distribution}
                metrics["empty_partitions"] = 100 - len(used_partitions)

        except SQLAlchemyError as e:
            logger.error(f"Error getting performance metrics: {e}")

        return metrics

    @staticmethod
    async def generate_health_report(session: AsyncSession) -> str:
        """Generate a comprehensive health report for partition implementation.

        Args:
            session: Database session

        Returns:
            Formatted health report string
        """
        # Gather all information
        impl = await PartitionImplementationDetector.detect_implementation(session)
        verification = await PartitionImplementationDetector.verify_partition_keys(session)
        metrics = await PartitionImplementationDetector.get_performance_metrics(session)

        # Format the report
        report = []
        report.append("=" * 60)
        report.append("PARTITION KEY IMPLEMENTATION HEALTH REPORT")
        report.append("=" * 60)
        report.append("")

        # Implementation details
        report.append("IMPLEMENTATION STATUS:")
        report.append(f"  PostgreSQL Version: {impl['postgres_version']}")
        report.append(f"  Current Method: {impl['method'].upper()}")
        report.append(f"  Is Optimal: {'YES' if impl['is_optimal'] else 'NO'}")

        if impl["performance_impact"]:
            report.append(f"  Performance Impact: {impl['performance_impact']}")

        report.append("")

        # Verification results
        report.append("DATA VERIFICATION:")
        report.append(f"  Records Checked: {verification['checked']}")
        report.append(f"  Correct Keys: {verification['correct']}")
        report.append(f"  Incorrect Keys: {verification['incorrect']}")
        report.append(f"  Validation: {'PASSED' if verification['is_valid'] else 'FAILED'}")

        if verification["incorrect"] > 0 and verification["errors"]:
            report.append("  Sample Errors:")
            for err in verification["errors"][:3]:  # Show first 3 errors
                if "error" in err:
                    report.append(f"    - {err['error']}")
                else:
                    report.append(
                        f"    - Collection {err['collection_id']}: "
                        f"stored={err['stored']}, expected={err['expected']}"
                    )

        report.append("")

        # Performance metrics
        report.append("PERFORMANCE METRICS:")
        report.append(f"  Total Chunks: {metrics['total_chunks']:,}")
        report.append(f"  Active Partitions: {metrics['partition_count']}/100")
        report.append(f"  Empty Partitions: {metrics['empty_partitions']}")

        if metrics["partition_count"] > 0:
            report.append(f"  Avg Chunks/Partition: {metrics['avg_chunks_per_partition']:.1f}")
            report.append(f"  Max Chunks in Partition: {metrics['max_chunks_in_partition']:,}")
            report.append(f"  Min Chunks in Partition: {metrics['min_chunks_in_partition']:,}")

        if metrics["hot_partitions"]:
            report.append(f"  Hot Partitions: {len(metrics['hot_partitions'])}")
            for hot in metrics["hot_partitions"][:3]:  # Show top 3
                report.append(
                    f"    - Partition {hot['partition_key']}: "
                    f"{hot['chunk_count']:,} chunks ({hot['ratio_to_avg']:.1f}x average)"
                )

        report.append("")

        # Recommendations
        if impl["recommendation"]:
            report.append("RECOMMENDATIONS:")
            for line in impl["recommendation"].split(". "):
                if line:
                    report.append(f"  â€¢ {line.strip()}{'.' if not line.endswith('.') else ''}")

        report.append("")
        report.append("=" * 60)

        return "\n".join(report)
