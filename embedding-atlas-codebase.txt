This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: packages/shared/database/models.py, packages/shared/database/repositories/projection_run_repository.py, alembic/versions/202510211200_add_projection_runs_table.py, alembic/v
  ersions/202510221045_normalize_operation_type_projection_case.py, packages/webui/services/projection_service.py, packages/webui/services/factory.py, packages/webui/api/v2/projections.py, packag
  es/webui/api/v2/schemas.py, packages/webui/tasks/projection.py, apps/webui-react/src/components/EmbeddingVisualizationTab.tsx, apps/webui-react/src/services/api/v2/projections.ts, apps/webui-re
  act/src/types/projection.ts, apps/webui-react/src/hooks/useProjections.ts, tests/webui/api/v2/test_projections.py, tests/webui/services/test_projection_service.py, tests/webui/test_projection_c
  ompute_regression.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
alembic/
  versions/
    202510211200_add_projection_runs_table.py
apps/
  webui-react/
    src/
      components/
        EmbeddingVisualizationTab.tsx
      hooks/
        useProjections.ts
      services/
        api/
          v2/
            projections.ts
packages/
  shared/
    database/
      repositories/
        projection_run_repository.py
      models.py
  webui/
    api/
      v2/
        projections.py
    services/
      factory.py
      projection_service.py
    tasks/
      projection.py
tests/
  webui/
    api/
      v2/
        test_projections.py
    services/
      test_projection_service.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="apps/webui-react/src/hooks/useProjections.ts">
import { useMutation, useQuery, useQueryClient } from '@tanstack/react-query';
import { projectionsV2Api } from '../services/api/v2/projections';
import { handleApiError } from '../services/api/v2/collections';
import { useUIStore } from '../stores/uiStore';
import type {
  ProjectionData,
  ProjectionMetadata,
  ProjectionStatus,
  StartProjectionRequest,
} from '../types/projection';

const DEFAULT_REFETCH_INTERVAL = 5000;

export const projectionKeys = {
  root: ['projections'] as const,
  lists: (collectionId: string) => [...projectionKeys.root, collectionId, 'list'] as const,
  detail: (collectionId: string, projectionId: string) =>
    [...projectionKeys.root, collectionId, 'detail', projectionId] as const,
  data: (collectionId: string, projectionId: string) =>
    [...projectionKeys.root, collectionId, 'data', projectionId] as const,
};

export function useCollectionProjections(collectionId: string | null) {
  const safeId = collectionId ?? '__none__';

  return useQuery<ProjectionMetadata[]>({
    queryKey: projectionKeys.lists(safeId),
    queryFn: async () => {
      if (!collectionId) return [];
      const response = await projectionsV2Api.list(collectionId);
      return response.data?.projections ?? [];
    },
    enabled: !!collectionId,
    initialData: [],
    refetchInterval: (query) => {
      const projections = query.state.data as ProjectionMetadata[] | undefined;
      const hasRunning = projections?.some((projection) =>
        ['pending', 'running'].includes(projection.status as ProjectionStatus)
      );
      return hasRunning ? DEFAULT_REFETCH_INTERVAL : false;
    },
  });
}

export function useProjectionMetadata(collectionId: string | null, projectionId: string | null) {
  const safeCollectionId = collectionId ?? '__none__';
  const safeProjectionId = projectionId ?? '__none__';

  return useQuery<ProjectionMetadata | null>({
    queryKey: projectionKeys.detail(safeCollectionId, safeProjectionId),
    queryFn: async () => {
      if (!collectionId || !projectionId) return null;
      const response = await projectionsV2Api.getMetadata(collectionId, projectionId);
      return response.data;
    },
    enabled: !!collectionId && !!projectionId,
  });
}

export function useProjectionData(collectionId: string | null, projectionId: string | null) {
  const safeCollectionId = collectionId ?? '__none__';
  const safeProjectionId = projectionId ?? '__none__';

  return useQuery<ProjectionData | null>({
    queryKey: projectionKeys.data(safeCollectionId, safeProjectionId),
    queryFn: async () => {
      if (!collectionId || !projectionId) return null;
      const response = await projectionsV2Api.getData(collectionId, projectionId);
      return response.data;
    },
    enabled: !!collectionId && !!projectionId,
  });
}

export function useStartProjection(collectionId: string | null) {
  const queryClient = useQueryClient();
  const { addToast } = useUIStore();

  return useMutation({
    mutationFn: async (payload: StartProjectionRequest) => {
      if (!collectionId) throw new Error('Collection ID is required');
      const response = await projectionsV2Api.start(collectionId, payload);
      return response.data;
    },
    onMutate: async () => {
      if (!collectionId) return undefined;
      await queryClient.cancelQueries({ queryKey: projectionKeys.lists(collectionId) });
      const previous = queryClient.getQueryData<ProjectionMetadata[]>(
        projectionKeys.lists(collectionId)
      );
      return { previous };
    },
    onError: (error, _variables, context) => {
      const message = handleApiError(error);
      addToast({ type: 'error', message });
      const collectionIdSafe = collectionId;
      if (collectionIdSafe && context?.previous) {
        queryClient.setQueryData(projectionKeys.lists(collectionIdSafe), context.previous);
      }
    },
    onSuccess: (data) => {
      addToast({ type: 'success', message: 'Projection started successfully' });
      if (!collectionId) return;
      queryClient.setQueryData<ProjectionMetadata[]>(
        projectionKeys.lists(collectionId),
        (prev = []) => {
          if (!data) return prev;
          const filtered = prev.filter((projection) => projection.id !== data.id);
          return [data, ...filtered];
        }
      );
    },
    onSettled: () => {
      if (!collectionId) return;
      queryClient.invalidateQueries({ queryKey: projectionKeys.lists(collectionId) });
    },
  });
}

export function useDeleteProjection(collectionId: string | null) {
  const queryClient = useQueryClient();
  const { addToast } = useUIStore();

  return useMutation({
    mutationFn: async (projectionId: string) => {
      if (!collectionId) throw new Error('Collection ID is required');
      await projectionsV2Api.delete(collectionId, projectionId);
      return projectionId;
    },
    onMutate: async (projectionId) => {
      if (!collectionId) return undefined;
      await queryClient.cancelQueries({ queryKey: projectionKeys.lists(collectionId) });
      const previous = queryClient.getQueryData<ProjectionMetadata[]>(
        projectionKeys.lists(collectionId)
      );
      queryClient.setQueryData<ProjectionMetadata[]>(
        projectionKeys.lists(collectionId),
        (prev = []) => prev.filter((projection) => projection.id !== projectionId)
      );
      return { previous };
    },
    onError: (error, _projectionId, context) => {
      const message = handleApiError(error);
      addToast({ type: 'error', message });
      if (!collectionId) return;
      if (context?.previous) {
        queryClient.setQueryData(
          projectionKeys.lists(collectionId),
          context.previous
        );
      }
    },
    onSuccess: () => {
      addToast({ type: 'success', message: 'Projection deleted successfully' });
    },
    onSettled: () => {
      if (!collectionId) return;
      queryClient.invalidateQueries({ queryKey: projectionKeys.lists(collectionId) });
    },
  });
}

export function useUpdateProjectionInCache() {
  const queryClient = useQueryClient();

  return (collectionId: string, updated: ProjectionMetadata) => {
    queryClient.setQueryData<ProjectionMetadata[]>(
      projectionKeys.lists(collectionId),
      (prev = []) => prev.map((projection) =>
        projection.id === updated.id ? { ...projection, ...updated } : projection
      )
    );
    queryClient.setQueryData(
      projectionKeys.detail(collectionId, updated.id),
      updated
    );
  };
}

export function useProjectionDataFromCache(
  collectionId: string,
  projectionId: string
): ProjectionData | undefined {
  return useQueryClient().getQueryData<ProjectionData>(
    projectionKeys.data(collectionId, projectionId)
  );
}
</file>

<file path="tests/webui/api/v2/test_projections.py">
"""Integration tests for the v2 projections API."""

from datetime import UTC, datetime
from uuid import uuid4

import pytest
from httpx import AsyncClient

from packages.shared.database.models import (
    OperationStatus,
    OperationType,
    ProjectionRun,
    ProjectionRunStatus,
)


@pytest.mark.asyncio()
async def test_start_projection_includes_operation_status(
    api_client: AsyncClient,
    api_auth_headers: dict[str, str],
    test_user_db,
    collection_factory,
    stub_celery_send_task,
) -> None:
    """Starting a projection should return operation_status in response."""
    collection = await collection_factory(owner_id=test_user_db.id)

    request_payload = {
        "reducer": "umap",
        "dimensionality": 2,
        "color_by": "document_id",
        "config": {"n_neighbors": 15, "min_dist": 0.1, "metric": "cosine"},
    }

    response = await api_client.post(
        f"/api/v2/collections/{collection.id}/projections",
        json=request_payload,
        headers=api_auth_headers,
    )

    assert response.status_code == 202, response.text
    body = response.json()

    # Verify celery task was dispatched
    stub_celery_send_task.assert_called_once()

    # Verify operation_status is present and valid
    assert "operation_status" in body
    assert body["operation_status"] in ["pending", "processing", "completed", "failed", "cancelled"]

    # Verify operation_id is also present
    assert "operation_id" in body
    assert body["operation_id"] is not None

    # Verify other fields
    assert body["reducer"] == "umap"
    assert body["dimensionality"] == 2


@pytest.mark.asyncio()
async def test_get_projection_includes_operation_status(
    api_client: AsyncClient,
    api_auth_headers: dict[str, str],
    test_user_db,
    collection_factory,
    operation_factory,
    db_session,
) -> None:
    """Fetching a single projection should include operation_status."""
    collection = await collection_factory(owner_id=test_user_db.id)

    # Create operation
    operation = await operation_factory(
        collection_id=collection.id,
        user_id=test_user_db.id,
        type=OperationType.PROJECTION_BUILD,
        status=OperationStatus.PROCESSING,
    )

    # Create projection run
    projection_run = ProjectionRun(
        uuid=str(uuid4()),
        collection_id=collection.id,
        operation_uuid=operation.uuid,
        reducer="umap",
        dimensionality=2,
        status=ProjectionRunStatus.RUNNING,
        config={"n_neighbors": 15},
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
    )
    db_session.add(projection_run)
    await db_session.commit()
    await db_session.refresh(projection_run)

    response = await api_client.get(
        f"/api/v2/collections/{collection.id}/projections/{projection_run.uuid}",
        headers=api_auth_headers,
    )

    assert response.status_code == 200, response.text
    body = response.json()

    # Verify operation_status is present and matches operation
    assert "operation_status" in body
    assert body["operation_status"] == "processing"

    # Verify operation_id
    assert body["operation_id"] == operation.uuid


@pytest.mark.asyncio()
async def test_list_projections_includes_operation_status(
    api_client: AsyncClient,
    api_auth_headers: dict[str, str],
    test_user_db,
    collection_factory,
    operation_factory,
    db_session,
) -> None:
    """Listing projections should include operation_status for each item."""
    collection = await collection_factory(owner_id=test_user_db.id)

    # Create first projection with PROCESSING operation
    operation1 = await operation_factory(
        collection_id=collection.id,
        user_id=test_user_db.id,
        type=OperationType.PROJECTION_BUILD,
        status=OperationStatus.PROCESSING,
    )
    projection1 = ProjectionRun(
        uuid=str(uuid4()),
        collection_id=collection.id,
        operation_uuid=operation1.uuid,
        reducer="umap",
        dimensionality=2,
        status=ProjectionRunStatus.RUNNING,
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
    )
    db_session.add(projection1)

    # Create second projection with COMPLETED operation
    operation2 = await operation_factory(
        collection_id=collection.id,
        user_id=test_user_db.id,
        type=OperationType.PROJECTION_BUILD,
        status=OperationStatus.COMPLETED,
    )
    projection2 = ProjectionRun(
        uuid=str(uuid4()),
        collection_id=collection.id,
        operation_uuid=operation2.uuid,
        reducer="pca",
        dimensionality=2,
        status=ProjectionRunStatus.COMPLETED,
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
    )
    db_session.add(projection2)

    await db_session.commit()

    response = await api_client.get(
        f"/api/v2/collections/{collection.id}/projections",
        headers=api_auth_headers,
    )

    assert response.status_code == 200, response.text
    body = response.json()

    assert "projections" in body
    projections = body["projections"]
    assert len(projections) == 2

    # Verify all projections have operation_status
    for projection in projections:
        assert "operation_status" in projection
        assert projection["operation_status"] in ["pending", "processing", "completed", "failed", "cancelled"]
        assert "operation_id" in projection

    # Find specific projections and verify their operation_status
    processing_projection = next((p for p in projections if p["id"] == projection1.uuid), None)
    completed_projection = next((p for p in projections if p["id"] == projection2.uuid), None)

    assert processing_projection is not None
    assert processing_projection["operation_status"] == "processing"

    assert completed_projection is not None
    assert completed_projection["operation_status"] == "completed"


@pytest.mark.asyncio()
async def test_projection_with_failed_operation_includes_error_message(
    api_client: AsyncClient,
    api_auth_headers: dict[str, str],
    test_user_db,
    collection_factory,
    operation_factory,
    db_session,
) -> None:
    """Projection with failed operation should include error message."""
    collection = await collection_factory(owner_id=test_user_db.id)

    # Create failed operation
    operation = await operation_factory(
        collection_id=collection.id,
        user_id=test_user_db.id,
        type=OperationType.PROJECTION_BUILD,
        status=OperationStatus.FAILED,
    )
    # Manually set error message (operation_factory doesn't support it)
    operation.error_message = "CUDA out of memory"
    db_session.add(operation)

    # Create projection run
    projection_run = ProjectionRun(
        uuid=str(uuid4()),
        collection_id=collection.id,
        operation_uuid=operation.uuid,
        reducer="umap",
        dimensionality=2,
        status=ProjectionRunStatus.FAILED,
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
    )
    db_session.add(projection_run)
    await db_session.commit()
    await db_session.refresh(projection_run)

    response = await api_client.get(
        f"/api/v2/collections/{collection.id}/projections/{projection_run.uuid}",
        headers=api_auth_headers,
    )

    assert response.status_code == 200, response.text
    body = response.json()

    # Verify operation_status is failed
    assert body["operation_status"] == "failed"

    # Verify error message is included
    assert "message" in body
    assert body["message"] == "CUDA out of memory"


@pytest.mark.asyncio()
async def test_projection_without_operation_has_no_operation_status(
    api_client: AsyncClient,
    api_auth_headers: dict[str, str],
    test_user_db,
    collection_factory,
    db_session,
) -> None:
    """Projection without an associated operation should have null operation_status."""
    collection = await collection_factory(owner_id=test_user_db.id)

    # Create projection run without operation
    projection_run = ProjectionRun(
        uuid=str(uuid4()),
        collection_id=collection.id,
        operation_uuid=None,  # No operation
        reducer="pca",
        dimensionality=2,
        status=ProjectionRunStatus.COMPLETED,
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
    )
    db_session.add(projection_run)
    await db_session.commit()
    await db_session.refresh(projection_run)

    response = await api_client.get(
        f"/api/v2/collections/{collection.id}/projections/{projection_run.uuid}",
        headers=api_auth_headers,
    )

    assert response.status_code == 200, response.text
    body = response.json()

    # operation_status should be present but None/null
    assert "operation_status" in body or body.get("operation_status") is None
    assert body.get("operation_id") is None
</file>

<file path="tests/webui/services/test_projection_service.py">
"""Unit tests for ProjectionService._encode_projection method."""

from datetime import UTC, datetime
from unittest.mock import MagicMock

from packages.shared.database.models import OperationStatus, ProjectionRunStatus
from packages.webui.services.projection_service import ProjectionService


def test_encode_projection_without_operation():
    """Test _encode_projection returns basic projection metadata without operation_status."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.COMPLETED
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = datetime(2025, 1, 15, 10, 30, 0, tzinfo=UTC)
    run.operation_uuid = "op-789"
    run.config = {"n_neighbors": 15, "min_dist": 0.1, "color_by": "document_id"}
    run.meta = {"legend": [{"index": 0, "label": "Doc A"}]}

    result = ProjectionService._encode_projection(run)

    assert result["collection_id"] == "coll-123"
    assert result["projection_id"] == "proj-456"
    assert result["status"] == "completed"
    assert result["reducer"] == "umap"
    assert result["dimensionality"] == 2
    assert result["created_at"] == datetime(2025, 1, 15, 10, 30, 0, tzinfo=UTC)
    assert result["operation_id"] == "op-789"
    assert result["config"] == {"n_neighbors": 15, "min_dist": 0.1, "color_by": "document_id"}
    assert result["meta"] == {"legend": [{"index": 0, "label": "Doc A"}], "color_by": "document_id"}
    assert result["message"] is None
    # operation_status should NOT be present when operation is not provided
    assert "operation_status" not in result


def test_encode_projection_with_operation():
    """Test _encode_projection includes operation_status when operation provided."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.RUNNING
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = datetime(2025, 1, 15, 10, 30, 0, tzinfo=UTC)
    run.operation_uuid = "op-789"
    run.config = {"n_neighbors": 15}
    run.meta = {}

    # Create mock operation
    operation = MagicMock()
    operation.uuid = "op-789"
    operation.status = OperationStatus.PROCESSING
    operation.error_message = None

    result = ProjectionService._encode_projection(run, operation=operation)

    assert result["operation_id"] == "op-789"
    assert result["operation_status"] == "processing"
    assert result["message"] is None


def test_encode_projection_with_failed_operation():
    """Test _encode_projection includes error message from failed operation."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.FAILED
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = datetime(2025, 1, 15, 10, 30, 0, tzinfo=UTC)
    run.operation_uuid = "op-789"
    run.config = None
    run.meta = None

    # Create mock failed operation
    operation = MagicMock()
    operation.uuid = "op-789"
    operation.status = OperationStatus.FAILED
    operation.error_message = "CUDA out of memory"

    result = ProjectionService._encode_projection(run, operation=operation)

    assert result["operation_status"] == "failed"
    assert result["message"] == "CUDA out of memory"


def test_encode_projection_custom_message_overrides_error():
    """Test that custom message parameter takes precedence over operation error_message."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.FAILED
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = None
    run.operation_uuid = "op-789"
    run.config = None
    run.meta = None

    # Create mock failed operation
    operation = MagicMock()
    operation.uuid = "op-789"
    operation.status = OperationStatus.FAILED
    operation.error_message = "CUDA out of memory"

    result = ProjectionService._encode_projection(run, operation=operation, message="Custom error message")

    assert result["operation_status"] == "failed"
    # Custom message should take precedence
    assert result["message"] == "Custom error message"


def test_encode_projection_color_by_in_config():
    """Test that color_by from config is propagated to meta."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.COMPLETED
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = None
    run.operation_uuid = None
    run.config = {"color_by": "filetype", "n_neighbors": 15}
    run.meta = {}

    result = ProjectionService._encode_projection(run)

    # color_by should be added to meta from config
    assert result["meta"]["color_by"] == "filetype"


def test_encode_projection_empty_meta_returns_none():
    """Test that empty meta dict becomes None in response."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.PENDING
    run.reducer = "pca"
    run.dimensionality = 2
    run.created_at = None
    run.operation_uuid = None
    run.config = None
    run.meta = {}

    result = ProjectionService._encode_projection(run)

    # Empty meta should become None
    assert result["meta"] is None


def test_encode_projection_with_completed_operation():
    """Test _encode_projection with completed operation status."""
    # Create mock projection run
    run = MagicMock()
    run.collection_id = "coll-123"
    run.uuid = "proj-456"
    run.status = ProjectionRunStatus.COMPLETED
    run.reducer = "umap"
    run.dimensionality = 2
    run.created_at = datetime(2025, 1, 15, 10, 30, 0, tzinfo=UTC)
    run.operation_uuid = "op-789"
    run.config = None
    run.meta = None

    # Create mock completed operation
    operation = MagicMock()
    operation.uuid = "op-789"
    operation.status = OperationStatus.COMPLETED
    operation.error_message = None

    result = ProjectionService._encode_projection(run, operation=operation)

    assert result["status"] == "completed"
    assert result["operation_status"] == "completed"
    assert result["message"] is None
</file>

<file path="apps/webui-react/src/services/api/v2/projections.ts">
import apiClient from './client';
import type {
  ProjectionArtifactName,
  ProjectionData,
  ProjectionListResponse,
  ProjectionMetadata,
  ProjectionSelectionResponse,
  StartProjectionRequest,
  StartProjectionResponse,
} from '../../../types/projection';

export const projectionsV2Api = {
  list: (collectionId: string) =>
    apiClient.get<ProjectionListResponse>(
      `/api/v2/collections/${collectionId}/projections`
    ),

  getMetadata: (collectionId: string, projectionId: string) =>
    apiClient.get<ProjectionMetadata>(
      `/api/v2/collections/${collectionId}/projections/${projectionId}`
    ),

  getData: (collectionId: string, projectionId: string) =>
    apiClient.get<ProjectionData>(
      `/api/v2/collections/${collectionId}/projections/${projectionId}/array`
    ),

  getArtifact: (collectionId: string, projectionId: string, artifactName: ProjectionArtifactName) =>
    apiClient.get<ArrayBuffer>(
      `/api/v2/collections/${collectionId}/projections/${projectionId}/arrays/${artifactName}`,
      { responseType: 'arraybuffer' }
    ),

  start: (collectionId: string, payload: StartProjectionRequest) =>
    apiClient.post<StartProjectionResponse>(
      `/api/v2/collections/${collectionId}/projections`,
      payload
    ),

  delete: (collectionId: string, projectionId: string) =>
    apiClient.delete<void>(
      `/api/v2/collections/${collectionId}/projections/${projectionId}`
    ),

  select: (collectionId: string, projectionId: string, ids: number[]) =>
    apiClient.post<ProjectionSelectionResponse>(
      `/api/v2/collections/${collectionId}/projections/${projectionId}/select`,
      { ids }
    ),
};
</file>

<file path="packages/shared/database/repositories/projection_run_repository.py">
"""Repository implementation for ProjectionRun model."""

from __future__ import annotations

import logging
from datetime import UTC, datetime
from typing import TYPE_CHECKING, Any
from uuid import uuid4

from shared.database.exceptions import DatabaseOperationError, EntityNotFoundError, ValidationError
from shared.database.models import Collection, ProjectionRun, ProjectionRunStatus
from sqlalchemy import Select, delete, func, select
from sqlalchemy.orm import selectinload

if TYPE_CHECKING:
    from collections.abc import Sequence

    from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class ProjectionRunRepository:
    """Repository providing CRUD helpers for projection runs."""

    def __init__(self, session: AsyncSession) -> None:
        self.session = session

    async def create(
        self,
        *,
        collection_id: str,
        reducer: str,
        dimensionality: int,
        config: dict[str, Any] | None = None,
        meta: dict[str, Any] | None = None,
    ) -> ProjectionRun:
        """Create a new projection run stub for a collection."""
        if dimensionality <= 0:
            raise ValidationError("dimensionality must be greater than zero", "dimensionality")
        if not reducer:
            raise ValidationError("reducer is required", "reducer")

        collection_exists = await self.session.scalar(
            select(func.count()).select_from(Collection).where(Collection.id == collection_id)
        )
        if not collection_exists:
            raise EntityNotFoundError("collection", collection_id)

        run = ProjectionRun(
            uuid=str(uuid4()),
            collection_id=collection_id,
            reducer=reducer,
            dimensionality=dimensionality,
            config=config or {},
            meta=meta or {},
        )

        try:
            self.session.add(run)
            await self.session.flush()
            logger.info("Created projection run %s for collection %s", run.uuid, collection_id)
            return run
        except Exception as exc:  # pragma: no cover - defensive logging
            logger.error("Failed to create projection run for collection %s: %s", collection_id, exc)
            raise DatabaseOperationError("create", "projection_run", str(exc)) from exc

    async def get_by_uuid(self, projection_uuid: str) -> ProjectionRun | None:
        """Fetch a projection run by its external UUID."""
        stmt: Select[tuple[ProjectionRun]] = (
            select(ProjectionRun)
            .where(ProjectionRun.uuid == projection_uuid)
            .options(selectinload(ProjectionRun.collection), selectinload(ProjectionRun.operation))
        )
        result = await self.session.execute(stmt)
        return result.scalar_one_or_none()

    async def list_for_collection(
        self,
        collection_id: str,
        *,
        limit: int = 50,
        offset: int = 0,
        statuses: Sequence[ProjectionRunStatus] | None = None,
    ) -> tuple[list[ProjectionRun], int]:
        """List projection runs for a collection with optional status filter."""
        stmt: Select[tuple[ProjectionRun]] = (
            select(ProjectionRun)
            .where(ProjectionRun.collection_id == collection_id)
            .order_by(ProjectionRun.created_at.desc())
            .limit(limit)
            .offset(offset)
        )
        if statuses:
            stmt = stmt.where(ProjectionRun.status.in_(tuple(statuses)))

        runs = list((await self.session.execute(stmt)).scalars().all())

        count_stmt = select(func.count(ProjectionRun.id)).where(ProjectionRun.collection_id == collection_id)
        if statuses:
            count_stmt = count_stmt.where(ProjectionRun.status.in_(tuple(statuses)))
        total = await self.session.scalar(count_stmt) or 0
        return runs, total

    async def update_status(
        self,
        projection_uuid: str,
        *,
        status: ProjectionRunStatus,
        error_message: str | None = None,
        started_at: datetime | None = None,
        completed_at: datetime | None = None,
    ) -> ProjectionRun:
        """Update lifecycle status for a projection run."""
        run = await self.get_by_uuid(projection_uuid)
        if not run:
            raise EntityNotFoundError("projection_run", projection_uuid)

        now = datetime.now(UTC)
        run.status = status
        run.updated_at = now

        if status == ProjectionRunStatus.RUNNING and run.started_at is None:
            run.started_at = started_at or now
        elif status in {ProjectionRunStatus.COMPLETED, ProjectionRunStatus.FAILED, ProjectionRunStatus.CANCELLED}:
            run.completed_at = completed_at or now
            if status == ProjectionRunStatus.COMPLETED:
                run.error_message = None

        if error_message is not None:
            run.error_message = error_message

        await self.session.flush()
        return run

    async def set_operation_uuid(self, projection_uuid: str, operation_uuid: str | None) -> ProjectionRun:
        """Link the projection run to a backing operation."""
        run = await self.get_by_uuid(projection_uuid)
        if not run:
            raise EntityNotFoundError("projection_run", projection_uuid)

        run.operation_uuid = operation_uuid
        await self.session.flush()
        return run

    async def update_metadata(
        self,
        projection_uuid: str,
        *,
        storage_path: str | None = None,
        point_count: int | None = None,
        meta: dict[str, Any] | None = None,
    ) -> ProjectionRun:
        """Update storage attributes associated with a projection run."""
        run = await self.get_by_uuid(projection_uuid)
        if not run:
            raise EntityNotFoundError("projection_run", projection_uuid)

        if point_count is not None and point_count < 0:
            raise ValidationError("point_count must be non-negative", "point_count")

        if storage_path is not None:
            run.storage_path = storage_path
        if point_count is not None:
            run.point_count = point_count
        if meta:
            merged_meta = dict(run.meta or {})
            merged_meta.update(meta)
            run.meta = merged_meta

        run.updated_at = datetime.now(UTC)
        await self.session.flush()
        return run

    async def delete(self, projection_uuid: str) -> None:
        """Delete a projection run by UUID."""
        run = await self.get_by_uuid(projection_uuid)
        if not run:
            raise EntityNotFoundError("projection_run", projection_uuid)

        stmt = delete(ProjectionRun).where(ProjectionRun.id == run.id)
        await self.session.execute(stmt)
        await self.session.flush()
</file>

<file path="alembic/versions/202510211200_add_projection_runs_table.py">
"""Add projection run tracking table.

Revision ID: 202510211200
Revises: 202510201015
Create Date: 2025-10-21 12:00:00.000000

"""

from __future__ import annotations

from typing import TYPE_CHECKING

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

from alembic import op

if TYPE_CHECKING:
    from collections.abc import Sequence

revision: str = "202510211200"
down_revision: str | Sequence[str] | None = "202510201015"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


projection_run_status = postgresql.ENUM(
    "pending",
    "running",
    "completed",
    "failed",
    "cancelled",
    name="projection_run_status",
    create_type=False,
)


def upgrade() -> None:
    """Create projection_runs table and extend operation_type enum."""

    op.execute("ALTER TYPE operation_type ADD VALUE IF NOT EXISTS 'projection_build'")

    # Ensure the ENUM exists without raising when the migration re-runs after a partial failure.
    op.execute(
        """
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1
                FROM pg_type t
                JOIN pg_namespace n ON n.oid = t.typnamespace
                WHERE t.typname = 'projection_run_status'
            ) THEN
                CREATE TYPE projection_run_status AS ENUM (
                    'pending',
                    'running',
                    'completed',
                    'failed',
                    'cancelled'
                );
            END IF;
        END
        $$;
        """
    )

    op.create_table(
        "projection_runs",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("uuid", sa.String(), nullable=False),
        sa.Column("collection_id", sa.String(), nullable=False),
        sa.Column("operation_uuid", sa.String(), nullable=True),
        sa.Column("status", projection_run_status, nullable=False, server_default="pending"),
        sa.Column("dimensionality", sa.Integer(), nullable=False),
        sa.Column("reducer", sa.String(), nullable=False),
        sa.Column("storage_path", sa.String(), nullable=True),
        sa.Column("point_count", sa.Integer(), nullable=True),
        sa.Column("config", sa.JSON(), nullable=True),
        sa.Column("meta", sa.JSON(), nullable=True),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            nullable=False,
            server_default=sa.text("timezone('utc', now())"),
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            nullable=False,
            server_default=sa.text("timezone('utc', now())"),
        ),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("completed_at", sa.DateTime(timezone=True), nullable=True),
        sa.ForeignKeyConstraint(["collection_id"], ["collections.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["operation_uuid"], ["operations.uuid"], ondelete="SET NULL"),
        sa.UniqueConstraint("uuid", name="uq_projection_runs_uuid"),
        sa.UniqueConstraint("operation_uuid", name="uq_projection_runs_operation_uuid"),
        sa.PrimaryKeyConstraint("id"),
        sa.CheckConstraint("dimensionality > 0", name="ck_projection_runs_dimensionality_positive"),
        sa.CheckConstraint(
            "point_count IS NULL OR point_count >= 0",
            name="ck_projection_runs_point_count_non_negative",
        ),
    )

    op.create_index("ix_projection_runs_collection_id", "projection_runs", ["collection_id"], unique=False)
    op.create_index("ix_projection_runs_status", "projection_runs", ["status"], unique=False)
    op.create_index("ix_projection_runs_created_at", "projection_runs", ["created_at"], unique=False)


def downgrade() -> None:
    """Drop projection_runs table and revert enum changes."""

    op.drop_index("ix_projection_runs_created_at", table_name="projection_runs")
    op.drop_index("ix_projection_runs_status", table_name="projection_runs")
    op.drop_index("ix_projection_runs_collection_id", table_name="projection_runs")
    op.drop_table("projection_runs")

    bind = op.get_bind()
    projection_run_status.drop(bind, checkfirst=True)

    op.execute("UPDATE operations SET type = 'INDEX' WHERE type IN ('PROJECTION_BUILD', 'projection_build')")
    op.execute("ALTER TYPE operation_type RENAME TO operation_type_old")
    op.execute("CREATE TYPE operation_type AS ENUM ('INDEX', 'APPEND', 'REINDEX', 'REMOVE_SOURCE', 'DELETE')")
    op.execute("ALTER TABLE operations ALTER COLUMN type TYPE operation_type USING type::text::operation_type")
    op.execute("DROP TYPE operation_type_old")
</file>

<file path="packages/webui/api/v2/projections.py">
"""Projection API v2 endpoints (scaffolding)."""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any

from fastapi import APIRouter, Depends, HTTPException, Response
from fastapi.responses import StreamingResponse

from packages.shared.database.exceptions import AccessDeniedError, EntityNotFoundError, ValidationError
from packages.webui.api.schemas import ErrorResponse
from packages.webui.api.v2.schemas import (
    ProjectionArrayResponse,
    ProjectionBuildRequest,
    ProjectionListResponse,
    ProjectionMetadataResponse,
    ProjectionSelectionItem,
    ProjectionSelectionRequest,
    ProjectionSelectionResponse,
)
from packages.webui.auth import get_current_user
from packages.webui.services.factory import get_projection_service

if TYPE_CHECKING:
    from packages.webui.services.projection_service import ProjectionService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v2/collections/{collection_id}/projections", tags=["projections-v2"])


def _to_metadata_response(
    collection_id: str, payload: dict[str, Any], *, fallback_id: str
) -> ProjectionMetadataResponse:
    """Normalise arbitrary projection metadata dictionaries."""

    return ProjectionMetadataResponse(
        id=payload.get("projection_id") or payload.get("id") or fallback_id,
        collection_id=payload.get("collection_id", collection_id),
        status=payload.get("status", "pending"),
        reducer=payload.get("reducer", "umap"),
        dimensionality=int(payload.get("dimensionality", 2) or 2),
        created_at=payload.get("created_at"),
        operation_id=payload.get("operation_id") or payload.get("operation_uuid"),
        operation_status=payload.get("operation_status"),
        message=payload.get("message"),
        config=payload.get("config"),
        meta=payload.get("meta"),
    )


@router.post(
    "",
    response_model=ProjectionMetadataResponse,
    status_code=202,
    responses={
        202: {"description": "Projection run accepted"},
        400: {"model": ErrorResponse},
        404: {"model": ErrorResponse},
    },
)
async def start_projection(
    collection_id: str,
    request: ProjectionBuildRequest,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> ProjectionMetadataResponse:
    """Schedule a projection build for the given collection (scaffold)."""

    try:
        result = await service.start_projection_build(collection_id, int(current_user["id"]), request.model_dump())
    except (EntityNotFoundError, AccessDeniedError) as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValidationError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc

    logger.debug("Projection run scheduled: %s", result)
    return _to_metadata_response(collection_id, result, fallback_id="pending-projection")


@router.get("", response_model=ProjectionListResponse)
async def list_projections(
    collection_id: str,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> ProjectionListResponse:
    """List projections for a collection (placeholder)."""

    runs = await service.list_projections(collection_id, int(current_user["id"]))
    projections = [
        _to_metadata_response(collection_id, payload, fallback_id=f"projection-{idx}")
        for idx, payload in enumerate(runs, start=1)
    ]
    return ProjectionListResponse(projections=projections)


@router.get(
    "/{projection_id}",
    response_model=ProjectionMetadataResponse,
    responses={404: {"model": ErrorResponse}},
)
async def get_projection(
    collection_id: str,
    projection_id: str,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> ProjectionMetadataResponse:
    """Return metadata for a projection run (placeholder)."""

    try:
        payload = await service.get_projection_metadata(collection_id, projection_id, int(current_user["id"]))
    except EntityNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc

    return _to_metadata_response(collection_id, payload, fallback_id=projection_id)


@router.get(
    "/{projection_id}/array",
    response_model=ProjectionArrayResponse,
)
async def get_projection_array(
    collection_id: str,
    projection_id: str,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> ProjectionArrayResponse:
    """Return projection coordinates as a binary payload (placeholder)."""

    data = await service.get_projection_array(collection_id, projection_id, int(current_user["id"]))

    if data:
        # TODO: stream the real binary payload rather than returning a placeholder message
        return ProjectionArrayResponse(
            projection_id=projection_id,
            message="Binary payload omitted in scaffold",
        )

    return ProjectionArrayResponse(projection_id=projection_id)


@router.get(
    "/{projection_id}/arrays/{artifact_name}",
    responses={
        200: {"description": "Projection artifact", "content": {"application/octet-stream": {}}},
        400: {"model": ErrorResponse},
        403: {"model": ErrorResponse},
        404: {"model": ErrorResponse},
    },
)
async def stream_projection_artifact(
    collection_id: str,
    projection_id: str,
    artifact_name: str,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
):
    """Stream one of the stored projection artifact files."""

    try:
        artifact_path = await service.resolve_artifact_path(
            collection_id,
            projection_id,
            artifact_name,
            int(current_user["id"]),
        )
    except EntityNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except AccessDeniedError as exc:
        raise HTTPException(status_code=403, detail=str(exc)) from exc
    except PermissionError as exc:
        raise HTTPException(status_code=403, detail=str(exc)) from exc
    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc

    file_stat = artifact_path.stat()

    def _file_iterator(chunk_size: int = 1024 * 1024):
        with artifact_path.open("rb") as buffer:
            while True:
                data = buffer.read(chunk_size)
                if not data:
                    break
                yield data

    headers = {
        "Cache-Control": "private, max-age=3600",
        "Content-Length": str(file_stat.st_size),
        "X-Content-Type-Options": "nosniff",
    }

    return StreamingResponse(
        _file_iterator(),
        media_type="application/octet-stream",
        headers=headers,
    )


@router.post(
    "/{projection_id}/select",
    response_model=ProjectionSelectionResponse,
)
async def select_projection_region(
    collection_id: str,
    projection_id: str,
    selection: ProjectionSelectionRequest,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> ProjectionSelectionResponse:
    """Open a selection over a projection (placeholder)."""

    payload = await service.select_projection_region(
        collection_id,
        projection_id,
        selection.model_dump(),
        int(current_user["id"]),
    )
    items = [ProjectionSelectionItem(**item) for item in payload.get("items", [])]
    return ProjectionSelectionResponse(
        projection_id=projection_id,
        items=items,
        missing_ids=[int(mid) for mid in payload.get("missing_ids", [])],
    )


@router.delete(
    "/{projection_id}",
    status_code=204,
    responses={
        204: {"description": "Projection deleted"},
        403: {"model": ErrorResponse},
        404: {"model": ErrorResponse},
    },
)
async def delete_projection(
    collection_id: str,
    projection_id: str,
    current_user: dict[str, Any] = Depends(get_current_user),
    service: ProjectionService = Depends(get_projection_service),
) -> Response:
    try:
        await service.delete_projection(collection_id, projection_id, int(current_user["id"]))
    except EntityNotFoundError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc
    except AccessDeniedError as exc:
        raise HTTPException(status_code=403, detail=str(exc)) from exc

    return Response(status_code=204)
</file>

<file path="apps/webui-react/src/components/EmbeddingVisualizationTab.tsx">
import { Suspense, useEffect, useMemo, useRef, useState, lazy } from 'react';
import { AlertCircle, Loader2, Play, Trash2, Eye, X } from 'lucide-react';
import {
  useCollectionProjections,
  useDeleteProjection,
  useStartProjection,
} from '../hooks/useProjections';
import { useOperationProgress } from '../hooks/useOperationProgress';
import { projectionsV2Api } from '../services/api/v2/projections';
import { searchV2Api } from '../services/api/v2/collections';
import { useUIStore } from '../stores/uiStore';
import type {
  ProjectionArtifactName,
  ProjectionLegendItem,
  ProjectionMetadata,
  ProjectionReducer,
  ProjectionSelectionItem,
  StartProjectionRequest,
} from '../types/projection';
import type { SearchResult } from '../services/api/v2/types';
import { getErrorMessage } from '../utils/errorUtils';

const EmbeddingView = lazy(() => import('embedding-atlas/react').then((mod) => ({ default: mod.EmbeddingView })));

interface ProjectionDataState {
  projectionId: string;
  pointCount: number;
  arrays?: {
    x: Float32Array;
    y: Float32Array;
    category: Uint8Array;
  };
  ids?: Int32Array;
  status: 'idle' | 'loading' | 'loaded' | 'error';
  error?: string;
}

interface EmbeddingVisualizationTabProps {
  collectionId: string;
}

const COLOR_BY_OPTIONS: Array<{ value: string; label: string }> = [
  { value: 'document_id', label: 'Document' },
  { value: 'source_dir', label: 'Source Folder' },
  { value: 'filetype', label: 'File Type' },
  { value: 'age_bucket', label: 'Age Bucket' },
];

const METRIC_OPTIONS = ['cosine', 'euclidean', 'manhattan'];
const SAMPLE_LIMIT_CAP = 200_000;

const REDUCER_OPTIONS: Array<{
  value: ProjectionReducer;
  label: string;
  description: string;
}> = [
  {
    value: 'umap',
    label: 'UMAP',
    description: 'Uniform Manifold Approximation and Projection (fast, good global + local structure).',
  },
  {
    value: 'tsne',
    label: 't-SNE',
    description: 't-distributed Stochastic Neighbor Embedding (great local detail, slower).',
  },
  {
    value: 'pca',
    label: 'PCA',
    description: 'Principal Component Analysis (linear baseline, very fast).',
  },
];

function statusBadge(status: ProjectionMetadata['status'] | string) {
  const base = 'px-2 py-1 rounded-full text-xs font-medium';
  switch (status) {
    case 'completed':
      return <span className={`${base} bg-green-100 text-green-800`}>Completed</span>;
    case 'running':
    case 'processing':
      return <span className={`${base} bg-blue-100 text-blue-800`}>Processing</span>;
    case 'failed':
      return <span className={`${base} bg-red-100 text-red-800`}>Failed</span>;
    case 'cancelled':
      return <span className={`${base} bg-gray-100 text-gray-600`}>Cancelled</span>;
    case 'pending':
      return <span className={`${base} bg-amber-100 text-amber-800`}>Pending</span>;
    default:
      return <span className={`${base} bg-amber-100 text-amber-800`}>Pending</span>;
  }
}

function projectionProgress(status: ProjectionMetadata['status'] | string) {
  if (status === 'completed') return 100;
  if (status === 'running' || status === 'processing') return 60;
  if (status === 'pending') return 10;
  return 0;
}

export function EmbeddingVisualizationTab({ collectionId }: EmbeddingVisualizationTabProps) {
  const [selectedReducer, setSelectedReducer] = useState<ProjectionReducer>('umap');
  const [selectedColorBy, setSelectedColorBy] = useState<string>('document_id');
  const [activeProjection, setActiveProjection] = useState<ProjectionDataState>({
    projectionId: '',
    pointCount: 0,
    status: 'idle',
  });
  const [activeProjectionMeta, setActiveProjectionMeta] = useState<{
    color_by?: string;
    legend?: ProjectionLegendItem[];
    sampled?: boolean;
    shown_count?: number;
    total_count?: number;
    degraded?: boolean;
  } | null>(null);
  const [selectionState, setSelectionState] = useState<{
    indices: number[];
    items: ProjectionSelectionItem[];
    missing: number[];
    loading: boolean;
    error?: string;
  }>({ indices: [], items: [], missing: [], loading: false });
  const selectionRequestId = useRef(0);
  const [similarSearchState, setSimilarSearchState] = useState<{
    loading: boolean;
    error: string | null;
    results: SearchResult[];
    visible: boolean;
  }>({ loading: false, error: null, results: [], visible: false });
  const [recomputeDialogOpen, setRecomputeDialogOpen] = useState(false);
  const [pendingOperationId, setPendingOperationId] = useState<string | null>(null);
  const [recomputeReducer, setRecomputeReducer] = useState<ProjectionReducer>('umap');
  const [recomputeParams, setRecomputeParams] = useState({
    n_neighbors: '15',
    min_dist: '0.1',
    metric: 'cosine',
    sample_n: '',
  });
  const [recomputeError, setRecomputeError] = useState<string | undefined>(undefined);
  const [currentOperationStatus, setCurrentOperationStatus] = useState<string | null>(null);
  const activeRequestId = useRef(0);
  const viewContainerRef = useRef<HTMLDivElement | null>(null);
  const [viewSize, setViewSize] = useState<{ width: number; height: number }>({ width: 960, height: 540 });
  const [pixelRatio, setPixelRatio] = useState<number>(typeof window !== 'undefined' ? window.devicePixelRatio || 1 : 1);
  const { data: projections = [], isLoading, refetch } = useCollectionProjections(collectionId);
  const startProjection = useStartProjection(collectionId);
  const deleteProjection = useDeleteProjection(collectionId);
  const { setShowDocumentViewer, addToast } = useUIStore();

  const { isConnected: isOperationConnected } = useOperationProgress(pendingOperationId, {
    showToasts: false,
    onComplete: () => {
      setPendingOperationId(null);
      setCurrentOperationStatus(null);
      refetch();
    },
    onError: (errorMessage) => {
      setPendingOperationId(null);
      setCurrentOperationStatus(null);
      if (errorMessage) {
        setRecomputeError(errorMessage);
      }
    },
  });

  useEffect(() => {
    if (typeof window === 'undefined') {
      return;
    }
    const handlePixelRatio = () => setPixelRatio(window.devicePixelRatio || 1);
    window.addEventListener('resize', handlePixelRatio);
    return () => {
      window.removeEventListener('resize', handlePixelRatio);
    };
  }, []);

  useEffect(() => {
    if (!viewContainerRef.current || typeof window === 'undefined' || typeof ResizeObserver === 'undefined') {
      return;
    }
    const node = viewContainerRef.current;
    const observer = new ResizeObserver((entries) => {
      const entry = entries[0];
      if (!entry) return;
      const width = Math.max(360, entry.contentRect.width);
      const height = Math.max(320, Math.round(width * 0.6));
      setViewSize({ width, height });
    });
    observer.observe(node);
    return () => {
      observer.disconnect();
    };
  }, [viewContainerRef]);

  const sortedProjections = useMemo(
    () =>
      [...projections].sort((a, b) => {
        const aDate = a.created_at ? new Date(a.created_at).getTime() : 0;
        const bDate = b.created_at ? new Date(b.created_at).getTime() : 0;
        return bDate - aDate;
      }),
    [projections]
  );

  // Find in-progress projection for fallback status display (when WebSocket not available)
  const inProgressProjection = useMemo(() => {
    if (pendingOperationId) return null; // Don't show fallback if we have active pending op
    return sortedProjections.find((p) => {
      const status = p.operation_status || p.status;
      return status === 'processing' || status === 'pending' || status === 'running';
    });
  }, [sortedProjections, pendingOperationId]);

  const startProjectionWithPayload = async (payload: StartProjectionRequest) => {
    const response = await startProjection.mutateAsync(payload);
    if (response?.operation_id) {
      setPendingOperationId(response.operation_id);
      setCurrentOperationStatus(response.operation_status || null);
    }
    refetch();
    setActiveProjectionMeta(null);
    setSelectionState({ indices: [], items: [], missing: [], loading: false });
    return response;
  };

  const handleStartProjection = async () => {
    await startProjectionWithPayload({ reducer: selectedReducer, color_by: selectedColorBy });
  };

  const handleDeleteProjection = async (projectionId: string) => {
    await deleteProjection.mutateAsync(projectionId);
    refetch();
    setActiveProjection((prev) =>
      prev.projectionId === projectionId
        ? { projectionId: '', pointCount: 0, status: 'idle' }
        : prev
    );
    setActiveProjectionMeta(null);
  };

  const handleViewProjection = async (projection: ProjectionMetadata) => {
    if (projection.status !== 'completed') return;

    const requestId = ++activeRequestId.current;
    setActiveProjection({ projectionId: projection.id, pointCount: 0, status: 'loading' });
    setActiveProjectionMeta(null);
    setSelectionState({ indices: [], items: [], missing: [], loading: false });

    try {
      const metadataResponse = await projectionsV2Api.getMetadata(collectionId, projection.id);
      const metaPayload = (metadataResponse.data?.meta as Record<string, unknown> | null) ?? {};
      const legendPayload = Array.isArray(metaPayload?.legend)
        ? (metaPayload.legend as ProjectionLegendItem[])
        : [];
      const metaColorBy = typeof metaPayload?.color_by === 'string'
        ? (metaPayload.color_by as string)
        : typeof metaPayload?.colorBy === 'string'
          ? (metaPayload.colorBy as string)
          : undefined;
      const metaSampled = Boolean(metaPayload?.sampled);
      const shownCountRaw = metaPayload?.shown_count ?? metaPayload?.shownCount ?? metaPayload?.point_count;
      const totalCountRaw = metaPayload?.total_count ?? metaPayload?.totalCount;
      const metaDegraded = Boolean(metaPayload?.degraded);

      const arrayNames: ProjectionArtifactName[] = ['x', 'y', 'cat', 'ids'];
      const responses = await Promise.all(
        arrayNames.map(async (name) => {
          const res = await projectionsV2Api.getArtifact(collectionId, projection.id, name);
          return res.data;
        })
      );

      const [xBuf, yBuf, catBuf, idsBuf] = responses;
      const x = new Float32Array(xBuf);
      const y = new Float32Array(yBuf);
      const category = new Uint8Array(catBuf);
      const ids = new Int32Array(idsBuf);

      if (x.length !== y.length || x.length !== category.length) {
        throw new Error('Projection arrays have inconsistent lengths');
      }

      if (requestId === activeRequestId.current) {
        const parsedShownCount = (() => {
          const value = Number(shownCountRaw);
          return Number.isFinite(value) && value > 0 ? value : x.length;
        })();
        const parsedTotalCount = (() => {
          const value = Number(totalCountRaw);
          if (Number.isFinite(value) && value > 0) {
            return Math.max(value, parsedShownCount);
          }
          return metaSampled ? Math.max(parsedShownCount, x.length) : x.length;
        })();

        setActiveProjection({
          projectionId: projection.id,
          pointCount: x.length,
          arrays: { x, y, category },
          ids,
          status: 'loaded',
        });
        setActiveProjectionMeta({
          color_by: metaColorBy,
          legend: legendPayload,
          sampled: metaSampled,
          shown_count: parsedShownCount,
          total_count: parsedTotalCount,
          degraded: metaDegraded,
        });
      }
    } catch (error: unknown) {
      if (requestId === activeRequestId.current) {
        setActiveProjection({
          projectionId: projection.id,
          pointCount: 0,
          status: 'error',
          error: error instanceof Error ? error.message : 'Failed to load projection data',
        });
        setActiveProjectionMeta(null);
      }
    }
  };

  const handleRecomputeProjection = async () => {
    openRecomputeDialog();
  };

  const shownCountDisplay = activeProjectionMeta?.shown_count ?? activeProjection.pointCount;
  const totalCountDisplay = activeProjectionMeta?.total_count ?? Math.max(shownCountDisplay, activeProjection.pointCount);

  const handleSelectionChange = async (indices: number[]) => {
    if (!activeProjection.projectionId || !activeProjection.ids) {
      setSelectionState({ indices: [], items: [], missing: [], loading: false });
      return;
    }

    const uniqueIndices = Array.from(new Set(indices)).filter(
      (index) => index >= 0 && index < activeProjection.ids!.length
    );

    if (uniqueIndices.length === 0) {
      setSelectionState({ indices: [], items: [], missing: [], loading: false });
      return;
    }

    const mappedIds = uniqueIndices
      .map((index) => activeProjection.ids![index])
      .filter((id): id is number => typeof id === 'number' && Number.isFinite(id));

    if (mappedIds.length === 0) {
      setSelectionState({ indices: [], items: [], missing: [], loading: false });
      return;
    }

    const requestId = ++selectionRequestId.current;
    setSelectionState((prev) => ({ ...prev, indices: uniqueIndices, loading: true, error: undefined }));
    try {
      const response = await projectionsV2Api.select(collectionId, activeProjection.projectionId, mappedIds);
      if (requestId === selectionRequestId.current) {
        setSelectionState({
          indices: uniqueIndices,
          items: response.data?.items ?? [],
          missing: response.data?.missing_ids ?? [],
          loading: false,
        });
        if (response.data?.degraded && activeProjectionMeta) {
          setActiveProjectionMeta({ ...activeProjectionMeta, degraded: true });
        }
      }
    } catch (error) {
      if (requestId === selectionRequestId.current) {
        setSelectionState({
          indices: uniqueIndices,
          items: [],
          missing: [],
          loading: false,
          error: error instanceof Error ? error.message : 'Failed to resolve selection',
        });
      }
    }
  };

  const handleOpenDocument = (item: ProjectionSelectionItem) => {
    if (!item.document_id) {
      addToast({ type: 'error', message: 'No document available to open' });
      return;
    }

    // Analytics logging
    console.log('projection_selection_open', {
      collectionId,
      documentId: item.document_id,
      chunkId: item.chunk_id,
      chunkIndex: item.chunk_index,
      timestamp: new Date().toISOString(),
    });

    // Open document viewer with chunk context
    setShowDocumentViewer({
      collectionId,
      docId: item.document_id,
      chunkId: item.chunk_id !== null && item.chunk_id !== undefined ? String(item.chunk_id) : undefined,
    });
  };

  const handleFindSimilar = async (item: ProjectionSelectionItem) => {
    if (!item.content_preview) {
      addToast({ type: 'error', message: 'No content available to search with' });
      return;
    }

    // Truncate query to reasonable length (500 chars)
    const query = item.content_preview.slice(0, 500);

    setSimilarSearchState({
      loading: true,
      error: null,
      results: [],
      visible: true,
    });

    try {
      const response = await searchV2Api.search({
        query,
        collection_uuids: [collectionId],
        k: 10,
        search_type: 'semantic',
      });

      // Analytics logging
      console.log('projection_selection_find_similar', {
        collectionId,
        chunkId: item.chunk_id,
        query: query.slice(0, 100), // Log truncated query
        resultCount: response.data?.results?.length ?? 0,
        timestamp: new Date().toISOString(),
      });

      setSimilarSearchState({
        loading: false,
        error: null,
        results: response.data?.results ?? [],
        visible: true,
      });
    } catch (error) {
      const errorMessage = getErrorMessage(error);
      setSimilarSearchState({
        loading: false,
        error: errorMessage,
        results: [],
        visible: true,
      });
      addToast({ type: 'error', message: `Failed to find similar chunks: ${errorMessage}` });
    }
  };

  const openRecomputeDialog = () => {
    setRecomputeReducer(selectedReducer);
    setRecomputeParams({
      n_neighbors: '15',
      min_dist: '0.1',
      metric: 'cosine',
      sample_n: '',
    });
    setRecomputeError(undefined);
    setRecomputeDialogOpen(true);
  };

  const closeRecomputeDialog = () => {
    if (!startProjection.isPending) {
      setRecomputeDialogOpen(false);
      setRecomputeError(undefined);
    }
  };

  const handleRecomputeSubmit = async () => {
    setRecomputeError(undefined);

    const payload: StartProjectionRequest = {
      reducer: recomputeReducer,
      color_by: selectedColorBy,
    };
    const config: Record<string, unknown> = {};
    if (recomputeReducer === 'umap') {
      const parsedNeighbors = Number(recomputeParams.n_neighbors);
      const parsedMinDist = Number(recomputeParams.min_dist);
      const metricValue = recomputeParams.metric?.trim() || 'cosine';
      const errors: string[] = [];

      if (!Number.isFinite(parsedNeighbors) || parsedNeighbors < 2) {
        errors.push('n_neighbors must be a number  2.');
      }
      if (!Number.isFinite(parsedMinDist) || parsedMinDist < 0 || parsedMinDist > 1) {
        errors.push('min_dist must be between 0 and 1.');
      }
      if (!metricValue) {
        errors.push('metric is required.');
      }

      if (errors.length > 0) {
        setRecomputeError(errors.join(' '));
        return;
      }

      config.n_neighbors = Math.floor(parsedNeighbors);
      config.min_dist = parsedMinDist;
      config.metric = metricValue;
    }
    if (recomputeParams.sample_n !== '') {
      const sampleNumeric = Number(recomputeParams.sample_n);
      if (!Number.isFinite(sampleNumeric) || sampleNumeric <= 0) {
        setRecomputeError('Sample size must be a positive number.');
        return;
      }
      config.sample_size = Math.floor(sampleNumeric);
    }

    if (Object.keys(config).length > 0) {
      payload.config = config;
    }

    try {
      await startProjectionWithPayload(payload);
      setRecomputeDialogOpen(false);
      setRecomputeError(undefined);
    } catch (error) {
      setRecomputeError(error instanceof Error ? error.message : 'Failed to start projection');
    }
  };

  return (
    <div className="space-y-6">
      <section className="bg-white border border-gray-200 rounded-lg p-4 shadow-sm">
        <h3 className="text-lg font-semibold text-gray-900 mb-4">Start a new projection</h3>
        <div className="space-y-4">
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-2">
              Projection method
            </label>
            <div className="grid md:grid-cols-3 gap-3">
              {REDUCER_OPTIONS.map((option) => (
                <button
                  key={option.value}
                  type="button"
                  onClick={() => setSelectedReducer(option.value)}
                  className={`border rounded-lg p-3 text-left transition-colors ${
                    selectedReducer === option.value
                      ? 'border-purple-500 bg-purple-50'
                      : 'border-gray-200 hover:border-purple-300'
                  }`}
                >
                  <div className="font-medium text-gray-900">{option.label}</div>
                  <p className="text-sm text-gray-600 mt-1">{option.description}</p>
                </button>
              ))}
            </div>
          </div>
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-2">Color by</label>
            <select
              value={selectedColorBy}
              onChange={(event) => setSelectedColorBy(event.target.value)}
              className="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
            >
              {COLOR_BY_OPTIONS.map((option) => (
                <option key={option.value} value={option.value}>
                  {option.label}
                </option>
              ))}
            </select>
          </div>
          <button
            type="button"
            onClick={handleStartProjection}
            disabled={startProjection.isPending}
            className="inline-flex items-center px-4 py-2 bg-purple-600 text-white rounded-md shadow-sm hover:bg-purple-700 disabled:opacity-50"
          >
            {startProjection.isPending ? (
              <Loader2 className="h-4 w-4 mr-2 animate-spin" />
            ) : (
              <Play className="h-4 w-4 mr-2" />
            )}
            Start Projection
          </button>
        </div>
      </section>

      {pendingOperationId && (
        <div className="rounded-md border border-blue-200 bg-blue-50 px-4 py-3 text-sm text-blue-700">
          <div className="flex items-center gap-2">
            <span className="font-medium">Projection recompute in progress</span>
            {currentOperationStatus && (
              <span className="px-2 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800">
                {currentOperationStatus}
              </span>
            )}
          </div>
          <div className="mt-1 flex flex-wrap items-center gap-2 text-xs text-blue-600">
            <span>Operation ID: {pendingOperationId}</span>
            <span>{isOperationConnected ? 'Live updates active.' : 'Connecting to progress updates'}</span>
            {!isOperationConnected && currentOperationStatus && (
              <span className="italic">Last known status: {currentOperationStatus}</span>
            )}
          </div>
        </div>
      )}

      {!pendingOperationId && inProgressProjection && (
        <div className="rounded-md border border-amber-200 bg-amber-50 px-4 py-3 text-sm text-amber-700">
          <div className="flex items-center gap-2">
            <span className="font-medium">Projection in progress</span>
            <span className="px-2 py-0.5 rounded-full text-xs font-medium bg-amber-100 text-amber-800">
              {inProgressProjection.operation_status || inProgressProjection.status}
            </span>
          </div>
          <div className="mt-1 flex flex-wrap items-center gap-2 text-xs text-amber-600">
            <span>Projection: {inProgressProjection.reducer.toUpperCase()}</span>
            {inProgressProjection.operation_id && (
              <span>Operation ID: {inProgressProjection.operation_id}</span>
            )}
            <span className="italic">Status from last refresh (WebSocket unavailable)</span>
          </div>
        </div>
      )}

      <section className="bg-white border border-gray-200 rounded-lg shadow-sm">
        <header className="flex items-center justify-between px-4 py-3 border-b border-gray-200">
          <h3 className="text-lg font-semibold text-gray-900">Projection runs</h3>
          {isLoading && <Loader2 className="h-4 w-4 animate-spin text-purple-600" />}
        </header>
        {sortedProjections.length === 0 ? (
          <div className="p-6 text-center text-gray-500">
            <p className="font-medium text-gray-700 mb-2">No projections yet</p>
            <p className="text-sm">Start a projection to generate a 2D representation of your embeddings.</p>
          </div>
        ) : (
          <div className="overflow-hidden">
            <table className="min-w-full divide-y divide-gray-200">
              <thead className="bg-gray-50">
                <tr>
                  <th className="px-4 py-3 text-left text-xs font-semibold text-gray-500 uppercase tracking-wider">
                    Projection
                  </th>
                  <th className="px-4 py-3 text-left text-xs font-semibold text-gray-500 uppercase tracking-wider">
                    Status
                  </th>
                  <th className="px-4 py-3 text-left text-xs font-semibold text-gray-500 uppercase tracking-wider">
                    Progress
                  </th>
                  <th className="px-4 py-3 text-left text-xs font-semibold text-gray-500 uppercase tracking-wider">
                    Created
                  </th>
                  <th className="px-4 py-3" />
                </tr>
              </thead>
              <tbody className="bg-white divide-y divide-gray-200">
                {sortedProjections.map((projection) => {
                  // Prefer operation_status over projection.status for more accurate state
                  const displayStatus = projection.operation_status || projection.status;
                  const progress = projectionProgress(displayStatus);
                  return (
                    <tr key={projection.id} className="hover:bg-gray-50">
                      <td className="px-4 py-3 text-sm text-gray-900">
                        <div className="font-medium text-gray-900">{projection.reducer.toUpperCase()}</div>
                        <div className="text-xs text-gray-500">ID: {projection.id}</div>
                        {projection.message && (
                          <div className="mt-1 text-xs text-gray-500">{projection.message}</div>
                        )}
                      </td>
                      <td className="px-4 py-3">{statusBadge(displayStatus)}</td>
                      <td className="px-4 py-3">
                        <div className="h-2 rounded bg-gray-200">
                          <div
                            className="h-2 rounded bg-purple-500 transition-all"
                            style={{ width: `${progress}%` }}
                          />
                        </div>
                      </td>
                      <td className="px-4 py-3 text-sm text-gray-500">
                        {projection.created_at
                          ? new Date(projection.created_at).toLocaleString()
                          : ''}
                      </td>
                      <td className="px-4 py-3 text-right">
                        <div className="flex items-center justify-end gap-3">
                          {projection.status === 'completed' && (
                            <button
                              type="button"
                              onClick={() => handleViewProjection(projection)}
                              className="text-sm text-purple-600 hover:text-purple-800 inline-flex items-center"
                            >
                              <Eye className="h-4 w-4 mr-1" /> View
                            </button>
                          )}
                          <button
                            type="button"
                            onClick={() => handleDeleteProjection(projection.id)}
                            disabled={deleteProjection.isPending}
                            className="text-sm text-red-600 hover:text-red-800 inline-flex items-center"
                          >
                            <Trash2 className="h-4 w-4 mr-1" /> Delete
                          </button>
                        </div>
                      </td>
                    </tr>
                  );
                })}
              </tbody>
            </table>
          </div>
        )}
      </section>

      <section className="bg-white border border-gray-200 rounded-lg shadow-sm p-4">
        <h3 className="text-lg font-semibold text-gray-900 mb-3">Projection preview</h3>
        {activeProjection.status === 'idle' && (
          <div className="text-sm text-gray-600">
            Select a completed projection to load its coordinates and visualize the embedding.
          </div>
        )}

        {activeProjection.status === 'loading' && (
          <div className="flex items-center gap-2 text-sm text-purple-700">
            <Loader2 className="h-4 w-4 animate-spin" /> Loading projection data
          </div>
        )}

        {activeProjection.status === 'error' && (
          <div className="flex items-center gap-2 text-sm text-red-600">
            <AlertCircle className="h-4 w-4" /> {activeProjection.error}
          </div>
        )}

        {activeProjection.status === 'loaded' && activeProjection.arrays && (
          <div className="space-y-4">
            <div className="flex flex-wrap items-center gap-3 text-sm text-gray-700">
              <span>
                Loaded <span className="font-semibold">{activeProjection.pointCount}</span> points.
                Categories: {new Set(activeProjection.arrays.category).size}.
              </span>
              {activeProjectionMeta?.sampled && (
                <span
                  className="inline-flex items-center gap-1 rounded-full border border-amber-300 bg-amber-50 px-2 py-0.5 text-xs font-medium text-amber-700"
                  title={`Showing ${shownCountDisplay.toLocaleString()} of ${totalCountDisplay.toLocaleString()} points`}
                >
                  Sampled
                </span>
              )}
            </div>
            <div className="flex flex-col gap-4">
              {activeProjectionMeta?.color_by && (
                <div className="flex items-center justify-between">
                  <p className="text-sm text-gray-600">
                    Colored by <span className="font-medium">{activeProjectionMeta.color_by}</span>
                  </p>
                  {(activeProjectionMeta.color_by !== selectedColorBy || activeProjectionMeta.degraded) && (
                    <button
                      type="button"
                      onClick={handleRecomputeProjection}
                      disabled={startProjection.isPending}
                      className="inline-flex items-center px-3 py-1.5 text-sm rounded-md border border-purple-500 text-purple-600 hover:bg-purple-50 disabled:opacity-50"
                    >
                      {startProjection.isPending ? (
                        <Loader2 className="h-4 w-4 mr-2 animate-spin" />
                      ) : null}
                      {activeProjectionMeta.degraded
                        ? 'Recompute to refresh'
                        : `Recompute with ${COLOR_BY_OPTIONS.find((opt) => opt.value === selectedColorBy)?.label ?? selectedColorBy}`}
                    </button>
                  )}
                </div>
              )}
              {activeProjectionMeta?.legend && activeProjectionMeta.legend.length > 0 && (
                <div className="bg-gray-50 border border-gray-200 rounded-md p-3">
                  <h4 className="text-sm font-semibold text-gray-700 mb-2">Legend</h4>
                  <ul className="max-h-48 overflow-y-auto text-sm text-gray-600 space-y-1">
                    {activeProjectionMeta.legend.map((entry) => (
                      <li key={entry.index} className="flex items-center justify-between">
                        <span>{entry.label}</span>
                        {typeof entry.count === 'number' && (
                          <span className="text-xs text-gray-500">{entry.count.toLocaleString()}</span>
                        )}
                      </li>
                    ))}
                  </ul>
                </div>
              )}
            </div>
            <div
              className="border border-gray-200 rounded-md overflow-hidden"
              ref={viewContainerRef}
              style={{ minHeight: '320px' }}
            >
              <Suspense fallback={<div className="p-4 text-sm text-purple-700">Rendering projection</div>}>
                <EmbeddingView
                  data={{
                    x: activeProjection.arrays.x,
                    y: activeProjection.arrays.y,
                    category: activeProjection.arrays.category,
                  }}
                  width={viewSize.width}
                  height={viewSize.height}
                  pixelRatio={pixelRatio}
                  theme={{ statusBar: false }}
                  onSelection={(points) => {
                    const indices = Array.isArray(points)
                      ? points
                          .map((point) => {
                            if (point && typeof point === 'object' && 'index' in point) {
                              const idx = (point as { index: unknown }).index;
                              return typeof idx === 'number' ? idx : -1;
                            }
                            return -1;
                          })
                          .filter((idx) => typeof idx === 'number' && idx >= 0)
                      : [];
                    void handleSelectionChange(indices);
                  }}
                />
              </Suspense>
            </div>
            {(selectionState.items.length > 0 || selectionState.loading || selectionState.error) && (
              <div className="border border-gray-200 rounded-md p-4 bg-white shadow-sm">
                <div className="flex items-center justify-between mb-3">
                  <h4 className="text-sm font-semibold text-gray-800">Selection</h4>
                  <span className="text-xs text-gray-500">
                    Indices: {selectionState.indices.length.toLocaleString()}
                  </span>
                </div>
                {selectionState.loading && (
                  <div className="flex items-center gap-2 text-sm text-purple-600">
                    <Loader2 className="h-4 w-4 animate-spin" /> Resolving selection
                  </div>
                )}
                {selectionState.error && !selectionState.loading && (
                  <div className="text-sm text-red-600">{selectionState.error}</div>
                )}
                {!selectionState.loading && selectionState.items.length > 0 && (
                  <>
                    {selectionState.items.length > 1 && (
                      <p className="text-xs text-gray-500 mb-2 italic">
                        Actions apply to the first selected point
                      </p>
                    )}
                    <ul className="space-y-3 text-sm text-gray-700 max-h-64 overflow-y-auto">
                      {selectionState.items.map((item) => (
                        <li key={`${item.selected_id}-${item.index}`} className="border border-gray-200 rounded-md p-3">
                          <div className="text-xs text-gray-500 mb-1">
                            Point #{item.index + 1}  ID {item.selected_id}
                          </div>
                          {item.document_id && (
                            <div className="font-medium text-gray-900">Document {item.document_id}</div>
                          )}
                          {item.chunk_index !== undefined && item.chunk_index !== null && (
                            <div className="text-xs text-gray-500">Chunk #{item.chunk_index}</div>
                          )}
                          {item.content_preview && (
                            <p className="mt-2 text-sm text-gray-600 line-clamp-3">{item.content_preview}</p>
                          )}
                          <div className="mt-3 flex gap-2">
                            <button
                              type="button"
                              onClick={() => handleOpenDocument(item)}
                              disabled={!item.document_id}
                              title="View the full document containing this chunk"
                              className="text-xs px-2 py-1 rounded border border-gray-300 text-gray-600 hover:bg-gray-100 disabled:opacity-50 disabled:cursor-not-allowed"
                            >
                              Open
                            </button>
                            <button
                              type="button"
                              onClick={() => handleFindSimilar(item)}
                              disabled={!item.content_preview || similarSearchState.loading}
                              title="Search for semantically similar content"
                              className="text-xs px-2 py-1 rounded border border-purple-400 text-purple-600 hover:bg-purple-50 disabled:opacity-50 disabled:cursor-not-allowed"
                            >
                              {similarSearchState.loading ? 'Searching...' : 'Find Similar'}
                            </button>
                          </div>
                        </li>
                      ))}
                    </ul>
                  </>
                )}
                {!selectionState.loading && selectionState.items.length === 0 && !selectionState.error && (
                  <p className="text-sm text-gray-500">No metadata available for the selected points.</p>
                )}
                {selectionState.missing.length > 0 && (
                  <p className="mt-2 text-xs text-amber-600">
                    {selectionState.missing.length.toLocaleString()} point(s) could not be resolved.
                  </p>
                )}

                {/* Similar Results Section */}
                {similarSearchState.visible && (
                  <div className="mt-4 pt-4 border-t border-gray-200">
                    <div className="flex items-center justify-between mb-3">
                      <h5 className="text-sm font-semibold text-gray-800">Similar Chunks</h5>
                      <button
                        type="button"
                        onClick={() => setSimilarSearchState((prev) => ({ ...prev, visible: false }))}
                        className="text-gray-500 hover:text-gray-700"
                        title="Close similar results"
                      >
                        <X className="h-4 w-4" />
                      </button>
                    </div>

                    {similarSearchState.loading && (
                      <div className="flex items-center gap-2 text-sm text-purple-600">
                        <Loader2 className="h-4 w-4 animate-spin" /> Searching for similar chunks
                      </div>
                    )}

                    {similarSearchState.error && !similarSearchState.loading && (
                      <div className="text-sm text-red-600">{similarSearchState.error}</div>
                    )}

                    {!similarSearchState.loading && similarSearchState.results.length > 0 && (
                      <ul className="space-y-2 text-sm text-gray-700 max-h-96 overflow-y-auto">
                        {similarSearchState.results.map((result) => (
                          <li
                            key={`${result.document_id}-${result.chunk_index}`}
                            className="border border-gray-200 rounded-md p-3 hover:bg-gray-50"
                          >
                            <div className="flex items-center justify-between mb-1">
                              <div className="font-medium text-gray-900 text-xs truncate">
                                {result.file_name}
                              </div>
                              <span className="text-xs text-purple-600 font-medium ml-2">
                                {(result.score * 100).toFixed(1)}%
                              </span>
                            </div>
                            <div className="text-xs text-gray-500 mb-2">
                              Chunk #{result.chunk_index}
                            </div>
                            <p className="text-sm text-gray-600 line-clamp-2 mb-2">{result.text}</p>
                            <button
                              type="button"
                              onClick={() => {
                                setShowDocumentViewer({
                                  collectionId,
                                  docId: result.document_id,
                                  chunkId: result.chunk_id,
                                });
                              }}
                              className="text-xs px-2 py-1 rounded border border-gray-300 text-gray-600 hover:bg-gray-100"
                            >
                              Open
                            </button>
                          </li>
                        ))}
                      </ul>
                    )}

                    {!similarSearchState.loading && similarSearchState.results.length === 0 && !similarSearchState.error && (
                      <p className="text-sm text-gray-500">No similar chunks found.</p>
                    )}
                  </div>
                )}
              </div>
            )}
          </div>
        )}
      </section>

      {recomputeDialogOpen && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black/50 px-4">
          <div className="w-full max-w-lg rounded-lg bg-white p-6 shadow-xl">
            <h3 className="text-lg font-semibold text-gray-900">Recompute Projection</h3>
            <p className="mt-2 text-sm text-gray-600">
              Choose reducer and sampling parameters for the new projection run.
            </p>

            <div className="mt-4 space-y-4">
              <div>
                <label className="block text-sm font-medium text-gray-700 mb-1">Reducer</label>
                <select
                  value={recomputeReducer}
                  onChange={(event) => setRecomputeReducer(event.target.value as ProjectionReducer)}
                  className="block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
                >
                  <option value="umap">UMAP</option>
                  <option value="pca">PCA</option>
                </select>
              </div>

              {recomputeReducer === 'umap' && (
                <div className="grid gap-4 md:grid-cols-2">
                  <div>
                    <label className="block text-sm font-medium text-gray-700 mb-1">n_neighbors</label>
                    <input
                      type="number"
                      min={2}
                      value={recomputeParams.n_neighbors}
                      onChange={(event) =>
                        setRecomputeParams((prev) => ({ ...prev, n_neighbors: event.target.value }))
                      }
                      className="block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
                    />
                  </div>
                  <div>
                    <label className="block text-sm font-medium text-gray-700 mb-1">min_dist</label>
                    <input
                      type="number"
                      min={0}
                      max={1}
                      step={0.05}
                      value={recomputeParams.min_dist}
                      onChange={(event) =>
                        setRecomputeParams((prev) => ({ ...prev, min_dist: event.target.value }))
                      }
                      className="block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
                    />
                  </div>
                  <div className="md:col-span-2">
                    <label className="block text-sm font-medium text-gray-700 mb-1">Metric</label>
                    <select
                      value={recomputeParams.metric}
                      onChange={(event) =>
                        setRecomputeParams((prev) => ({ ...prev, metric: event.target.value }))
                      }
                      className="block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
                    >
                      {METRIC_OPTIONS.map((metricOption) => (
                        <option key={metricOption} value={metricOption}>
                          {metricOption}
                        </option>
                      ))}
                    </select>
                  </div>
                </div>
              )}

              <div>
                <label className="block text-sm font-medium text-gray-700 mb-1">Sample size</label>
                <input
                  type="number"
                  min={1}
                  placeholder="Optional"
                  value={recomputeParams.sample_n}
                  onChange={(event) =>
                    setRecomputeParams((prev) => ({ ...prev, sample_n: event.target.value }))
                  }
                  className="block w-full rounded-md border-gray-300 shadow-sm focus:border-purple-500 focus:ring-purple-500 sm:text-sm"
                />
                <p className="mt-1 text-xs text-gray-500">
                  Leave blank to use the default cap ({SAMPLE_LIMIT_CAP.toLocaleString()} points).
                </p>
              </div>

              {recomputeError && (
                <div className="rounded-md border border-red-200 bg-red-50 px-3 py-2 text-sm text-red-700">
                  {recomputeError}
                </div>
              )}
            </div>

            <div className="mt-6 flex justify-end gap-2">
              <button
                type="button"
                onClick={closeRecomputeDialog}
                className="inline-flex items-center rounded-md border border-gray-300 px-4 py-2 text-sm font-medium text-gray-700 hover:bg-gray-100"
                disabled={startProjection.isPending}
              >
                Cancel
              </button>
              <button
                type="button"
                onClick={handleRecomputeSubmit}
                disabled={startProjection.isPending}
                className="inline-flex items-center rounded-md bg-purple-600 px-4 py-2 text-sm font-medium text-white hover:bg-purple-700 disabled:opacity-50"
              >
                {startProjection.isPending ? (
                  <>
                    <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                    Starting
                  </>
                ) : (
                  'Start'
                )}
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}

export default EmbeddingVisualizationTab;
</file>

<file path="packages/webui/services/projection_service.py">
"""Service layer for embedding projection operations and artifact access."""

from __future__ import annotations

import json
import logging
import shutil
import uuid
from array import array
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any

from fastapi import HTTPException
from shared.config import settings
from shared.database.exceptions import AccessDeniedError, EntityNotFoundError
from shared.database.models import OperationStatus, OperationType, ProjectionRun, ProjectionRunStatus
from shared.database.repositories.chunk_repository import ChunkRepository
from shared.database.repositories.document_repository import DocumentRepository

from packages.webui.celery_app import celery_app

if TYPE_CHECKING:
    from shared.database.repositories.collection_repository import CollectionRepository
    from shared.database.repositories.operation_repository import OperationRepository
    from shared.database.repositories.projection_run_repository import ProjectionRunRepository
    from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class ProjectionService:
    """Facade for projection run orchestration.

    The current implementation only provides scaffolding so that API routes and
    workers can be wired without the full projection pipeline. Each method
    returns placeholder payloads and records TODO markers where the real logic
    will eventually live.
    """

    def __init__(
        self,
        *,
        db_session: AsyncSession,
        projection_repo: ProjectionRunRepository,
        operation_repo: OperationRepository,
        collection_repo: CollectionRepository,
    ) -> None:
        self.db_session = db_session
        self.projection_repo = projection_repo
        self.operation_repo = operation_repo
        self.collection_repo = collection_repo

    @staticmethod
    def _encode_projection(
        run: ProjectionRun, *, operation: Any | None = None, message: str | None = None
    ) -> dict[str, Any]:
        """Convert a ProjectionRun ORM instance into a serialisable payload.

        Args:
            run: The ProjectionRun database model to encode
            operation: Optional Operation model to include status from
            message: Optional status message to include

        Returns:
            Dictionary with projection metadata including operation_status if operation provided
        """

        created_at = run.created_at if isinstance(run.created_at, datetime) else None
        config = run.config if isinstance(run.config, dict) else None
        meta_raw = run.meta if isinstance(run.meta, dict) else None
        meta = dict(meta_raw) if meta_raw is not None else {}

        if config and "color_by" in config and "color_by" not in meta:
            meta["color_by"] = config["color_by"]

        if not meta:
            meta = None

        # Build base response
        response = {
            "collection_id": run.collection_id,
            "projection_id": run.uuid,
            "status": run.status.value,
            "reducer": run.reducer,
            "dimensionality": run.dimensionality,
            "created_at": created_at,
            "operation_id": run.operation_uuid,
            "config": config,
            "meta": meta,
            "message": message,
        }

        # Include operation status if operation provided
        if operation is not None:
            response["operation_status"] = operation.status.value
            # Override message with error message if operation failed
            if operation.error_message and not message:
                response["message"] = operation.error_message

        return response

    @staticmethod
    def _normalise_reducer_config(reducer: str, config: dict[str, Any] | None) -> dict[str, Any] | None:
        """Validate and normalise reducer-specific configuration."""

        reducer_key = reducer.lower()
        if reducer_key == "umap":
            if config is None:
                cfg: dict[str, Any] = {}
            elif isinstance(config, dict):
                cfg = dict(config)
            else:
                raise HTTPException(status_code=400, detail="config must be an object")
            try:
                n_neighbors = int(cfg.get("n_neighbors", 15))
            except (TypeError, ValueError):
                raise HTTPException(status_code=400, detail="n_neighbors must be an integer") from None
            if n_neighbors < 2:
                raise HTTPException(status_code=400, detail="n_neighbors must be >= 2")

            try:
                min_dist = float(cfg.get("min_dist", 0.1))
            except (TypeError, ValueError):
                raise HTTPException(status_code=400, detail="min_dist must be a number") from None
            if not 0.0 <= min_dist <= 1.0:
                raise HTTPException(status_code=400, detail="min_dist must be between 0 and 1")

            metric = str(cfg.get("metric", "cosine"))
            if not metric:
                raise HTTPException(status_code=400, detail="metric must be a non-empty string")

            cfg["n_neighbors"] = n_neighbors
            cfg["min_dist"] = min_dist
            cfg["metric"] = metric
            return cfg

        if reducer_key == "tsne":
            if config is None:
                cfg = {}
            elif isinstance(config, dict):
                cfg = dict(config)
            else:
                raise HTTPException(status_code=400, detail="config must be an object")

            try:
                perplexity = float(cfg.get("perplexity", 30.0))
            except (TypeError, ValueError):
                raise HTTPException(status_code=400, detail="perplexity must be a number") from None
            if perplexity <= 0:
                raise HTTPException(status_code=400, detail="perplexity must be > 0")

            try:
                learning_rate = float(cfg.get("learning_rate", 200.0))
            except (TypeError, ValueError):
                raise HTTPException(status_code=400, detail="learning_rate must be a number") from None
            if learning_rate <= 0:
                raise HTTPException(status_code=400, detail="learning_rate must be > 0")

            try:
                n_iter = int(cfg.get("n_iter", 1_000))
            except (TypeError, ValueError):
                raise HTTPException(status_code=400, detail="n_iter must be an integer") from None
            if n_iter < 250:
                raise HTTPException(status_code=400, detail="n_iter must be >= 250")

            metric = str(cfg.get("metric", "euclidean"))
            if not metric:
                raise HTTPException(status_code=400, detail="metric must be a non-empty string")

            init = str(cfg.get("init", "pca")).lower()
            if init not in {"pca", "random"}:
                init = "pca"

            return {
                "perplexity": perplexity,
                "learning_rate": learning_rate,
                "n_iter": n_iter,
                "metric": metric,
                "init": init,
            }

        if config is None:
            return None
        if not isinstance(config, dict):
            raise HTTPException(status_code=400, detail="config must be an object")
        return config

    async def start_projection_build(
        self,
        collection_id: str,
        user_id: int,
        parameters: dict[str, Any],
    ) -> dict[str, Any]:
        """Kick off a projection run for a collection.

        Returns a placeholder response until the compute pipeline is implemented.
        """

        logger.info(
            "Scheduling projection build for collection %s (user=%s) with params=%s",
            collection_id,
            user_id,
            parameters,
        )

        # Validate collection existence and permissions prior to acknowledgement
        collection = await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)

        reducer = str(parameters.get("reducer") or "umap").lower()
        dimensionality = int(parameters.get("dimensionality") or 2)
        if dimensionality != 2:
            raise HTTPException(status_code=400, detail="Only 2D projections are currently supported")

        raw_config = parameters.get("config") if isinstance(parameters.get("config"), dict) else None
        normalised_config = self._normalise_reducer_config(reducer, raw_config)

        colour_by = str(parameters.get("color_by") or "document_id").lower()
        run_config: dict[str, Any] = dict(normalised_config or {})
        run_config["color_by"] = colour_by

        run = await self.projection_repo.create(
            collection_id=collection.id,
            reducer=reducer,
            dimensionality=dimensionality,
            config=run_config,
            meta={"initiated_by": user_id},
        )
        operation = await self.operation_repo.create(
            collection_id=collection.id,
            user_id=user_id,
            operation_type=OperationType.PROJECTION_BUILD,
            config={
                "projection_run_id": run.uuid,
                "reducer": reducer,
                "dimensionality": dimensionality,
                "config": run_config,
            },
            meta={"projection_run_uuid": run.uuid},
        )

        run.operation_uuid = operation.uuid

        await self.db_session.flush()

        # Commit transaction BEFORE dispatching celery task
        await self.db_session.commit()

        try:
            celery_app.send_task(
                "webui.tasks.process_collection_operation",
                args=[operation.uuid],
                task_id=str(uuid.uuid4()),
            )
        except Exception as exc:  # pragma: no cover - broker failures
            logger.error("Failed to dispatch projection operation %s: %s", operation.uuid, exc)

            await self.operation_repo.update_status(
                operation.uuid,
                OperationStatus.FAILED,
                error_message=str(exc),
            )
            await self.projection_repo.update_status(
                run.uuid,
                status=ProjectionRunStatus.FAILED,
                error_message=str(exc),
            )
            await self.db_session.commit()
            raise HTTPException(status_code=503, detail="Failed to enqueue projection build task") from exc

        return self._encode_projection(run, operation=operation, message="Projection scheduling not yet implemented")

    async def list_projections(self, collection_id: str, user_id: int) -> list[dict[str, Any]]:
        """List projection runs for a collection (placeholder)."""

        logger.debug("Listing projections for collection %s", collection_id)
        await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)
        runs, _total = await self.projection_repo.list_for_collection(collection_id)
        projections: list[dict[str, Any]] = []
        for run in runs:
            operation = None
            if run.operation_uuid:
                operation = await self.operation_repo.get_by_uuid(run.operation_uuid)
            payload = self._encode_projection(run, operation=operation)
            projections.append(payload)
        return projections

    async def get_projection_metadata(self, collection_id: str, projection_id: str, user_id: int) -> dict[str, Any]:
        """Fetch metadata for a projection run (placeholder)."""

        logger.debug("Fetching projection metadata collection=%s projection=%s", collection_id, projection_id)
        collection = await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)
        owner_id = getattr(collection, "owner_id", None) or getattr(collection, "user_id", None)
        if owner_id is not None and owner_id != user_id:
            raise AccessDeniedError("collection", collection_id)
        run = await self.projection_repo.get_by_uuid(projection_id)
        if not run or run.collection_id != collection_id:
            raise EntityNotFoundError("projection_run", projection_id)

        operation = None
        if run.operation_uuid:
            operation = await self.operation_repo.get_by_uuid(run.operation_uuid)

        return self._encode_projection(run, operation=operation)

    _ALLOWED_ARTIFACTS: dict[str, str] = {
        "x": "x.f32.bin",
        "y": "y.f32.bin",
        "ids": "ids.i32.bin",
        "cat": "cat.u8.bin",
    }

    async def _resolve_storage_directory(self, run: ProjectionRun, storage_path_raw: str) -> Path:
        """Resolve the storage directory for a projection run across environments."""

        data_dir = settings.data_dir.resolve()
        raw_path = Path(storage_path_raw)

        def _projection_suffix(path: Path) -> Path | None:
            parts = path.parts
            for idx in range(len(parts) - 1):
                if parts[idx] == "semantik" and parts[idx + 1] == "projections":
                    return Path(*parts[idx:])
            return None

        candidates: list[Path] = []

        def _add_candidate(path: Path) -> None:
            resolved = path if path.is_absolute() else data_dir / path
            resolved = resolved.resolve(strict=False)
            if resolved not in candidates:
                candidates.append(resolved)

        if raw_path.is_absolute():
            _add_candidate(raw_path)
            try:
                relative_path = raw_path.relative_to(data_dir)
            except ValueError:
                suffix = _projection_suffix(raw_path)
                if suffix is not None:
                    _add_candidate(suffix)
            else:
                _add_candidate(relative_path)
        else:
            _add_candidate(raw_path)

        resolved_dir: Path | None = None
        for candidate in candidates:
            if candidate.exists() and candidate.is_dir():
                resolved_dir = candidate
                break

        if resolved_dir is None:
            raise FileNotFoundError("Projection artifacts directory not found")

        try:
            normalized_relative = resolved_dir.relative_to(data_dir)
        except ValueError as exc:  # pragma: no cover - defensive
            raise PermissionError("Attempted access outside projection storage root") from exc

        normalized_storage = str(normalized_relative)
        if normalized_storage != storage_path_raw:
            run.storage_path = normalized_storage
            await self.db_session.flush()

        return resolved_dir

    async def resolve_artifact_path(
        self,
        collection_id: str,
        projection_id: str,
        artifact_name: str,
        user_id: int,
    ) -> Path:
        """Resolve and validate the on-disk path for a projection artifact."""

        normalized_name = artifact_name.strip().lower()
        if normalized_name not in self._ALLOWED_ARTIFACTS:
            raise ValueError(f"Unsupported projection artifact '{artifact_name}'")

        run = await self.projection_repo.get_by_uuid(projection_id)
        if not run or run.collection_id != collection_id:
            raise EntityNotFoundError("projection_run", projection_id)

        await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)

        storage_path_raw = getattr(run, "storage_path", None)
        if not storage_path_raw:
            raise HTTPException(status_code=409, detail="Projection artifacts are not yet available")

        resolved_dir = await self._resolve_storage_directory(run, storage_path_raw)

        file_path = (resolved_dir / self._ALLOWED_ARTIFACTS[normalized_name]).resolve()

        if resolved_dir not in file_path.parents and file_path != resolved_dir:
            raise PermissionError("Attempted access outside projection storage root")

        if not file_path.is_file():
            raise FileNotFoundError(f"Projection artifact '{artifact_name}' not found")

        return file_path

    async def select_projection_region(
        self,
        collection_id: str,
        projection_id: str,
        selection: dict[str, Any],
        user_id: int,
    ) -> dict[str, Any]:
        """Resolve selection requests over a projection to chunk/document metadata."""

        logger.debug(
            "Selecting projection region collection=%s projection=%s selection=%s",
            collection_id,
            projection_id,
            selection,
        )

        ids = selection.get("ids")
        if not isinstance(ids, list) or not ids:
            raise HTTPException(status_code=400, detail="ids list is required")

        try:
            ordered_ids: list[int] = []
            seen_ids: set[int] = set()
            for value in ids:
                int_value = int(value)
                if int_value not in seen_ids:
                    seen_ids.add(int_value)
                    ordered_ids.append(int_value)
        except (TypeError, ValueError) as exc:
            raise HTTPException(status_code=400, detail="ids must be integers") from exc

        await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)
        run = await self.projection_repo.get_by_uuid(projection_id)
        if not run or run.collection_id != collection_id:
            raise EntityNotFoundError("projection_run", projection_id)

        storage_path = getattr(run, "storage_path", None)
        if not storage_path:
            raise FileNotFoundError("Projection artifacts have not been generated yet")

        try:
            artifacts_dir = await self._resolve_storage_directory(run, storage_path)
        except FileNotFoundError as exc:
            raise HTTPException(status_code=404, detail=str(exc)) from exc
        except PermissionError as exc:
            raise HTTPException(status_code=403, detail=str(exc)) from exc
        ids_path = artifacts_dir / self._ALLOWED_ARTIFACTS["ids"]
        meta_path = artifacts_dir / "meta.json"

        if not ids_path.is_file():
            raise HTTPException(status_code=404, detail="Projection ids artifact is missing")

        meta_payload: dict[str, Any] = {}
        if meta_path.is_file():
            try:
                meta_payload = json.loads(meta_path.read_text(encoding="utf-8"))
            except Exception:  # pragma: no cover - defensive
                meta_payload = {}

        run_meta = run.meta if isinstance(run.meta, dict) else {}
        projection_meta = (
            run_meta.get("projection_artifacts") if isinstance(run_meta.get("projection_artifacts"), dict) else {}
        )
        if not projection_meta and meta_payload:
            projection_meta = meta_payload

        degraded_flag = bool(run_meta.get("degraded") or projection_meta.get("degraded"))

        original_ids: list[str] | None = None
        if projection_meta:
            original_ids = (
                projection_meta.get("original_ids") if isinstance(projection_meta.get("original_ids"), list) else None
            )

        id_array = array("i")
        with ids_path.open("rb") as buffer:
            id_array.frombytes(buffer.read())

        requested_ids_set = set(ordered_ids)
        id_to_index: dict[int, int] = {}
        for index, value in enumerate(id_array):
            if value in requested_ids_set and value not in id_to_index:
                id_to_index[value] = index
                if len(id_to_index) == len(requested_ids_set):
                    break

        chunk_repo = ChunkRepository(self.db_session)
        document_repo = DocumentRepository(self.db_session)

        items: list[dict[str, Any]] = []
        missing_ids: list[int] = []

        for selected_id in ordered_ids:
            index = id_to_index.get(selected_id)
            if index is None:
                missing_ids.append(selected_id)
                continue

            original_identifier: str | None = None
            if original_ids and 0 <= index < len(original_ids):
                raw_identifier = original_ids[index]
                original_identifier = str(raw_identifier)
            else:
                raw_identifier = None

            chunk_data: dict[str, Any] | None = None
            document_data: dict[str, Any] | None = None

            chunk_id: int | None = None
            if isinstance(raw_identifier, str):
                try:
                    chunk_id = int(raw_identifier)
                except ValueError:
                    chunk_id = None
            elif isinstance(raw_identifier, int):
                chunk_id = raw_identifier

            if chunk_id is not None:
                try:
                    chunk = await chunk_repo.get_chunk_by_id(chunk_id, collection_id)
                except Exception:
                    chunk = None

                if chunk:
                    chunk_data = {
                        "chunk_id": chunk.id,
                        "document_id": chunk.document_id,
                        "chunk_index": chunk.chunk_index,
                        "content_preview": (chunk.content or "")[:200] if chunk.content else None,
                    }
                    if chunk.document_id:
                        try:
                            document = await document_repo.get_by_id(chunk.document_id)
                        except Exception:
                            document = None
                        if document:
                            document_data = {
                                "document_id": document.id,
                                "file_name": document.file_name,
                                "source_id": document.source_id,
                                "mime_type": document.mime_type,
                            }

            items.append(
                {
                    "selected_id": selected_id,
                    "index": index,
                    "original_id": original_identifier,
                    "chunk_id": chunk_data.get("chunk_id") if chunk_data else None,
                    "document_id": chunk_data.get("document_id") if chunk_data else None,
                    "chunk_index": chunk_data.get("chunk_index") if chunk_data else None,
                    "content_preview": chunk_data.get("content_preview") if chunk_data else None,
                }
            )

            if document_data:
                items[-1]["document"] = document_data

        return {
            "items": items,
            "missing_ids": missing_ids,
            "degraded": degraded_flag,
        }

    async def delete_projection(
        self,
        collection_id: str,
        projection_id: str,
        user_id: int,
    ) -> None:
        """Delete a projection run and associated on-disk artifacts."""

        await self.collection_repo.get_by_uuid_with_permission_check(collection_id, user_id)
        run = await self.projection_repo.get_by_uuid(projection_id)
        if not run or run.collection_id != collection_id:
            raise EntityNotFoundError("projection_run", projection_id)

        storage_path = getattr(run, "storage_path", None)
        if storage_path:
            try:
                artifacts_dir = await self._resolve_storage_directory(run, storage_path)
            except FileNotFoundError:
                artifacts_dir = None
            except PermissionError as exc:  # pragma: no cover - defensive cleanup
                logger.warning("Projection storage path for %s is outside data dir: %s", projection_id, exc)
                artifacts_dir = None

            if artifacts_dir:
                try:
                    shutil.rmtree(artifacts_dir, ignore_errors=False)
                except FileNotFoundError:
                    pass
                except Exception as exc:  # pragma: no cover - defensive cleanup
                    logger.warning("Failed to delete projection artifacts %s: %s", artifacts_dir, exc)

        await self.projection_repo.delete(projection_id)
        await self.db_session.commit()
</file>

<file path="packages/webui/tasks/projection.py">
"""Projection Celery tasks for computing and tracking embedding projections."""

from __future__ import annotations

import inspect
import json
import sys
import uuid
from collections import Counter
from collections.abc import AsyncIterator, Callable, Mapping
from contextlib import asynccontextmanager, suppress
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import numpy as np

try:  # Optional UMAP dependency
    import umap  # type: ignore[import]
except Exception:  # pragma: no cover - optional dependency
    umap = None  # type: ignore[assignment]

try:  # Optional scikit-learn dependency for t-SNE
    from sklearn.manifold import TSNE  # type: ignore[import]
except Exception:  # pragma: no cover - optional dependency
    TSNE = None  # type: ignore[assignment]
from shared.database.models import OperationStatus, ProjectionRunStatus
from shared.database.postgres_database import PostgresConnectionManager
from shared.database.repositories.collection_repository import CollectionRepository
from shared.database.repositories.operation_repository import OperationRepository
from shared.database.repositories.projection_run_repository import ProjectionRunRepository

from packages.webui.tasks.utils import (
    CeleryTaskWithOperationUpdates,
    _sanitize_error_message,
    celery_app,
    logger,
    resolve_awaitable_sync,
    resolve_qdrant_manager,
    settings,
)

DEFAULT_SAMPLE_LIMIT = 200_000
QDRANT_SCROLL_BATCH = 1_000
OVERFLOW_CATEGORY_INDEX = 255
OVERFLOW_LEGEND_LABEL = "Other"
UNKNOWN_CATEGORY_LABEL = "unknown"
ALLOWED_COLOR_BY = {"document_id", "source_dir", "filetype", "age_bucket"}


def _parse_timestamp(value: Any) -> datetime | None:
    """Parse assorted timestamp representations into timezone-aware datetimes."""

    if value is None:
        return None
    if isinstance(value, datetime):
        if value.tzinfo is None:
            return value.replace(tzinfo=UTC)
        return value.astimezone(UTC)
    if isinstance(value, int | float):
        try:
            timestamp = float(value)
        except (TypeError, ValueError):  # pragma: no cover - defensive
            return None

        abs_ts = abs(timestamp)
        # Heuristic conversions: treat large magnitudes as milliseconds/microseconds.
        if abs_ts >= 1e14:  # microseconds since epoch
            timestamp /= 1_000_000
        elif abs_ts >= 1e12:  # milliseconds since epoch
            timestamp /= 1_000

        try:
            return datetime.fromtimestamp(timestamp, tz=UTC)
        except (ValueError, OSError):  # pragma: no cover - defensive
            return None
    if isinstance(value, str):
        candidate = value.strip()
        if not candidate:
            return None
        if candidate.endswith("Z"):
            candidate = candidate[:-1] + "+00:00"
        try:
            parsed = datetime.fromisoformat(candidate)
        except ValueError:
            return None
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=UTC)
        return parsed.astimezone(UTC)
    return None


def _bucket_age(timestamp: datetime, now: datetime) -> str:
    """Bucket timestamp deltas into coarse age groups."""

    delta = now - timestamp
    if delta.total_seconds() < 0:
        return "future"
    days = delta.total_seconds() / 86_400
    if days <= 1:
        return "1d"
    if days <= 7:
        return "7d"
    if days <= 30:
        return "30d"
    if days <= 90:
        return "90d"
    if days <= 180:
        return "180d"
    if days <= 365:
        return "1y"
    return ">1y"


def _extract_source_dir(payload: Mapping[str, Any]) -> str:
    source_path = payload.get("source_path")
    metadata = payload.get("metadata") if isinstance(payload.get("metadata"), Mapping) else {}
    if not source_path:
        source_path = payload.get("path") or metadata.get("source_path")
    if not source_path or not isinstance(source_path, str):
        return UNKNOWN_CATEGORY_LABEL
    try:
        path_obj = Path(source_path)
    except Exception:  # pragma: no cover - defensive
        return str(source_path) or UNKNOWN_CATEGORY_LABEL
    if path_obj.suffix:
        parent = path_obj.parent
        if parent and parent.name:
            return parent.name
    if path_obj.name:
        return path_obj.name
    if path_obj.parent and path_obj.parent.name:
        return path_obj.parent.name
    return str(path_obj) or UNKNOWN_CATEGORY_LABEL


def _extract_filetype(payload: Mapping[str, Any]) -> str:
    metadata = payload.get("metadata") if isinstance(payload.get("metadata"), Mapping) else {}
    mime = payload.get("mime_type") or metadata.get("mime_type")
    if isinstance(mime, str) and mime:
        return mime.lower()
    path_value = payload.get("path") or payload.get("source_path") or metadata.get("source_path")
    if isinstance(path_value, str) and path_value:
        try:
            suffix = Path(path_value).suffix.lower()
        except Exception:  # pragma: no cover - defensive
            suffix = ""
        if suffix:
            return suffix.lstrip(".") or UNKNOWN_CATEGORY_LABEL
    return UNKNOWN_CATEGORY_LABEL


def _extract_age_bucket(payload: Mapping[str, Any], now: datetime) -> str:
    metadata = payload.get("metadata") if isinstance(payload.get("metadata"), Mapping) else {}
    timestamp_value = (
        payload.get("ingested_at")
        or payload.get("created_at")
        or payload.get("updated_at")
        or payload.get("timestamp")
        or metadata.get("ingested_at")
        or metadata.get("created_at")
        or metadata.get("updated_at")
        or metadata.get("timestamp")
    )
    parsed = _parse_timestamp(timestamp_value)
    if parsed is None:
        return UNKNOWN_CATEGORY_LABEL
    return _bucket_age(parsed, now)


def _derive_category_label(
    payload: Mapping[str, Any] | None,
    color_by: str,
    now: datetime,
) -> tuple[str, str | None]:
    """Return the category label and optional document identifier."""

    if not isinstance(payload, Mapping):
        return UNKNOWN_CATEGORY_LABEL, None

    if color_by == "document_id":
        doc_identifier = (
            payload.get("doc_id") or payload.get("document_id") or payload.get("chunk_id") or payload.get("source_id")
        )
        if doc_identifier is None:
            return UNKNOWN_CATEGORY_LABEL, None
        return str(doc_identifier), str(doc_identifier)

    if color_by == "source_dir":
        return _extract_source_dir(payload), None

    if color_by == "filetype":
        return _extract_filetype(payload), None

    if color_by == "age_bucket":
        return _extract_age_bucket(payload, now), None

    # Fallback to document_id semantics for unexpected values
    doc_identifier = (
        payload.get("doc_id") or payload.get("document_id") or payload.get("chunk_id") or payload.get("source_id")
    )
    if doc_identifier is None:
        return UNKNOWN_CATEGORY_LABEL, None
    return str(doc_identifier), str(doc_identifier)


def _ensure_float32(array: np.ndarray) -> np.ndarray:
    """Return ``array`` cast to ``np.float32`` without unnecessary copies."""

    return array.astype(np.float32, copy=False)


def _compute_pca_projection(vectors: np.ndarray) -> dict[str, np.ndarray]:
    """Compute a 2D PCA projection using NumPy's SVD implementation."""

    if vectors.ndim != 2:
        raise ValueError(f"Expected a 2D array for PCA, got {vectors.shape!r}")
    if vectors.shape[0] < 2:
        raise ValueError("At least two samples are required for PCA projection")
    if vectors.shape[1] < 2:
        raise ValueError("Vectors must have at least two dimensions for PCA projection")

    mean = vectors.mean(axis=0, keepdims=True)
    centered = vectors - mean

    # Compute SVD on centered data; full_matrices=False keeps outputs minimal.
    _, singular_values, vt = np.linalg.svd(centered, full_matrices=False)
    top_components = vt[:2]

    projection = centered @ top_components.T

    total_variance = float(np.square(singular_values).sum())
    top_singular_values = singular_values[:2]
    if total_variance > 0:
        explained_variance_ratio = np.square(top_singular_values) / total_variance
    else:  # Degenerate case where all vectors are identical
        explained_variance_ratio = np.zeros_like(top_singular_values)

    return {
        "projection": _ensure_float32(projection),
        "components": _ensure_float32(top_components),
        "mean": _ensure_float32(mean.squeeze(axis=0)),
        "singular_values": _ensure_float32(top_singular_values),
        "explained_variance_ratio": _ensure_float32(explained_variance_ratio),
    }


def _compute_umap_projection(
    vectors: np.ndarray,
    *,
    n_neighbors: int,
    min_dist: float,
    metric: str,
) -> dict[str, np.ndarray]:
    """Compute a 2D UMAP projection using umap-learn."""

    if umap is None:
        raise RuntimeError("umap-learn is not installed")

    reducer = umap.UMAP(
        n_neighbors=n_neighbors,
        min_dist=min_dist,
        metric=metric,
        n_components=2,
        random_state=42,
    )
    embedding = reducer.fit_transform(vectors)
    return {
        "projection": _ensure_float32(embedding),
    }


def _compute_tsne_projection(
    vectors: np.ndarray,
    *,
    perplexity: float,
    learning_rate: float,
    n_iter: int,
    metric: str,
    init: str = "pca",
) -> dict[str, np.ndarray]:
    """Compute a 2D t-SNE projection using scikit-learn."""

    if TSNE is None:
        raise RuntimeError("scikit-learn is not installed; t-SNE reducer unavailable")

    if vectors.ndim != 2:
        raise ValueError(f"Expected a 2D array for t-SNE, got {vectors.shape!r}")
    n_samples = vectors.shape[0]
    if n_samples < 2:
        raise ValueError("At least two samples are required for t-SNE projection")

    # Adjust perplexity to remain within valid bounds for the dataset size.
    max_perplexity = max(1.0, min(perplexity, n_samples - 1))
    effective_perplexity = max(1.0, min(perplexity, max_perplexity))

    effective_learning_rate = max(10.0, float(learning_rate))
    effective_n_iter = max(250, int(n_iter))
    init_mode = init if init in {"pca", "random"} else "pca"

    tsne_signature = inspect.signature(TSNE.__init__)
    tsne_params = tsne_signature.parameters

    tsne_kwargs: dict[str, Any] = {
        "n_components": 2,
        "perplexity": effective_perplexity,
        "metric": metric,
        "init": init_mode,
        "random_state": 42,
    }

    if "learning_rate" in tsne_params:
        tsne_kwargs["learning_rate"] = effective_learning_rate
    elif "learning_rate_init" in tsne_params:
        tsne_kwargs["learning_rate_init"] = effective_learning_rate

    iteration_param: str | None = None
    if "n_iter" in tsne_params:
        tsne_kwargs["n_iter"] = effective_n_iter
        iteration_param = "n_iter"
    elif "max_iter" in tsne_params:
        tsne_kwargs["max_iter"] = effective_n_iter
        iteration_param = "max_iter"

    if "square_distances" in tsne_params:
        tsne_kwargs["square_distances"] = True

    tsne = TSNE(**tsne_kwargs)

    if iteration_param is None:
        # As a fallback for very old sklearn versions, try to set the attribute directly.
        with suppress(Exception):
            tsne.n_iter = effective_n_iter

    embedding = tsne.fit_transform(vectors)

    kl_divergence = float(getattr(tsne, "kl_divergence_", float("nan")))
    iterations_run = getattr(tsne, "n_iter_", getattr(tsne, "n_iter", effective_n_iter))

    return {
        "projection": _ensure_float32(embedding),
        "perplexity": float(effective_perplexity),
        "learning_rate": float(effective_learning_rate),
        "n_iter": int(iterations_run),
        "kl_divergence": kl_divergence,
    }


def _write_binary(path: Path, array: np.ndarray, dtype: np.dtype) -> None:
    """Write ``array`` to ``path`` as raw binary in the specified dtype."""

    array.astype(dtype, copy=False).tofile(path)


def _write_meta(path: Path, payload: dict[str, Any]) -> None:
    path.write_text(json.dumps(payload, indent=2, sort_keys=True), encoding="utf-8")


@asynccontextmanager
async def _operation_updates(operation_id: str | None) -> AsyncIterator[CeleryTaskWithOperationUpdates | None]:
    """Yield an update publisher when an operation ID is available."""

    if not operation_id:
        yield None
        return

    async with CeleryTaskWithOperationUpdates(operation_id) as updater:
        yield updater


@celery_app.task(bind=True, name="webui.tasks.compute_projection")
def compute_projection(self: Any, projection_id: str) -> dict[str, Any]:  # noqa: ANN401, ARG001
    """Compute a 2D PCA projection for the requested projection run."""

    logger.info("compute_projection task invoked for projection_id=%s", projection_id)

    try:
        result = resolve_awaitable_sync(_compute_projection_async(projection_id))
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.exception("Projection computation failed for %s", projection_id)
        return {
            "projection_id": projection_id,
            "status": "failed",
            "message": str(exc),
        }

    return result


async def _compute_projection_async(projection_id: str) -> dict[str, Any]:
    """Async implementation for ``compute_projection``."""

    pg_manager = PostgresConnectionManager()
    await pg_manager.initialize()

    if pg_manager._sessionmaker is None:
        await pg_manager.close()
        raise RuntimeError("Failed to initialize projection session maker")

    session_factory = pg_manager._sessionmaker
    session: Any | None = None
    session_guard: Callable[[], Any] | None = None

    try:
        async with session_factory() as session:
            ensure_open_guard = getattr(session, "ensure_open", None)
            if callable(ensure_open_guard):
                session_guard = ensure_open_guard

            projection_repo = ProjectionRunRepository(session)
            operation_repo = OperationRepository(session)
            collection_repo = CollectionRepository(session)

            run = await projection_repo.get_by_uuid(projection_id)
            if not run:
                raise ValueError(f"Projection run {projection_id} not found")

            operation_uuid = getattr(run, "operation_uuid", None)
            collection = await collection_repo.get_by_uuid(run.collection_id)
            if not collection:
                raise ValueError(f"Collection {run.collection_id} for projection {projection_id} not found")

            vector_collection_name = getattr(collection, "vector_store_name", None) or getattr(
                collection, "vector_collection_id", None
            )
            if not vector_collection_name:
                raise ValueError("Collection is missing a vector store name for projection computation")

            config = run.config or {}
            color_by = str(config.get("color_by") or "document_id").lower()
            if color_by not in ALLOWED_COLOR_BY:
                color_by = "document_id"

            configured_sample = config.get("sample_size")
            if configured_sample is None:
                configured_sample = config.get("sample_limit")
            if configured_sample is None:
                configured_sample = config.get("sample_n")
            try:
                sample_limit = int(configured_sample) if configured_sample is not None else DEFAULT_SAMPLE_LIMIT
            except (TypeError, ValueError):
                sample_limit = DEFAULT_SAMPLE_LIMIT
            sample_limit = max(sample_limit, 1)

            run_dir = settings.data_dir / "semantik" / "projections" / run.collection_id / run.uuid
            run_dir.mkdir(parents=True, exist_ok=True)

            now = datetime.now(UTC)

            async with _operation_updates(operation_uuid) as updater:
                try:
                    await projection_repo.update_status(
                        run.uuid,
                        status=ProjectionRunStatus.RUNNING,
                        started_at=now,
                    )
                    if operation_uuid:
                        await operation_repo.update_status(
                            operation_uuid,
                            OperationStatus.PROCESSING,
                            started_at=now,
                            error_message=None,
                        )
                    await session.commit()

                    vectors: list[np.ndarray] = []
                    original_ids: list[str] = []
                    categories: list[int] = []

                    if updater:
                        await updater.send_update(
                            "projection_started",
                            {
                                "projection_id": run.uuid,
                                "collection_id": run.collection_id,
                                "sample_limit": sample_limit,
                                "color_by": color_by,
                            },
                        )

                    category_index_map: dict[str, int] = {}
                    label_for_index: dict[int, str] = {}
                    category_counts: Counter[int] = Counter()
                    doc_category_map: dict[str, int] = {} if color_by == "document_id" else {}
                    overflow_logged = False

                    manager = resolve_qdrant_manager()
                    qdrant_client = getattr(manager, "client", None)
                    if qdrant_client is None and hasattr(manager, "get_client"):
                        qdrant_client = manager.get_client()
                    if qdrant_client is None:
                        raise RuntimeError("Unable to acquire Qdrant client from manager")

                    offset: Any = None
                    while len(vectors) < sample_limit:
                        remaining = sample_limit - len(vectors)
                        batch_limit = max(1, min(QDRANT_SCROLL_BATCH, remaining))
                        records, offset = qdrant_client.scroll(
                            collection_name=vector_collection_name,
                            offset=offset,
                            limit=batch_limit,
                            with_payload=True,
                            with_vectors=True,
                        )

                        if not records:
                            break

                        for record in records:
                            vector_values = getattr(record, "vector", None)
                            if isinstance(vector_values, dict):
                                vector_values = next((val for val in vector_values.values() if val is not None), None)
                            if vector_values is None:
                                continue

                            vector_array = np.asarray(vector_values, dtype=np.float32)
                            if vector_array.ndim != 1:
                                continue

                            vectors.append(vector_array)
                            original_ids.append(str(record.id))

                            payload = getattr(record, "payload", {}) or {}
                            category_label, doc_identifier = _derive_category_label(payload, color_by, now)

                            if not category_label:
                                category_label = UNKNOWN_CATEGORY_LABEL

                            category_idx = category_index_map.get(category_label)
                            if category_idx is None:
                                if len(category_index_map) < OVERFLOW_CATEGORY_INDEX:
                                    category_idx = len(category_index_map)
                                    category_index_map[category_label] = category_idx
                                    label_for_index[category_idx] = category_label
                                else:
                                    category_idx = OVERFLOW_CATEGORY_INDEX
                            else:
                                if category_idx != OVERFLOW_CATEGORY_INDEX:
                                    label_for_index.setdefault(category_idx, category_label)

                            if category_idx == OVERFLOW_CATEGORY_INDEX:
                                label_for_index.setdefault(OVERFLOW_CATEGORY_INDEX, OVERFLOW_LEGEND_LABEL)
                                if not overflow_logged:
                                    logger.warning(
                                        "Projection %s exceeded 255 categories; using overflow bucket",
                                        projection_id,
                                    )
                                    overflow_logged = True

                            categories.append(int(category_idx))
                            category_counts[category_idx] += 1

                            if color_by == "document_id" and doc_identifier is not None:
                                doc_key = str(doc_identifier)
                                if doc_key not in doc_category_map:
                                    doc_category_map[doc_key] = int(category_idx)

                        if updater:
                            await updater.send_update(
                                "projection_fetch_progress",
                                {
                                    "projection_id": run.uuid,
                                    "fetched": len(vectors),
                                    "sample_limit": sample_limit,
                                    "color_by": color_by,
                                },
                            )

                        if offset is None:
                            break

                    point_count = len(vectors)
                    total_vectors = getattr(collection, "vector_count", None)
                    if isinstance(total_vectors, int) and total_vectors > 0:
                        total_vectors = max(total_vectors, point_count)
                    else:
                        total_vectors = point_count
                    sampled_flag = point_count < total_vectors

                    if point_count < 2:
                        raise ValueError("Not enough vectors available to compute projection (need at least 2)")

                    vectors_array = np.stack(vectors, axis=0)

                    requested_reducer = (run.reducer or "pca").lower()
                    reducer_used = requested_reducer
                    reducer_params: dict[str, Any] = {}
                    fallback_reason: str | None = None

                    try:
                        if requested_reducer == "umap":
                            params = config if isinstance(config, dict) else {}
                            n_neighbors = int(params.get("n_neighbors", 15))
                            min_dist = float(params.get("min_dist", 0.1))
                            metric = str(params.get("metric", "cosine"))
                            reducer_params = {
                                "n_neighbors": n_neighbors,
                                "min_dist": min_dist,
                                "metric": metric,
                            }
                            projection_result = _compute_umap_projection(
                                vectors_array,
                                n_neighbors=n_neighbors,
                                min_dist=min_dist,
                                metric=metric,
                            )
                            reducer_used = "umap"
                        elif requested_reducer == "tsne":
                            params = config if isinstance(config, dict) else {}
                            perplexity = float(params.get("perplexity", 30.0))
                            learning_rate = float(params.get("learning_rate", 200.0))
                            n_iter = int(params.get("n_iter", 1_000))
                            metric = str(params.get("metric", "euclidean"))
                            init = str(params.get("init", "pca"))
                            projection_result = _compute_tsne_projection(
                                vectors_array,
                                perplexity=perplexity,
                                learning_rate=learning_rate,
                                n_iter=n_iter,
                                metric=metric,
                                init=init,
                            )
                            reducer_used = "tsne"
                            reducer_params = {
                                "perplexity": float(projection_result.get("perplexity", perplexity)),
                                "learning_rate": float(projection_result.get("learning_rate", learning_rate)),
                                "n_iter": int(projection_result.get("n_iter", n_iter)),
                                "metric": metric,
                                "init": init,
                            }
                        elif requested_reducer == "pca":
                            projection_result = _compute_pca_projection(vectors_array)
                            reducer_used = "pca"
                        else:
                            fallback_reason = f"Unsupported reducer '{requested_reducer}'"
                            logger.warning(
                                "Reducer %s not supported for projection %s; falling back to PCA",
                                requested_reducer,
                                projection_id,
                            )
                            projection_result = _compute_pca_projection(vectors_array)
                            reducer_used = "pca"
                            reducer_params = config if isinstance(config, dict) else {}
                    except Exception as exc:
                        fallback_reason = str(exc)
                        logger.warning(
                            "Reducer %s failed for projection %s; falling back to PCA: %s",
                            requested_reducer,
                            projection_id,
                            exc,
                        )
                        projection_result = _compute_pca_projection(vectors_array)
                        reducer_used = "pca"
                        if requested_reducer == "umap":
                            params = config if isinstance(config, dict) else {}
                            reducer_params = {
                                "n_neighbors": int(params.get("n_neighbors", 15)),
                                "min_dist": float(params.get("min_dist", 0.1)),
                                "metric": str(params.get("metric", "cosine")),
                            }
                        elif requested_reducer == "tsne":
                            params = config if isinstance(config, dict) else {}
                            reducer_params = {
                                "perplexity": float(params.get("perplexity", 30.0)),
                                "learning_rate": float(params.get("learning_rate", 200.0)),
                                "n_iter": int(params.get("n_iter", 1_000)),
                                "metric": str(params.get("metric", "euclidean")),
                                "init": str(params.get("init", "pca")),
                            }
                        else:
                            reducer_params = config if isinstance(config, dict) else {}

                    projection_array = projection_result["projection"]
                    x_path = run_dir / "x.f32.bin"
                    y_path = run_dir / "y.f32.bin"
                    ids_path = run_dir / "ids.i32.bin"
                    cat_path = run_dir / "cat.u8.bin"
                    meta_path = run_dir / "meta.json"

                    x_values = projection_array[:, 0]
                    y_values = projection_array[:, 1]
                    ids_array = []
                    warned_fallback = False
                    int32_info = np.iinfo(np.int32)
                    for idx, point_id in enumerate(original_ids):
                        use_sequential = False
                        try:
                            numeric = int(point_id)
                        except (TypeError, ValueError):
                            use_sequential = True
                        else:
                            if numeric < int32_info.min or numeric > int32_info.max:
                                use_sequential = True

                        if use_sequential:
                            if not warned_fallback:
                                logger.warning(
                                    "Projection %s has point IDs that are non-integer or outside int32 bounds; "
                                    "using sequential fallback",
                                    projection_id,
                                )
                                warned_fallback = True
                            numeric = idx

                        ids_array.append(numeric)

                    ids_array = np.asarray(ids_array, dtype=np.int32)
                    categories_array = np.array(categories, dtype=np.uint8)

                    legend_entries = [
                        {
                            "index": int(idx),
                            "label": label,
                            "count": int(category_counts.get(idx, 0)),
                        }
                        for idx, label in sorted(label_for_index.items())
                    ]
                    if category_counts.get(OVERFLOW_CATEGORY_INDEX) and not any(
                        entry["index"] == OVERFLOW_CATEGORY_INDEX for entry in legend_entries
                    ):
                        legend_entries.append(
                            {
                                "index": OVERFLOW_CATEGORY_INDEX,
                                "label": OVERFLOW_LEGEND_LABEL,
                                "count": int(category_counts[OVERFLOW_CATEGORY_INDEX]),
                            }
                        )

                    _write_binary(x_path, x_values, np.float32)
                    _write_binary(y_path, y_values, np.float32)
                    _write_binary(ids_path, ids_array, np.int32)
                    _write_binary(cat_path, categories_array, np.uint8)

                    meta_payload: dict[str, Any] = {
                        "projection_id": run.uuid,
                        "collection_id": run.collection_id,
                        "created_at": datetime.now(UTC).isoformat(),
                        "point_count": point_count,
                        "total_count": total_vectors,
                        "shown_count": point_count,
                        "sampled": sampled_flag,
                        "reducer_requested": requested_reducer,
                        "reducer_used": reducer_used,
                        "reducer_params": reducer_params,
                        "dimensionality": 2,
                        "source_vector_collection": vector_collection_name,
                        "sample_limit": sample_limit,
                        "files": {
                            "x": x_path.name,
                            "y": y_path.name,
                            "ids": ids_path.name,
                            "categories": cat_path.name,
                        },
                        "color_by": color_by,
                        "legend": legend_entries,
                    }
                    if "explained_variance_ratio" in projection_result:
                        meta_payload["explained_variance_ratio"] = (
                            projection_result["explained_variance_ratio"].tolist()
                            if isinstance(projection_result["explained_variance_ratio"], np.ndarray)
                            else projection_result["explained_variance_ratio"]
                        )
                    if "singular_values" in projection_result:
                        meta_payload["singular_values"] = (
                            projection_result["singular_values"].tolist()
                            if isinstance(projection_result["singular_values"], np.ndarray)
                            else projection_result["singular_values"]
                        )
                    if "kl_divergence" in projection_result and projection_result["kl_divergence"] is not None:
                        try:
                            meta_payload["kl_divergence"] = float(projection_result["kl_divergence"])
                        except (TypeError, ValueError):
                            meta_payload["kl_divergence"] = projection_result["kl_divergence"]
                    if fallback_reason:
                        meta_payload["fallback_reason"] = fallback_reason
                    meta_payload["original_ids"] = original_ids
                    if color_by == "document_id":
                        meta_payload["category_map"] = {key: int(val) for key, val in doc_category_map.items()}
                    meta_payload["category_counts"] = {
                        str(int(idx)): int(count) for idx, count in category_counts.items()
                    }

                    _write_meta(meta_path, meta_payload)

                    try:
                        storage_path_value = str(run_dir.relative_to(settings.data_dir))
                    except ValueError:
                        storage_path_value = str(run_dir)

                    await projection_repo.update_metadata(
                        run.uuid,
                        storage_path=storage_path_value,
                        point_count=point_count,
                        meta={
                            "projection_artifacts": meta_payload,
                            "color_by": color_by,
                            "legend": legend_entries,
                            "sampled": sampled_flag,
                            "shown_count": point_count,
                            "total_count": total_vectors,
                        },
                    )
                    await projection_repo.update_status(
                        run.uuid,
                        status=ProjectionRunStatus.COMPLETED,
                        completed_at=datetime.now(UTC),
                    )

                    if operation_uuid:
                        await operation_repo.update_status(
                            operation_uuid,
                            OperationStatus.COMPLETED,
                            completed_at=datetime.now(UTC),
                            error_message=None,
                        )

                    await session.commit()

                    if updater:
                        await updater.send_update(
                            "projection_completed",
                            {
                                "projection_id": run.uuid,
                                "point_count": point_count,
                                "reducer": reducer_used,
                                "storage_path": str(run_dir),
                                "color_by": color_by,
                                "legend": legend_entries,
                                "sampled": sampled_flag,
                                "shown_count": point_count,
                                "total_count": total_vectors,
                            },
                        )

                    return {
                        "projection_id": projection_id,
                        "status": "completed",
                        "reducer": reducer_used,
                        "message": None,
                        "point_count": point_count,
                        "storage_path": str(run_dir),
                    }

                except Exception as exc:
                    sanitized_error = _sanitize_error_message(str(exc))
                    logger.exception("Projection computation failed for %s", projection_id)

                    await session.rollback()

                    if operation_uuid:
                        await operation_repo.update_status(
                            operation_uuid,
                            OperationStatus.FAILED,
                            error_message=sanitized_error,
                        )
                    await projection_repo.update_status(
                        run.uuid,
                        status=ProjectionRunStatus.FAILED,
                        error_message=sanitized_error,
                    )
                    await session.commit()

                    if updater:
                        await updater.send_update(
                            "projection_failed",
                            {
                                "projection_id": run.uuid,
                                "error": sanitized_error,
                            },
                        )

                    raise

                finally:
                    if updater:
                        await updater.close()
    finally:
        guard_exception: Exception | None = None
        active_exc_type = sys.exc_info()[0]
        if session_guard is not None:
            try:
                session_guard()
            except Exception as exc:  # pragma: no cover - surface cleanup errors
                guard_exception = exc

        await pg_manager.close()

        if guard_exception is not None and active_exc_type is None:
            raise guard_exception


async def _process_projection_operation(
    operation: dict[str, Any],
    collection: dict[str, Any],
    projection_repo: Any,
    updater: CeleryTaskWithOperationUpdates,
) -> dict[str, Any]:
    """Async handler invoked from the ingestion dispatcher for projections."""
    logger.info(
        "Processing projection operation operation_id=%s collection_id=%s",
        operation.get("uuid"),
        collection.get("id"),
    )

    operation_config = operation.get("config") or {}
    projection_id = (
        operation_config.get("projection_run_id")
        or operation_config.get("projection_id")
        or operation_config.get("projection_uuid")
    )
    if not projection_id:
        raise ValueError("Projection operation missing projection run identifier")

    projection_id = str(projection_id)

    session = getattr(projection_repo, "session", None)
    if session is None:
        raise RuntimeError("Projection repository is missing bound session")

    operation_repo = OperationRepository(session)

    run = await projection_repo.get_by_uuid(projection_id)
    if not run:
        raise ValueError(f"Projection run {projection_id} not found")

    try:
        if getattr(run, "operation_uuid", None) != operation.get("uuid"):
            await projection_repo.set_operation_uuid(projection_id, operation.get("uuid"))

        await projection_repo.update_status(
            projection_id,
            status=ProjectionRunStatus.RUNNING,
            error_message=None,
            started_at=datetime.now(UTC),
        )

        await session.commit()
    except Exception as exc:
        await session.rollback()
        logger.exception("Failed preparing projection %s before enqueue: %s", projection_id, exc)
        raise

    if updater:
        await updater.send_update(
            "projection_enqueued",
            {
                "projection_id": projection_id,
                "operation_id": operation.get("uuid"),
                "status": ProjectionRunStatus.RUNNING.value,
            },
        )

    try:
        compute_projection.apply_async(args=(projection_id,), task_id=str(uuid.uuid4()))
    except Exception as exc:  # pragma: no cover - broker failure path
        sanitized_error = _sanitize_error_message(str(exc))
        logger.error(
            "Failed to enqueue compute_projection for %s: %s",
            projection_id,
            sanitized_error,
        )

        await projection_repo.update_status(
            projection_id,
            status=ProjectionRunStatus.FAILED,
            error_message=sanitized_error,
            completed_at=datetime.now(UTC),
        )
        await operation_repo.update_status(
            operation.get("uuid"),
            OperationStatus.FAILED,
            error_message=sanitized_error,
            completed_at=datetime.now(UTC),
        )
        await session.commit()

        if updater:
            await updater.send_update(
                "projection_failed",
                {
                    "projection_id": projection_id,
                    "error": sanitized_error,
                },
            )

        return {"success": False, "message": sanitized_error}

    return {
        "success": True,
        "defer_completion": True,
        "projection_id": projection_id,
        "message": "Projection compute enqueued",
    }
</file>

<file path="packages/webui/services/factory.py">
"""Factory functions for creating service instances with dependencies."""

import logging

import httpx
from fastapi import Depends
from shared.database.repositories.collection_repository import CollectionRepository
from shared.database.repositories.document_repository import DocumentRepository
from shared.database.repositories.operation_repository import OperationRepository
from shared.database.repositories.projection_run_repository import ProjectionRunRepository
from sqlalchemy.ext.asyncio import AsyncSession

from packages.shared.database import get_db
from packages.shared.managers import QdrantManager
from packages.webui.utils.qdrant_manager import qdrant_manager as qdrant_connection_manager

from .chunking.adapter import ChunkingServiceAdapter
from .chunking.container import (
    get_chunking_orchestrator as container_get_chunking_orchestrator,
)
from .chunking.container import (
    get_redis_manager as container_get_redis_manager,
)
from .chunking.container import (
    resolve_api_chunking_dependency,
)
from .chunking.orchestrator import ChunkingOrchestrator
from .chunking_service import ChunkingService
from .collection_service import CollectionService
from .directory_scan_service import DirectoryScanService
from .document_scanning_service import DocumentScanningService
from .operation_service import OperationService
from .projection_service import ProjectionService
from .redis_manager import RedisManager
from .resource_manager import ResourceManager
from .search_service import SearchService

logger = logging.getLogger(__name__)


def get_redis_manager() -> RedisManager:
    """Backward-compatible wrapper over chunking container Redis manager."""

    manager = container_get_redis_manager()
    logger.debug("Reusing Redis manager from chunking container")
    return manager


def create_collection_service(
    db: AsyncSession,
    *,
    qdrant_manager_override: QdrantManager | None = None,
) -> CollectionService:
    """Create a CollectionService instance with all required dependencies.

    This factory function simplifies dependency injection for FastAPI endpoints.

    Args:
        db: AsyncSession instance from FastAPI's dependency injection
        qdrant_manager_override: Optional pre-built manager, useful for tests

    Returns:
        Configured CollectionService instance

    Example:
        ```python
        from fastapi import Depends
        from sqlalchemy.ext.asyncio import AsyncSession
        from shared.database import get_db
        from webui.services.factory import create_collection_service

        async def get_collection_service(
            db: AsyncSession = Depends(get_db)
        ) -> CollectionService:
            return create_collection_service(db)

        @router.post("/collections")
        async def create_collection(
            request: CreateCollectionRequest,
            service: CollectionService = Depends(get_collection_service),
        ):
            # Use service here
            pass
        ```
    """
    # Create repository instances
    collection_repo = CollectionRepository(db)
    operation_repo = OperationRepository(db)
    document_repo = DocumentRepository(db)

    qdrant_manager_instance = qdrant_manager_override
    if qdrant_manager_instance is None:
        try:
            qdrant_client = qdrant_connection_manager.get_client()
            qdrant_manager_instance = QdrantManager(qdrant_client)
        except Exception as exc:  # pragma: no cover - fallback when Qdrant is offline
            logger.warning("Qdrant client unavailable for collection service: %s", exc)

    # Create and return service
    return CollectionService(
        db_session=db,
        collection_repo=collection_repo,
        operation_repo=operation_repo,
        document_repo=document_repo,
        qdrant_manager=qdrant_manager_instance,
    )


def create_document_scanning_service(db: AsyncSession) -> DocumentScanningService:
    """Create a DocumentScanningService instance with required dependencies.

    This factory function simplifies dependency injection for file scanning operations.

    Args:
        db: AsyncSession instance from FastAPI's dependency injection

    Returns:
        Configured DocumentScanningService instance

    Example:
        ```python
        from fastapi import Depends
        from sqlalchemy.ext.asyncio import AsyncSession
        from shared.database import get_db
        from webui.services.factory import create_document_scanning_service

        async def get_document_scanning_service(
            db: AsyncSession = Depends(get_db)
        ) -> DocumentScanningService:
            return create_document_scanning_service(db)

        # In your task or endpoint
        async def scan_and_register_files(
            collection_id: str,
            source_path: str,
            service: DocumentScanningService = Depends(get_document_scanning_service),
        ):
            stats = await service.scan_directory_and_register_documents(
                collection_id=collection_id,
                source_path=source_path
            )
            return stats
        ```
    """
    # Create repository instances
    document_repo = DocumentRepository(db)

    # Create and return service
    return DocumentScanningService(
        db_session=db,
        document_repo=document_repo,
    )


def create_resource_manager(db: AsyncSession) -> ResourceManager:
    """Create a ResourceManager instance with required dependencies.

    This factory function creates a resource manager for monitoring and managing
    resource allocation for collection operations.

    Args:
        db: AsyncSession instance from FastAPI's dependency injection

    Returns:
        Configured ResourceManager instance

    Example:
        ```python
        from fastapi import Depends
        from sqlalchemy.ext.asyncio import AsyncSession
        from shared.database import get_db
        from webui.services.factory import create_resource_manager

        async def get_resource_manager(
            db: AsyncSession = Depends(get_db)
        ) -> ResourceManager:
            return create_resource_manager(db)

        # In your endpoint or service
        async def check_resources(
            user_id: int,
            resource_manager: ResourceManager = Depends(get_resource_manager),
        ):
            can_create = await resource_manager.can_create_collection(user_id)
            return {"can_create": can_create}
        ```
    """
    # Create repository instances
    collection_repo = CollectionRepository(db)
    operation_repo = OperationRepository(db)

    qdrant_manager_instance = None
    try:
        qdrant_client = qdrant_connection_manager.get_client()
        qdrant_manager_instance = QdrantManager(qdrant_client)
    except Exception as exc:  # pragma: no cover - fallback when Qdrant is offline
        logger.warning("Qdrant client unavailable for resource metrics: %s", exc)

    # Create and return resource manager
    return ResourceManager(
        collection_repo=collection_repo,
        operation_repo=operation_repo,
        qdrant_manager=qdrant_manager_instance,
    )


# FastAPI dependency functions


async def get_collection_service(db: AsyncSession = Depends(get_db)) -> CollectionService:
    """FastAPI dependency for CollectionService injection."""
    return create_collection_service(db)


def create_operation_service(db: AsyncSession) -> OperationService:
    """Create an OperationService instance with required dependencies.

    Args:
        db: AsyncSession instance from FastAPI's dependency injection

    Returns:
        Configured OperationService instance
    """
    # Create repository instances
    operation_repo = OperationRepository(db)

    # Create and return service
    return OperationService(
        db_session=db,
        operation_repo=operation_repo,
    )


async def get_operation_service(db: AsyncSession = Depends(get_db)) -> OperationService:
    """FastAPI dependency for OperationService injection."""
    return create_operation_service(db)


def create_projection_service(db: AsyncSession) -> ProjectionService:
    """Create a ProjectionService instance with required dependencies."""

    projection_repo = ProjectionRunRepository(db)
    operation_repo = OperationRepository(db)
    collection_repo = CollectionRepository(db)

    return ProjectionService(
        db_session=db,
        projection_repo=projection_repo,
        operation_repo=operation_repo,
        collection_repo=collection_repo,
    )


async def get_projection_service(db: AsyncSession = Depends(get_db)) -> ProjectionService:
    """FastAPI dependency for ProjectionService injection."""

    return create_projection_service(db)


def create_search_service(
    db: AsyncSession,
    default_timeout: httpx.Timeout | None = None,
    retry_timeout_multiplier: float = 4.0,
) -> SearchService:
    """Create a SearchService instance with required dependencies.

    Args:
        db: AsyncSession instance from FastAPI's dependency injection
        default_timeout: Optional default timeout configuration for HTTP requests
        retry_timeout_multiplier: Multiplier for timeout values on retry attempts

    Returns:
        Configured SearchService instance
    """
    # Create repository instances
    collection_repo = CollectionRepository(db)

    # Create and return service
    return SearchService(
        db_session=db,
        collection_repo=collection_repo,
        default_timeout=default_timeout,
        retry_timeout_multiplier=retry_timeout_multiplier,
    )


async def get_search_service(db: AsyncSession = Depends(get_db)) -> SearchService:
    """FastAPI dependency for SearchService injection."""
    # Use default timeout configuration, can be customized if needed
    return create_search_service(db)


async def get_directory_scan_service() -> DirectoryScanService:
    """FastAPI dependency for DirectoryScanService injection.

    Note: DirectoryScanService doesn't require database access as it only
    provides preview functionality without persisting data.
    """
    return DirectoryScanService()


async def create_chunking_orchestrator(db: AsyncSession) -> ChunkingOrchestrator:
    """Create orchestrator using composition root."""

    return await container_get_chunking_orchestrator(db)


async def create_chunking_service(db: AsyncSession) -> ChunkingService | ChunkingServiceAdapter | ChunkingOrchestrator:
    """Return chunking dependency that emulates legacy service."""

    return await resolve_api_chunking_dependency(db, prefer_adapter=True)


def create_celery_chunking_service(db_session: AsyncSession) -> ChunkingService:
    """Create ChunkingService for Celery tasks without Redis."""

    collection_repo = CollectionRepository(db_session)
    document_repo = DocumentRepository(db_session)

    return ChunkingService(
        db_session=db_session,
        collection_repo=collection_repo,
        document_repo=document_repo,
        redis_client=None,
    )


def create_celery_chunking_service_with_repos(
    db_session: AsyncSession,
    collection_repo: CollectionRepository,
    document_repo: DocumentRepository,
) -> ChunkingService:
    """Create ChunkingService using pre-built repositories."""

    return ChunkingService(
        db_session=db_session,
        collection_repo=collection_repo,
        document_repo=document_repo,
        redis_client=None,
    )


async def get_chunking_orchestrator(db: AsyncSession = Depends(get_db)) -> ChunkingOrchestrator:
    """FastAPI dependency for orchestrator injection (new architecture)."""

    return await container_get_chunking_orchestrator(db)


async def get_chunking_service(
    db: AsyncSession = Depends(get_db),
) -> ChunkingService | ChunkingServiceAdapter | ChunkingOrchestrator:
    """FastAPI dependency for legacy ChunkingService consumers."""

    return await create_chunking_service(db)


# Expose commonly used dependency providers to builtins for tests that
# reference them without importing (legacy tests convenience)
try:  # pragma: no cover
    import builtins as _builtins

    _builtins.get_chunking_service = get_chunking_service  # type: ignore[attr-defined]
    _builtins.get_collection_service = get_collection_service  # type: ignore[attr-defined]
except Exception:
    pass
</file>

<file path="packages/shared/database/models.py">
#!/usr/bin/env python3
"""
SQLAlchemy declarative models for the Semantik database.

This module defines the database schema using SQLAlchemy's declarative mapping.
These models are used by Alembic for migrations and can be used for ORM operations.

Note on Timestamps:
All DateTime fields use timezone=True to ensure consistent timezone-aware datetime
handling across the application. This allows proper storage and retrieval of
timezones with datetime values, preventing timezone-related bugs.

We use DateTime columns for new tables but maintain String columns for existing
user-related tables for backward compatibility. A future migration could convert
these to proper DateTime columns.

Partitioned Tables:
The chunks table is partitioned by HASH(collection_id) to improve performance and
scalability. When working with partitioned tables:

1. ALWAYS include the partition key (collection_id) in WHERE clauses
2. Group bulk operations by partition key for efficiency
3. Be aware that cross-partition queries are expensive
4. The partition key must be part of any unique constraint or primary key

See the Chunk model and packages.shared.database.partition_utils for detailed
examples and utilities for working with partitioned tables.
"""

import enum
from typing import Any, cast

from sqlalchemy import (
    JSON,
    BigInteger,
    Boolean,
    CheckConstraint,
    Column,
    DateTime,
    Enum,
    Float,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
    UniqueConstraint,
    func,
)
from sqlalchemy.orm import DeclarativeBase, relationship


# Create the declarative base
class Base(DeclarativeBase):
    """Base class for all SQLAlchemy models."""


# Enums
class DocumentStatus(str, enum.Enum):
    """Status of document processing."""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    DELETED = "deleted"


class PermissionType(str, enum.Enum):
    """Types of permissions for collections."""

    READ = "read"
    WRITE = "write"
    ADMIN = "admin"


class CollectionStatus(str, enum.Enum):
    """Status of a collection."""

    PENDING = "pending"
    READY = "ready"
    PROCESSING = "processing"
    ERROR = "error"
    DEGRADED = "degraded"


class OperationStatus(str, enum.Enum):
    """Status of an operation."""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

    @classmethod
    def _missing_(cls, value: Any) -> "OperationStatus | None":
        """Provide case-insensitive lookup for enum values.

        This allows constructing OperationStatus from values like "PROCESSING"
        that appear in some test helpers, while keeping canonical values
        lowercase in the database.
        """
        if isinstance(value, str):
            result = cls.__members__.get(value.upper()) or cls._value2member_map_.get(value.lower())
            return cast("OperationStatus | None", result)
        return None


class OperationType(str, enum.Enum):
    """Types of collection operations."""

    INDEX = "index"
    APPEND = "append"
    REINDEX = "reindex"
    REMOVE_SOURCE = "remove_source"
    DELETE = "delete"
    PROJECTION_BUILD = "projection_build"

    @classmethod
    def _missing_(cls, value: Any) -> "OperationType | None":
        """Provide case-insensitive lookup for enum values."""

        if isinstance(value, str):
            normalized = value.lower()
            return cls._value2member_map_.get(normalized) or cls.__members__.get(value.upper())
        return None

    def __str__(self) -> str:  # pragma: no cover - simple helper for driver bindings
        """Return the canonical string value for the enum member."""

        return self.value


class ProjectionRunStatus(str, enum.Enum):
    """Lifecycle states for embedding projection runs."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class User(Base):
    """User model for authentication."""

    __tablename__ = "users"

    id = Column(Integer, primary_key=True, autoincrement=True)
    username = Column(String, unique=True, nullable=False, index=True)
    email = Column(String, unique=True, nullable=False, index=True)
    full_name = Column(String)
    hashed_password = Column(String, nullable=False)
    is_active = Column(Boolean, default=True)
    is_superuser = Column(Boolean, default=False, nullable=False)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now())
    last_login = Column(DateTime(timezone=True))

    # Relationships
    collections = relationship("Collection", back_populates="owner", cascade="all, delete-orphan")
    api_keys = relationship("ApiKey", back_populates="user", cascade="all, delete-orphan")
    refresh_tokens = relationship("RefreshToken", back_populates="user", cascade="all, delete-orphan")
    permissions = relationship("CollectionPermission", back_populates="user", cascade="all, delete-orphan")
    operations = relationship("Operation", back_populates="user")
    audit_logs = relationship("CollectionAuditLog", back_populates="user")


class Collection(Base):
    """Collection model for organizing documents."""

    __tablename__ = "collections"

    id = Column(String, primary_key=True)  # UUID as string
    name = Column(String, unique=True, nullable=False, index=True)
    description = Column(Text)
    owner_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    vector_store_name = Column(String, unique=True, nullable=False)  # Qdrant collection name
    embedding_model = Column(String, nullable=False)
    quantization = Column(String, nullable=False, default="float16")  # float32, float16, int8
    chunk_size = Column(Integer, nullable=False, default=1000)
    chunk_overlap = Column(Integer, nullable=False, default=200)
    chunking_strategy = Column(String, nullable=True)  # New field for chunking strategy type
    chunking_config = Column(JSON, nullable=True)  # New field for strategy-specific configuration
    is_public = Column(Boolean, nullable=False, default=False, index=True)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    meta = Column(JSON)

    # New fields from second migration
    status = Column(
        Enum(CollectionStatus, name="collection_status", native_enum=True, create_constraint=False),
        nullable=False,
        default=CollectionStatus.PENDING,
        index=True,
    )  # type: ignore[var-annotated]
    status_message = Column(Text)
    qdrant_collections = Column(JSON)  # List of Qdrant collection names
    qdrant_staging = Column(JSON)  # Staging collections during reindex
    document_count = Column(Integer, nullable=False, default=0)
    vector_count = Column(Integer, nullable=False, default=0)
    total_size_bytes = Column(Integer, nullable=False, default=0)

    # New chunking-related fields
    default_chunking_config_id = Column(Integer, ForeignKey("chunking_configs.id"), nullable=True, index=True)
    chunks_total_count = Column(Integer, nullable=False, default=0)
    chunking_completed_at = Column(DateTime(timezone=True), nullable=True)

    # Relationships
    owner = relationship("User", back_populates="collections")
    documents = relationship("Document", back_populates="collection", cascade="all, delete-orphan")
    permissions = relationship("CollectionPermission", back_populates="collection", cascade="all, delete-orphan")
    sources = relationship("CollectionSource", back_populates="collection", cascade="all, delete-orphan")
    operations = relationship("Operation", back_populates="collection", cascade="all, delete-orphan")
    projection_runs = relationship("ProjectionRun", back_populates="collection", cascade="all, delete-orphan")
    audit_logs = relationship("CollectionAuditLog", back_populates="collection", cascade="all, delete-orphan")
    resource_limits = relationship(
        "CollectionResourceLimits", back_populates="collection", uselist=False, cascade="all, delete-orphan"
    )
    default_chunking_config = relationship(
        "ChunkingConfig", foreign_keys=[default_chunking_config_id], back_populates="collections"
    )
    chunks = relationship("Chunk", back_populates="collection", cascade="all, delete-orphan")


class Document(Base):
    """Document model representing documents in collections."""

    __tablename__ = "documents"

    id = Column(String, primary_key=True)  # UUID as string
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    source_id = Column(Integer, ForeignKey("collection_sources.id"), nullable=True, index=True)
    file_path = Column(String, nullable=False)
    file_name = Column(String, nullable=False)
    file_size = Column(Integer, nullable=False)
    mime_type = Column(String)
    content_hash = Column(String, nullable=False, index=True)
    status = Column(
        Enum(DocumentStatus, name="document_status", native_enum=True, create_constraint=False),
        nullable=False,
        default=DocumentStatus.PENDING,
        index=True,
    )  # type: ignore[var-annotated]
    error_message = Column(Text)
    chunk_count = Column(Integer, nullable=False, default=0)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    meta = Column(JSON)

    # New chunking-related fields
    chunking_config_id = Column(Integer, ForeignKey("chunking_configs.id"), nullable=True, index=True)
    chunks_count = Column(Integer, nullable=False, default=0)
    chunking_started_at = Column(DateTime(timezone=True), nullable=True)
    chunking_completed_at = Column(DateTime(timezone=True), nullable=True)

    # Relationships
    collection = relationship("Collection", back_populates="documents")
    source = relationship("CollectionSource", back_populates="documents")
    chunking_config = relationship("ChunkingConfig", back_populates="documents")
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")

    # Indexes
    __table_args__ = (
        Index("ix_documents_collection_content_hash", "collection_id", "content_hash", unique=True),
        Index("ix_documents_collection_id_chunking_completed_at", "collection_id", "chunking_completed_at"),
    )


class ApiKey(Base):
    """API key model for programmatic access."""

    __tablename__ = "api_keys"

    id = Column(String, primary_key=True)  # UUID as string
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
    name = Column(String, nullable=False)
    key_hash = Column(String, unique=True, nullable=False, index=True)
    permissions = Column(JSON)  # Store collection access rights
    last_used_at = Column(DateTime(timezone=True))
    expires_at = Column(DateTime(timezone=True))
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    is_active = Column(Boolean, nullable=False, default=True, index=True)

    # Relationships
    user = relationship("User", back_populates="api_keys")
    collection_permissions = relationship(
        "CollectionPermission", back_populates="api_key", cascade="all, delete-orphan"
    )


class CollectionPermission(Base):
    """Permission model for fine-grained access control."""

    __tablename__ = "collection_permissions"

    id = Column(Integer, primary_key=True, autoincrement=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), index=True)
    api_key_id = Column(String, ForeignKey("api_keys.id", ondelete="CASCADE"), index=True)
    permission = Column(Enum(PermissionType, name="permission_type"), nullable=False)  # type: ignore[var-annotated]
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())

    # Relationships
    collection = relationship("Collection", back_populates="permissions")
    user = relationship("User", back_populates="permissions")
    api_key = relationship("ApiKey", back_populates="collection_permissions")

    # Constraints
    __table_args__ = (
        CheckConstraint(
            "(user_id IS NOT NULL AND api_key_id IS NULL) OR (user_id IS NULL AND api_key_id IS NOT NULL)",
            name="check_user_or_api_key",
        ),
        Index(
            "ix_collection_permissions_unique_user",
            "collection_id",
            "user_id",
            unique=True,
            postgresql_where="user_id IS NOT NULL",
        ),
        Index(
            "ix_collection_permissions_unique_api_key",
            "collection_id",
            "api_key_id",
            unique=True,
            postgresql_where="api_key_id IS NOT NULL",
        ),
    )


class RefreshToken(Base):
    """Refresh token model for JWT authentication."""

    __tablename__ = "refresh_tokens"

    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    token_hash = Column(String, unique=True, nullable=False, index=True)
    expires_at = Column(DateTime(timezone=True), nullable=False)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    is_revoked = Column(Boolean, default=False)

    # Relationships
    user = relationship("User", back_populates="refresh_tokens")


class CollectionSource(Base):
    """Source model for tracking collection data sources."""

    __tablename__ = "collection_sources"

    id = Column(Integer, primary_key=True, autoincrement=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    source_path = Column(String, nullable=False)
    source_type = Column(String, nullable=False, default="directory")  # directory, file, url, etc.
    document_count = Column(Integer, nullable=False, default=0)
    size_bytes = Column(Integer, nullable=False, default=0)
    last_indexed_at = Column(DateTime(timezone=True))
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    meta = Column(JSON)

    # Relationships
    collection = relationship("Collection", back_populates="sources")
    documents = relationship("Document", back_populates="source")

    # Constraints
    __table_args__ = (UniqueConstraint("collection_id", "source_path", name="uq_collection_source_path"),)


class Operation(Base):
    """Operation model for tracking async collection operations."""

    __tablename__ = "operations"

    id = Column(Integer, primary_key=True, autoincrement=True)
    uuid = Column(String, unique=True, nullable=False)  # For external reference
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    type = Column(
        Enum(
            OperationType,
            name="operation_type",
            native_enum=True,
            create_constraint=False,
            values_callable=lambda enum_cls: [e.value for e in enum_cls],
            validate_strings=True,
        ),
        nullable=False,
        index=True,
    )  # type: ignore[var-annotated]
    status = Column(
        Enum(
            OperationStatus,
            name="operation_status",
            native_enum=True,
            create_constraint=False,
            values_callable=lambda enum_cls: [e.value for e in enum_cls],
        ),
        nullable=False,
        default=OperationStatus.PENDING,
        index=True,
    )  # type: ignore[var-annotated]
    task_id = Column(String)  # Celery task ID
    config = Column(JSON, nullable=False)
    error_message = Column(Text)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now(), index=True)
    started_at = Column(DateTime(timezone=True))
    completed_at = Column(DateTime(timezone=True))
    meta = Column(JSON)

    # Relationships
    collection = relationship("Collection", back_populates="operations")
    user = relationship("User")
    audit_logs = relationship("CollectionAuditLog", back_populates="operation")
    metrics = relationship("OperationMetrics", back_populates="operation", cascade="all, delete-orphan")
    projection_run = relationship("ProjectionRun", back_populates="operation", uselist=False)


class ProjectionRun(Base):
    """Dimensionality reduction run persisted for visualization."""

    __tablename__ = "projection_runs"
    __table_args__ = (
        CheckConstraint("dimensionality > 0", name="ck_projection_runs_dimensionality_positive"),
        CheckConstraint("point_count IS NULL OR point_count >= 0", name="ck_projection_runs_point_count_non_negative"),
    )

    id = Column(Integer, primary_key=True, autoincrement=True)
    uuid = Column(String, unique=True, nullable=False, index=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    operation_uuid = Column(
        String,
        ForeignKey("operations.uuid", ondelete="SET NULL"),
        unique=True,
        nullable=True,
        index=True,
    )
    status = Column(
        Enum(
            ProjectionRunStatus,
            name="projection_run_status",
            native_enum=True,
            create_constraint=False,
            values_callable=lambda enum_cls: [e.value for e in enum_cls],
        ),
        nullable=False,
        default=ProjectionRunStatus.PENDING,
        index=True,
    )  # type: ignore[var-annotated]
    dimensionality = Column(Integer, nullable=False)
    reducer = Column(String, nullable=False)
    storage_path = Column(String, nullable=True)
    point_count = Column(Integer, nullable=True)
    config = Column(JSON, nullable=True)
    meta = Column(JSON, nullable=True)
    error_message = Column(Text)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now(), index=True)
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    started_at = Column(DateTime(timezone=True))
    completed_at = Column(DateTime(timezone=True))

    collection = relationship("Collection", back_populates="projection_runs")
    operation = relationship("Operation", back_populates="projection_run")


class CollectionAuditLog(Base):
    """Audit log model for tracking collection actions."""

    __tablename__ = "collection_audit_log"

    id = Column(Integer, primary_key=True, autoincrement=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, index=True)
    operation_id = Column(Integer, ForeignKey("operations.id"), nullable=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=True, index=True)
    action = Column(String, nullable=False)  # created, updated, deleted, reindexed, etc.
    details = Column(JSON)
    ip_address = Column(String)
    user_agent = Column(String)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now(), index=True)

    # Relationships
    collection = relationship("Collection", back_populates="audit_logs")
    operation = relationship("Operation", back_populates="audit_logs")
    user = relationship("User")


class CollectionResourceLimits(Base):
    """Resource limits model for collection quotas."""

    __tablename__ = "collection_resource_limits"

    id = Column(Integer, primary_key=True, autoincrement=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), nullable=False, unique=True)
    max_documents = Column(Integer, default=100000)
    max_storage_gb = Column(Float, default=50.0)
    max_operations_per_hour = Column(Integer, default=10)
    max_sources = Column(Integer, default=10)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())

    # Relationships
    collection = relationship("Collection", back_populates="resource_limits")


class OperationMetrics(Base):
    """Metrics model for tracking operation performance."""

    __tablename__ = "operation_metrics"

    id = Column(Integer, primary_key=True, autoincrement=True)
    operation_id = Column(Integer, ForeignKey("operations.id", ondelete="CASCADE"), nullable=False, index=True)
    metric_name = Column(String, nullable=False)
    metric_value = Column(Float, nullable=False)
    recorded_at = Column(DateTime(timezone=True), nullable=False, default=func.now(), index=True)

    # Relationships
    operation = relationship("Operation", back_populates="metrics")


class ChunkingStrategy(Base):
    """Chunking strategy model for document processing."""

    __tablename__ = "chunking_strategies"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String, unique=True, nullable=False)
    description = Column(Text)
    version = Column(String, nullable=False, default="1.0.0")
    is_active = Column(Boolean, nullable=False, default=True, index=True)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    meta = Column(JSON)

    # Relationships
    configs = relationship("ChunkingConfig", back_populates="strategy")


class ChunkingConfig(Base):
    """Chunking configuration model (deduplicated)."""

    __tablename__ = "chunking_configs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    strategy_id = Column(Integer, ForeignKey("chunking_strategies.id"), nullable=False, index=True)
    config_hash = Column(String(64), unique=True, nullable=False, index=True)
    config_data = Column(JSON, nullable=False)
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    use_count = Column(Integer, nullable=False, default=0, index=True)
    last_used_at = Column(DateTime(timezone=True))

    # Relationships
    strategy = relationship("ChunkingStrategy", back_populates="configs")
    chunks = relationship("Chunk", back_populates="chunking_config")
    collections = relationship(
        "Collection", foreign_keys="Collection.default_chunking_config_id", back_populates="default_chunking_config"
    )
    documents = relationship("Document", back_populates="chunking_config")


class Chunk(Base):
    """Chunk model for partitioned document storage.

    IMPORTANT: This table is partitioned by LIST(partition_key) in PostgreSQL with 100 partitions.

    Partition Awareness:
    - The table uses LIST partitioning on partition_key (0-99)
    - partition_key is computed automatically via trigger: abs(hashtext(collection_id)) % 100
    - Primary key is (id, collection_id, partition_key) to support partitioning
    - Always include collection_id in WHERE clauses for optimal partition pruning
    - Bulk operations should be grouped by collection_id for efficiency
    - Cross-collection queries will scan multiple partitions (use sparingly)

    Usage Examples:
        # Good - partition pruning enabled
        chunks = session.query(Chunk).filter(
            Chunk.collection_id == collection_id,
            Chunk.document_id == document_id
        ).all()

        # Bad - scans all partitions
        chunks = session.query(Chunk).filter(
            Chunk.document_id == document_id
        ).all()

    Bulk Insert Example:
        # Group chunks by collection_id for efficient partition routing
        chunks_by_collection = {}
        for chunk in chunks_to_insert:
            chunks_by_collection.setdefault(chunk.collection_id, []).append(chunk)

        for collection_id, chunks in chunks_by_collection.items():
            session.bulk_insert_mappings(Chunk, chunks)
    """

    __tablename__ = "chunks"

    # Primary key includes partition key for partitioned table support
    # Note: id is BigInteger with auto-incrementing sequence
    id = Column(BigInteger, primary_key=True, autoincrement=True)
    collection_id = Column(String, ForeignKey("collections.id", ondelete="CASCADE"), primary_key=True, nullable=False)
    partition_key = Column(Integer, primary_key=True, nullable=False, server_default="0")  # Computed via trigger

    # Foreign keys and data columns
    document_id = Column(String, ForeignKey("documents.id", ondelete="CASCADE"), nullable=True)  # Can be NULL
    chunking_config_id = Column(Integer, ForeignKey("chunking_configs.id"), nullable=True)  # Can be NULL
    chunk_index = Column(Integer, nullable=False)
    content = Column(Text, nullable=False)
    start_offset = Column(Integer)  # Can be NULL
    end_offset = Column(Integer)  # Can be NULL
    token_count = Column(Integer)
    embedding_vector_id = Column(String)  # Reference to Qdrant
    meta = Column(
        "metadata", JSON
    )  # Column name is 'metadata' in DB, but 'meta' in Python to avoid SQLAlchemy reserved word
    created_at = Column(DateTime(timezone=True), nullable=False, default=func.now())
    updated_at = Column(DateTime(timezone=True), nullable=False, default=func.now())

    # Relationships
    collection = relationship("Collection", back_populates="chunks")
    document = relationship("Document", back_populates="chunks")
    chunking_config = relationship("ChunkingConfig", back_populates="chunks")

    # Composite indexes optimized for partition pruning
    __table_args__ = (
        # Indexes that exist at the database level
        Index("idx_chunks_part_collection", "collection_id"),  # Per-partition index
        Index("idx_chunks_part_created", "created_at"),  # Per-partition index
        Index("idx_chunks_part_chunk_index", "collection_id", "chunk_index"),  # Per-partition index
        Index("idx_chunks_part_document", "document_id"),  # Per-partition conditional index
        {
            "comment": "Partitioned by LIST(partition_key) with 100 partitions. partition_key is computed via trigger.",
            "info": {
                "partition_key": "partition_key",
                "partition_method": "LIST",
                "partition_count": 100,
                "partition_trigger": "compute_partition_key()",
            },
        },
    )
</file>

</files>
